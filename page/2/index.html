<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="seven 的精神家园，学习笔记">
<meta name="keywords" content="云计算,大数据，kuberntes">
<meta property="og:type" content="website">
<meta property="og:title" content="岳阳北寒">
<meta property="og:url" content="http://sevengarden.club/page/2/index.html">
<meta property="og:site_name" content="岳阳北寒">
<meta property="og:description" content="seven 的精神家园，学习笔记">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="岳阳北寒">
<meta name="twitter:description" content="seven 的精神家园，学习笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://sevengarden.club/page/2/"/>





  <title>岳阳北寒</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">岳阳北寒</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">要有最朴素的生活和最遥远的梦想，即使明日天寒地冻，路远马亡.......</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-somrthing">
          <a href="/有料" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            somrthing
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2019/08/17/CDN技术详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/17/CDN技术详解/" itemprop="url">CDN技术详解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-17T19:56:45+08:00">
                2019-08-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CDN/" itemprop="url" rel="index">
                    <span itemprop="name">CDN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p><em>摘自《CDN技术详解》</em></p>
</blockquote>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><ul>
<li>广义互联网，两层组成<ul>
<li>以TCP/IP为代表的网络层（狭义的互联网），网络的基础，一般说TCP/IP网络；将各种信息的数据报文以极低的成本进行进行传输，俗称“管道”，所有信息都在这一管道中传送</li>
<li>以万维网（WWW）为代表的应用层，是广义互联网的上层。包括各种类型的流量和应用，邮件、软件、在线影视、游戏、电子商务等</li>
</ul>
</li>
<li>“第一公里”是指万维网流量向用户传送的第一个出口，是网站服务器接入互联网的链路所能提供的带宽。这个带宽决定了一个 网站能为用户提供的访问速度和并发访问量。如果业务繁忙，用户的访问数越多，拥塞越严重，网站会在最需要向用户提供服务时失去用户。（还有“中间一公里” 和“最后一公里”分别代表互联网传输传输和万维网流量向用户传送的最后一段接入链路）</li>
<li>从互联网的架构来看，不同网络之间的互联互通带宽，对任何一个运营商网络的流量来说，占比都比较小，收敛比是非常高的，因此这里通常都是互联网传输中的拥堵点（运营商互联互通的问题）</li>
<li>其次是骨干网堵塞问题，由于互联网上的绝大部分流量都要通过骨干网络进行传输，这就要求骨干网络的承载能力必须与互联网 的应用同步发展，但实际上两者并不是同步的，当骨干网络的升级和扩容滞后于互联网之上的应用的发展时，就会阶段性地使得大型骨干网的承载能力成为影响互联 网性能的瓶颈（区域互联互通问题，骨干网带宽瓶颈）</li>
<li>服务响应时间由服务响应时间和网络时延组成。网站到用户之间要经过IDC、骨干网、用户所在的局域网、用户所在的接入网</li>
<li>在互联网领域有一个“8秒定律”，用户访问一个网站时，如果等待网页打开的时间超过8秒，会有超过30%的用户放弃等待</li>
<li>CDN技术在网络传输上利用缓存技术使得Web服务数据流能就近访问，是优化网络数据传输非常有效的技术，从而获得高速的体验和品质保证</li>
<li>CDN的全称是Content Delivery Network，即内容分发网络。其目的是通过在现有的Internet中增加一层新的网络架构，将网站的内容发布到最接近用户的网络”边缘”，使用户可以就近取得所需的内容，解决Internet网络拥塞状况，提高用户访问网站的响应速度。从技术上全面解决由于网络带宽小、用户访问量大、网点分布不均等原因，解决用户访问网站的响应速度慢的根本原因</li>
<li>CDN工作原理<div align="center">
<a href="https://sm.ms/image/M7J4bgop89NzVOs" target="_blank"><img src="https://i.loli.net/2019/08/17/M7J4bgop89NzVOs.png" alt="ScreenShot_20190804144149.png"></a>
</div>
</li>
</ul>
<ol>
<li>当用户点击网站页面上的内容URL，经过本地DNS系统解析，DNS系统会最终将域名的解析权交给CNAME指向的CDN专用DNS服务器</li>
<li>CDN的DNS服务器将CDN的全局负载均衡设备IP地址返回用户</li>
<li>用户向CDN的全局负载均衡设备发起内容URL访问请求</li>
<li>CDN全局负载均衡设备根据用户IP地址，以及用户请求的内容URL，选择一台用户所属区域的区域负载均衡设备，告诉用户向这台设备发起请求</li>
<li>区域负载均衡设备会为用户选择一台合适的缓存服务器提供服务，选择的依据包括：根据用户IP地址，判断哪一台服务器距用户最近；根据用户所请求的URL中携带的内容名称，判断哪一台服务器上用户所需内容；查询各个服务器当前的负载情况，判断哪一台服务器尚有服务能力。基于以上这些条件综合分析之后，区域负载均衡设备返回一台缓存服务器的IP地址。</li>
<li>全局负载均衡设备把服务器的IP地址返回给用户</li>
<li>用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器并没有用户想要的内容，而区域均衡设备依然将它分配给用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地</li>
</ol>
<ul>
<li>DNS服务器根据用户IP地址，将域名解析成相应节点的缓存服务器IP地址，实现用户就近访问。使用CDN服务的网站，只需要将域名解析权交给CDN的GSLB设备，将需要分发的内容注入CDN，就可以实现内容加速了</li>
</ul>
<h1 id="CDN技术"><a href="#CDN技术" class="headerlink" title="CDN技术"></a>CDN技术</h1><ul>
<li>CDN提供一种机制，当用户请求内容时，该内容能够由以最快速度交付的Cache来向用户提供，这个挑选“最优”的过程就叫做负载均衡</li>
<li>CDN系统架构</li>
</ul>
<p><img src="https://i.loli.net/2019/08/17/Ns9kElYohHR8OxB.png" alt="ScreenShot_20190804145702.png"></p>
<ul>
<li><strong>分发服务系统</strong>：最基本的工作单元就是Cache设备，cache（边缘cache）负责直接响应最终用户的访问请求，把缓存在本地的内容快速地提供给用 户。同时cache还负责与源站点进行内容同步，把更新的内容以及本地没有的内容从源站点获取并保存在本地。Cache设备的数量、规模、总服务能力是衡 量一个CDN系统服务能力的最基本的指标</li>
<li><strong>负载均衡系统</strong>：主要功能是负责对所有发起服务请求的用户进行访问调度，确定提供给用户的最终实际访问地址。两级调度体系分为全局负载均衡（GSLB）和本 地负载均衡（SLB）。GSLB主要根据用户就近性原则，通过对每个服务节点进行“最优”判断，确定向用户提供服务的cache的物理位置。SLB主要负 责节点内部的设备负载均衡</li>
<li><strong>运营管理系统</strong>：分为运营管理和网络管理子系统，负责处理业务层面的与外界系统交互所必须的收集、整理、交付工作，包含客户管理、产品管理、计费管理、统计分析等功能。<h2 id="China-Cache-CDN-系统架构图"><a href="#China-Cache-CDN-系统架构图" class="headerlink" title="China Cache CDN 系统架构图"></a>China Cache CDN 系统架构图</h2><img src="https://i.loli.net/2019/08/17/vKr6QbXO5fjk8ZP.png" alt="ScreenShot_20190804150239.png"></li>
<li>部署架构<ul>
<li>节点是CDN系统中最基本的部署的单元，一个CDN系统由大量、地理位置分散的POP节点组成，为用户提供就近的内容访问服务。</li>
<li>CDN节点 网络主要包含CDN骨干节点和POP节点。</li>
<li>中心和区域节点一般称之为骨干姐点，主要作为内容分发和边缘未命中时的服务点；边缘节点被称之为POP（point-of-presence）节点，pop节点主要作为直接向用户提供服务的节点。</li>
<li>从节点构成来讲，无论是骨干节点还是pop节点，都有Cache设备和本地负载均衡设备构成</li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/17/ew6baKtMBGnJCHO.png" alt="ScreenShot_20190804150754.png"></p>
<ul>
<li>基于承载内容类型<ul>
<li>网页加速：网页静态内容</li>
<li>流媒体加速：直播和点播</li>
<li>文件传输加速：软件下载、更新等</li>
<li>应用协议加速：企业应用加速主要是动态加速和SSL加速</li>
</ul>
</li>
</ul>
<ol>
<li>SSL应用加速：由于需要大量的加密解密运算，SSL应用对服务器端的资源消耗是非常巨大的。CDN提供SSL应用加速后，由CDN的专用SSL加速硬件来完成加密解密运算工作</li>
<li>网页压缩：HTTP1.1提出对网页压缩的支持。在服务器端可以先对网页数据进行压缩，然后将压缩后的文件提供给访问用户，最后在用户浏览器端解压显示（但要衡量加解压时间）（HTTP2.0支持压缩）</li>
</ol>
<ul>
<li>基于内容生成机制的分类和分层加速服务<ul>
<li>静态内容：网页HTML文件提供，任何人在任何时间浏览静态内容看到的都是一样的内容</li>
<li>动态内容：不同的访问者在不同时间访问同一个web页面时可能得到不同的页面内容，内容具有实时性，访问过程具有交互性；同时还需要采用数据管理系统和业务逻辑程序来使网站具有更多自动和高级功能</li>
</ul>
</li>
</ul>
<h1 id="内容缓存工作原理"><a href="#内容缓存工作原理" class="headerlink" title="内容缓存工作原理"></a>内容缓存工作原理</h1><ul>
<li>缓存技术（caching）：缓存代理缓存被访问过的内容，后续的相同内容访问直接通过缓存代理获得服务；CDN是缓存技术的基础上发展起来的，是缓存的分布式集群实现</li>
<li>Web架构的精华有三处：<ul>
<li>超文本技术HTML实现信息与信息的连接；</li>
<li>统一资源标志符URI实现全球信息的精确定位</li>
<li>应用层协议HTTP实现分布式的信息共享</li>
</ul>
</li>
<li>TCP连接在每一次HTTP（HTTP 1.0）请求和响应完成后就关闭，如果客户端还要请求其他对象，需要重新为每个对象建立TCP连接。当一个Web页面内包含多个对象并全部显示时，客户端需要与服务器建立的TCP连接数较多，对整个时延和网络流量造成了较大的影响</li>
<li>HTTP1.1采用了效率更高 的持续连接机制，即客户端和服务器端建立TCP连接后，后续相关联的HTTP请求可以重复利用已经建立起来的TCP连接，不仅整个Web页面（包括基本的 HTML文件和其他对象）可以使用这个持续的TCP连接来完成HTTP请求和响应，而且同一个服务器内的多个Web页面也可以通过同一个持续TCP连接来 请求和响应。通常情况下，这个持续的TCP连接会在空闲一段特定的时间后关闭，而这个最大空闲时间时可以设置的（连接复用）。</li>
<li>HTTP协议中的缓存技术：新 鲜度（时间值）和验证（验证信息如ETag或last-modified）时确定内容可否直接提供服务的最重要依据。如果缓存内容足够新鲜，缓存的内容就 能直接满足HTTP访问的需求了；如果内容过期，而经源服务器验证后发现内容没有发生变化，缓存服务器也会避免将内容从源服务器重新传输一遍</li>
<li>如果要通过META标签来控制页面不缓存，一般情况下会在Web页面的<head><meta name="generator" content="Hexo 3.9.0">区域中增加”pragma:no-cache”</head></li>
<li>验证的目的就是检验缓存内容是否可用。当中间缓存存在一个过期的缓存内容，并且对应的访问请求到达时，缓存应该首先向源服务器或者其他保存有未过期的缓存服务器请求验证来确定本地的缓存内容是否可用。（缓存内容过期，但源服务器没有更新内容，即缓存内容仍可用</li>
<li>HTTP1.1介绍了cache-control显示指令来让网站发布者可以更全面地控制他们的内容，并对过期时间进行限制（控制是否缓存，怎么缓存）</li>
<li>HTTP gzip压缩：大多数情况需要压缩的文件时网页中出现最频繁的HTML、CSS、javascript、XML等文件，这类本身是没有经过压缩的文本文件，可以取得较好的压缩效果</li>
<li>HTTP与TCP关系</li>
</ul>
<div align="center"> ![ScreenShot_20190804162308.png](https://i.loli.net/2019/08/17/EHS8wOft6PdhmZb.png)</div>

<ul>
<li>HTTP通常时架构在TCP传输协议之上，出于安全考虑，HTTP还需要经过TLS或SSL层封装</li>
<li>HTTP客户端（浏览器）首先发起建立于服务器的TCP连接，然后客户端于服务器的HTTP进程就可以通过各自的套接字（socket）来下层的TCP</li>
<li>HTTP协议是无状态协议，每次请求和响应都是独立的<ul>
<li>Web通过cookie和session 解决无状态带来的问题</li>
</ul>
</li>
<li>Cookie在客户端存放</li>
<li>Session，保持用户的状态访问，服务器侧（Http cookie机制或者URL重写）<ul>
<li>安全控制通过HTTPS协议实现</li>
<li>Web cache关键技术性能指标</li>
</ul>
</li>
<li>并发量</li>
<li>吞吐率</li>
<li>命中率</li>
<li>响应时间和丢包率（DNS解析、建立连接时间、重定向时间、收到第一个包时间、图片下载时间、页面下载总时间）<ul>
<li>内容更新机制</li>
</ul>
</li>
</ul>
<ol>
<li>若HTTP响应头信息告诉cache不需要缓存，那么cache就不会缓存相应内容</li>
<li>若对某内容的请求信息是需要认证或安全加密的，cache也不会缓存相应内容</li>
<li>若HTTP相应中没有Etag或者Last-modified头信息，cache会认为缺乏直接的更新信息，默认该内容不可缓存（last-modified 时间精度上有一定缺陷）</li>
<li>一个缓存的副本若含有一下信息，cache会认为它是足够新的，会直接从缓存中送出，不会向源服务器发送请求<br>含有完整的过期时间和寿命控制的头信息，并且内容仍在生存期内<br>浏览器已经使用过这个缓存副本，并且在同一个会话中已经检查过内容的新鲜度</li>
<li>若缓存的内容副本已经过时，cache将向源站服务器请求校验，用于确定是否可以继续使用当前版本继续服务。若经校验发现副本的原件没有变化，cache会避免从源站获取副本，通常来讲，HTML文件、图片、css、xml、js、音频、流媒体等静态资源会被缓存，而动态地址、asp、py、jsp、php等动态资源不被缓存<ul>
<li>Web cache优化</li>
</ul>
</li>
<li>HTTP链接聚合</li>
<li>HTTP Gzip压缩</li>
</ol>
<h1 id="集群服务与负载均衡"><a href="#集群服务与负载均衡" class="headerlink" title="集群服务与负载均衡"></a>集群服务与负载均衡</h1><ul>
<li>web集群是由多个同时运行同一个web应用的服务器组成，在外界看来就像一个服务器一样，这多台服务器共同来为客户提供更高性能的服务。集群更标准的定义是：一组相互独立的服务器在网络中表现为单一的系统，并以单一系统的模式加以管理，此单一系统为客户工作站提供高可靠性的服务。</li>
<li>而负载均衡的任务就是负责多个服务器之 间（集群内）实现合理的任务分配，使这些服务器（集群）不会出现因某一台超负荷、而其他的服务器却没有充分发挥处理能力的情况。负载均衡有两个方面的含 义：首先，把大量的并发访问或数据流量分担到多台节点上分别处理，减少用户等待响应的时间；其次，单个高负载的运算分担到多台节点上做并行处理，每个节点 设备处理结束后，将结果汇总，再返回给用户，使得信息系统处理能力可以得到大幅度提高</li>
<li>高可用性集群(HA Cluster)</li>
<li>指为了使群集的整体服务尽可能可用，减少服务宕机时间为目的的集群技术。如果高可用性集群中的某节点发生了故障，那么这段时间内将由其他节点代替它的工作。当然对于其他节点来讲，负载相应的就增加了。</li>
<li>为了提高整个系统的可用性，除了提高计算机各个部件的可靠性以外，一般情况下都会采用该集群的方案。</li>
<li>对于该集群方案，一般会有两种工作方式：</li>
</ul>
<ol>
<li>主-主(Active-Active)工作方式<ul>
<li>这是最常用的集群模型，它提供了高可用性，并且在只有一个节点时也能提供可以接受的性能，该模型允许最大程度的利用硬件资源。每个节点都通过网络对客户机 提供资源，每个节点的容量被定义好，使得性能达到最优，并且每个节点都可以在故障转移时临时接管另一个节点的工作。所有的服务在故障转移后仍保持可用，但 是性能通常都会下降。</li>
<li>这是目前运用最为广泛的双节点双应用的Active/Active模式。</li>
<li>支撑用户业务的应用程序在正常状态下分别在两台节点上运行，各自有自己的资源，比如IP地址、磁盘阵列上的卷或者文件系统。当某一方的系统或者资源出现故障时，就会将应用和相关资源切换到对方的节点上。</li>
<li>这种模式的最大优点是不会有服务器的“闲置”，两台服务器在正常情况下都在工作。但如果有故障发生导致切换，应用将放在同一台服务器上运行，由于服务器的处理能力有可能不能同时满足数据库和应用程序的峰值要求，这将会出现处理能力不够的情况，降低业务响应水平。</li>
</ul>
</li>
<li>主-从(Active-Standby)工作方式<ul>
<li>为了提供最大的可用性，以及对性能最小的影响，主-从工作方式需要一个在正常工作时处于备用状态的节点，主节点处理客户机的请求，而备用节点处于空闲状态，当主节点出现故障时，备用节点会接管主节点的工作，继续为客户机提供服务，并且不会有任何性能上影响。</li>
<li>两节点的Active/Standby模式是HA中最简单的一种，两台服务器通过双心跳线路组成一个集群。应用Application联合各个可选的系统组件如：外置共享的磁盘阵列、文件系统和浮动IP地址等组成业务运行环境。</li>
<li>此环境提供了完全冗余的服务器配置。这种模式的优缺点：</li>
<li>缺点：Node2在Node1正常工作时是处于“闲置”状态，造成服务器资源的浪费。</li>
<li>优点：当Node1发生故障时，Node2能完全接管应用，并且能保证应用运行时的对处理能力要求</li>
</ul>
</li>
</ol>
<ul>
<li>高可扩展性集群<ul>
<li>这里指带有负载均衡策略（算法）的服务器群集技术。带负载均衡集群为企业需求提供了更实用的方案，它使负载可以在计算机集群中尽可能平均地分摊处理。而需 要均衡的可能是应用程序处理负载或是网络流量负载。该方案非常适合于运行同一组应用程序的节点。每个节点都可以处理一部分负载，并且可以在节点之间动态分 配负载， 以实现平衡。对于网络流量也是如此。通常，单个节点对于太大的网络流量无法迅速处理，这就需要将流量发送给在其它节点。还可以根据每个节点上不同的可用资 源或网络的特殊环境来进行优化。</li>
<li>负载均衡集群在多节点之间按照一定的策略（算法）分发网络或计算处理负载。负载均衡建立在现有网络结构之上，它提供了一种廉价有效的方法来扩展服务器带宽，增加吞吐量，提高数据处理能力，同时又可以避免单点故障。</li>
</ul>
</li>
<li>web 负载均衡的作用就是把请求均匀的分配给各个节点，它是一种动态均衡，通过一些工具实时地分析数据包，掌握网络中的数据流量状况，把请求理分配出去。对于不 同的应用环境（如电子商务网站，它的计 算负荷大；再如网络数据库应用，读写频繁，服务器的存储子系统系统面临很大压力；再如视频服务应用，数据传输量大，网络接口负担重压。），使用的均衡策略 (算法)是不同的。 所以均衡策略（算法）也就有了多种多样的形式，广义上的负载均衡既可以设置专门的网关、负载均衡器，也可以通过一些专用软件与协议来实现。在OSI七层协 议模型中的第二（数据链路层）、第三（网络层）、第四（传输层）、第七层（应用层）都有相应的负载均衡策略（算法），在数据链路层上实现负载均衡的原理是 根据数据包的目的MAC地址选择不同的路径；在网络层上可利用基于IP地址的分配方式将数据流疏通到多个节点；而传输层和应用层的交换（Switch）， 本身便是一种基于访问流量的控制方式，能够实现负载均衡。</li>
<li>目前，基于负载均衡的算法主要有三种：轮循（Round-Robin）、最小连接数（Least Connections First），和快速响应优先（Faster Response Precedence）。<ol>
<li>轮循算法，就是将来自网络的请求依次分配给集群中的节点进行处理。</li>
<li>最小连接数算法，就是为集群中的每台服务器设置一个记数器，记录每个服务器当前的连接数，负载均衡系统总是选择当前连接数最少的服务器分配任务。 这要比”轮循算法”好很多，因为在有些场合中，简单的轮循不能判断哪个节点的负载更低，也许新的工作又被分配给了一个已经很忙的服务器了。</li>
<li>快速响应优先算法，是根据群集中的节点的状态（CPU、内存等主要处理部分）来分配任务。 这一点很难做到，事实上到目前为止，采用这个算法的负载均衡系统还很少。尤其对于硬件负载均衡设备来说，只能在TCP/IP协议方面做工作，几乎不可能深入到服务器的处理系统中进行监测。但是它是未来发展的方向。</li>
</ol>
</li>
<li>上面是负载均衡常用的算法，基于以上负载均衡算法的使用方式上，又分为如下几种：<ol>
<li>DNS轮询<br>最早的负载均衡技术是通过DNS来实现的，在DNS中为多个地址配置同一个名字，因而查询这个名字的客户机将得到其中一个地址，从而使得不同的客户访问不同的服务器，达到负载均衡的目的。<br>DNS负载均衡是一种简单而有效的方法，但是它不能区分服务器的差异，也不能反映服务器的当前运行状态。当使用DNS负载均衡的时候，必须尽量保证不同的 客户计算机能均匀获得不同的地址。由于DNS数据具备刷新时间标志，一旦超过这个时间限制，其他DNS服务器就需要和这个服务器交互，以重新获得地址数 据，就有可能获得不同IP地址。因此为了使地址能随机分配，就应使刷新时间尽量短，不同地方的DNS服务器能更新对应的地址，达到随机获得地址，然而将过 期时间设置得过短，将使DNS流量大增，而造成额外的网络问题。DNS负载均衡的另一个问题是，一旦某个服务器出现故障，即使及时修改了DNS设置，还是 要等待足够的时间（刷新时间）才能发挥作用，在此期间，保存了故障服务器地址的客户计算机将不能正常访问服务器</li>
<li>反向代理服务器<br>使用代理服务器，可以将请求转发给内部的服务器，使用这种加速模式显然可以提升静态网页的访问速度。然而，也可以考虑这样一种技术，使用代理服务器将请求均匀转发给多台服务器，从而达到负载均衡的目的。<br>　  这种代理方式与普通的代理方式有所不同，标准代理方式是客户使用代理访问多个外部服务器，而这种代理方式是代理多个客户访问内部服务器，因此也被称为反向代理模式。虽然实现这个任务并不算是特别复杂，然而由于要求特别高的效率，实现起来并不简单。<br>　使用反向代理的好处是，可以将负载均衡和代理服务器的高速缓存技术结合在一起，提供有益的性能。然而它本身也存在一些问题，首先就是必须为每一种服务都专门开发一个反向代理服务器，这就不是一个轻松的任务。<br>　代理服务器本身虽然可以达到很高效率，但是针对每一次代理，代理服务器就必须维护两个连接，一个对外的连接，一个对内的连接，因此对于特别高的连接请求， 代理服务器的负载也就非常之大。反向代理方式下能应用优化的负载均衡策略，每次访问最空闲的内部服务器来提供服务。但是随着并发连接数量的增加，代理服务 器本身的负载也变得非常大，最后反向代理服务器本身会成为服务的瓶颈。 <ol start="3">
<li>地址转换网关<br>支持负载均衡的地址转换网关，可以将一个外部IP地址映射为多个内部IP地址，对每次TCP连接请求动态使用其中一个内部地址，达到负载均衡的目的。很多 硬件厂商将这种技术集成在他们的交换机中，作为他们第四层交换的一种功能来实现，一般采用随机选择、根据服务器的连接数量或者响应时间进行选择的负载均衡 策略来分配负载。由于地址转换相对来讲比较接近网络的低层，因此就有可能将它集成在硬件设备中，通常这样的硬件设备是局域网交换机.</li>
</ol>
</li>
</ol>
</li>
</ul>
<h1 id="全局负载均衡"><a href="#全局负载均衡" class="headerlink" title="全局负载均衡"></a>全局负载均衡</h1><ul>
<li>全局负载均衡（GSLB）的负载均衡主要是在多个节点之间进行均衡，其结果可能直接终结负载均衡过程，也可能将用户访问交付下一层次的（区域或本地）负载均衡系统进行处理。GSLB最通用的是基于DNS解析方式，还有HTTP重定向、IP路由等方法</li>
<li>当需要访问abc.com这个站点时，实际上我们想要浏览的网页内容都存放在互联网中对应某个IP的服务器上，而浏览器的任务就是找到我们想要访问的这台服务器的IP地址，然后向它请求内容。</li>
<li>本地DNS服务器（local DNS server）是用户所在局域网或ISP网络中的域名服务器。当客户端在浏览器里请求abc.com时，浏览器会首先向本地DNS服务器请求将 abc.com解析成IP地址，本地DNS服务器再向整个DNS系统查询，直到找到解析结果。客户端可以配置DNS服务器或通过DHCP来分配</li>
<li>DNS给使用它的互联网应用带来额外的时延，有时时延还比较大，为了解决问题，需要引入“缓存”机制。缓存是指DNS查 询结果在主机（local DNS server）中缓存。在区内主机对某个域名发起第一次查询请求时，负责处理递归查询的DNS服务器要发送好几次查询（先查.root，再查.com之 类，再定位IP地址等）才能找到结果，不过在这过程中它也得到了许多信息，比如各区域权威DNS服务器（就是告诉你最终abc.com在哪里的DNS服务 器）和它们的地址、域名解析最终结果。他会把这些信息保存起来，当其他主机向它发起查询请求时，它就直接向主机返回缓存中能够找到的结果，直到数据过期</li>
<li>客户端浏览器也可以缓存DNS响应信息</li>
<li>Internet类资源记录分为<ul>
<li>A记录（address）：域名-&gt;多个IP的映射。对同一个域名，可以有多条A记录</li>
<li>NS记录（name server）：指定由哪台DNS服务器来解析</li>
<li>SOA记录（start of authority）：指定该区域的权威域名服务器</li>
<li>CNAME记录（canonical name）：多个域名-&gt;服务器的映射</li>
<li>PTR记录（pointer record）：IP-&gt;域名的映射</li>
</ul>
</li>
<li>DNS系统本身是具备简单负载分配能力的，这是基于DNS的轮询机制。如果有多台Web服务器（多源）同时为站点 abc.com提供服务，abc.com的权威服务器可能会解析出一个或多个IP地址。权威域名服务器还可以调整响应中IP地址的排列方式，即在每次响应 中将不同的IP地址置于首位（取决于可服务能力和服务质量），通过这种方式实现对这些Web服务器的负载均衡</li>
<li>通过CNAME方式实现负载均衡：域名服务器获得CNAME记录后，就会用记录中的别名来替换查找的域名或主机名（实现多个域名-&gt;服务器映射）。后面会查询这个别名的A记录来获取相应的IP地址。</li>
<li>具体操作为：先将GSLB的主机名定义为所查询域名的权威DNS服务器的别名，然后将GSLB主机名添加多条A记录，分别对应多个服务器的IP地址。这样，本地DNS服务器会向客户端返回多个IP地址作为域名的查询结果，并且这些IP地址的排列顺序是轮换的。客户端一般会选择首个IP地址进行访问</li>
<li>负载均衡器作为权威DNS服务器：负载均衡器就会接收所有对这个域名的DNS请求，从而能够根据预先设置的一些策略来提 供对域名的智能DNS解析。F5的DNS具有完整的DNS功能以及增强的GSLB特性，Foundry、Nortel、Cisco和Radware的产品 能实现部分DNS功能</li>
<li>负载均衡作为代理DNS服务器：负载均衡器被注册为一个域名空间的权威DNS服务器，而真正的权威域名服务器则部署在负 载均衡器后面。所有的DNS请求都会先到达负载均衡器，由负载均衡器转发到真正的权威DNS服务器，然后修改权威DNS服务器返回的响应信息。真正的权威 DNS服务器正常响应浏览器的DNS请求，返回域名解析结果列表，这个响应会先发送到负载均衡器，而负载均衡器会根据自己的策略选择一个性能最好的服务器 IP并修改需要实现GSLB的域名的DNS查询响应，对其他请求透明转发，这样就不会影响整个域名空间的解析性能。</li>
<li>在基于DNS方式下无论采用何 种工作方式，都会有一些请求不会到达GSLB，这是DNS系统本身的缓存机制在起作用。当用户请求的域名在本地DNS或本机（客户端浏览器）得到了解析结 果，这些请求就不会达到GSLB。Cache更新时间越短，用户请求达到GSLB的几率越大。由于DNS的缓存机制屏蔽掉相当一部分用户请求，从而大大减 轻了GSLB处理压力，使得系统抗流量冲击能力显著提升，这也是很多商业CDN选择DNS机制做全局负载均衡的原因之一。但弊端在于，如果在DNS缓存刷 新间隔之内系统发生影响用户服务的变化，比如某个节点故障，某个链路拥塞等，用户依然会被调度到故障部位去</li>
<li>智能DNS功能，它在向本地DNS返回应答之前会先根据一些静态或动态策略进行智能计算。<ul>
<li>服务器的“健康状况”</li>
<li>地理区域距离</li>
<li>会话保持</li>
<li>响应时间</li>
<li>IP地址权重</li>
<li>会话能力阈值</li>
<li>往返时间（TTL）</li>
<li>其他信息，包括服务器当前可用会话数、最少选择次数、轮询等</li>
</ul>
</li>
</ul>
<h2 id="关于GSLB的部署问题"><a href="#关于GSLB的部署问题" class="headerlink" title="关于GSLB的部署问题"></a>关于GSLB的部署问题</h2><ul>
<li>关于内容的缓存问题（如何智能调度最有效）和配置</li>
<li>在有些CDN中（用于视频网站加速的情况较多），网站需要加速的内容全部先缓存在OCS（内容中心），然后再将一部分 （通常是热门的内容）分发到个POP节点（Cache边缘集群），所以POP节点在某些时候会出现本地不命中而需要回OCS取内容或者从其他POP节点取 内容的情况</li>
<li>纯粹基于DNS方式的GSLB只能完成就近性判断。为实现智能调度，大多数解决方案需要在GSLB设备附近以旁路的方式 部署一台辅助设备（为方便描述，我们可称之为GRM——全局资源管理设备），用以实现和各POP节点的本地资源管理设备进行通信，完成CDN对各POP节 点的状态检查，并根据POP节点的状态和流量情况，重新制订用户调度策略，将策略实时发送到GSLB中去执行</li>
<li>因为DNS服务采用以UDP为基础的、默认无连接的访问方式，给分布式攻击（DDoS）带来了更大的便利。（有DNSSEC可以提供某程度的DDoS攻擊保護）</li>
<li>隐藏节点的存在很大程度上可以避免GSLB被攻击致瘫痪的机会，实际隐藏节点的实现方法就是在实际组网时除了部署正常工作的GSLB以外，再部署一台备份的GSLB设备，并将这一备份GSLB设备隐藏起来，不对外公布。</li>
<li>HTTP重定向（CDN GSLB用302重定向）：在HTTP协议中，有三类重定向状态吗：301永久性转移（permanently moved）、302暂时转移（temporarily moved）、meta fresh在特定时间后重定向到新的网页</li>
<li>HTTP重定向只适用于HTTP应用，不适用于任何其他应用。比如微软的MMS协议，RTSP协议，就不能使用这种方式 进行重定向。其次，由于HTTP重定向过程需要额外解析域名URL，还需要与URL建立TCP连接并且发送HTTP请求，使得响应时间加长。第三，不同于 DNS方式，没有任何用户请求能被外部系统终结（不能缓存），所有请求都必须进入GSLB系统，这将成为性能和可靠性的瓶颈。（流媒体用的比较多）</li>
<li>基于IP路由的GSLB</li>
<li>基于路由协议算法选择一条路由到达这两个本地均衡器中的一个。因为每次访问请求的终端IP地址不同，路由条件也不同，所以在多个路由器上优选的路由不同，从统计复用的角度来看基本是在负载均衡器1和2之间均匀分布的。</li>
<li>IP路由在多个POP点之间实现的负载均衡是一种概率上的均衡，而不是真正的均衡（没做智能调度）</li>
</ul>
<table>
<thead>
<tr>
<th><strong>比较项</strong></th>
<th><strong>基于DNS解析方式</strong></th>
<th><strong>基于HTTP重定向方式</strong></th>
<th><strong>基于IP路由方式</strong></th>
</tr>
</thead>
<tbody><tr>
<td>性能</td>
<td>本地DNS服务器和用户终端DNS缓存能力使GSLB的负载得到有效分担</td>
<td>GSLB处理压力大，容易成为系统性能的瓶颈</td>
<td>借助IP网络设备完成负载均衡，没有单点性能瓶颈</td>
</tr>
<tr>
<td>准确度</td>
<td>定位准确度取决于本地DNS覆盖范围，本地DNS设置错误会造成定位不准确</td>
<td>在对用户IP地址数据进行有效维护的前提下，定位准确且精度高</td>
<td>就近性调度准确，但对设备健康性等动态信息响应会有延迟</td>
</tr>
<tr>
<td>效率</td>
<td>效率约等于DNS系统本身处理效率</td>
<td>依靠服务器做处理，对硬件资源的要求高</td>
<td>效率约等于IP设备本身效率</td>
</tr>
<tr>
<td>扩展性</td>
<td>扩展性和通用性好</td>
<td>扩展性较差，需对各种应用协议进行定制开发</td>
<td>通用性好，但适用范围有限</td>
</tr>
<tr>
<td>商用性</td>
<td>在Web加速领域使用较多</td>
<td>国内流媒体CDN应用较多</td>
<td>尚无商用案例</td>
</tr>
</tbody></table>
<h1 id="流媒体CDN系统的组成"><a href="#流媒体CDN系统的组成" class="headerlink" title="流媒体CDN系统的组成"></a>流媒体CDN系统的组成</h1><ul>
<li>流媒体业务是一种对实时性、连续性、时序性要求非常高的业务，无论从带宽消耗上还是质量保障上来说，对best-effort的IP网络都是一个不小的冲击<ul>
<li>高带宽要求</li>
<li>高QoS要求</li>
<li>组播、广播要求（目前IP网络无法实现端到端的组播业务）</li>
</ul>
</li>
<li>播放一个视频分为以下四个步骤<ul>
<li>Access</li>
<li>Demux（音视频分离）</li>
<li>Decode（解码解压缩）</li>
<li>Output</li>
</ul>
</li>
<li>RTP、RTCP、RTSP、RTMP的关系：RTSP协议用来实现远程播放控制，RTP用来提供时间信息和实现流同步，RTCP协助RTP完成传输质量控制&lt;=（播放控制），</li>
<li>=&gt;（传输控制）RTMP和HTTP streaming则是将流同步、播放控制、质量控制集成起来的企业自有流媒体传送协议</li>
<li>RTMP是adobe的传输协议。RTMP的基本通信单元：消息块（chunk）和消息（message）</li>
<li>RTMP协议架构在TCP层之上，但RTMP消息并不是直接封装在TCP中，而是通过一个被称为消息块的封装单元进行传输。消息在网络上发送之前往往需要分割成多个较小的部分，这样较小的部分就是消息块，属于不同消息流的消息块可以在网络上交叉发送。</li>
<li>RTSP/RTP和HTTP streaming是目前应用最广泛的流化协议，目前电信运营商在IPTV（特殊通道的基于IP的流媒体播放）的流化上主要以RTSP/RTP技术为主，而互联网视频网站（点播/直播）则多倾向于使用HTTP streaming的流化技术。</li>
<li>HTTP streaming前身是progressive download（渐进式下载：边下载边播放，直到下载完）。HTTP streaming首先会将视频数据（包括直播的视频流和点播的视频文件）在服务器上进行编码，然后将编码后的数据进行更细粒度的分片，再把每个分片通过 HTTP协议传输到客户端。HTTP streaming的客户端需要对视频文件的每个分片都发出一个HTTP请求，这样，在视频播放速度低于下载速度的情况下，客户端可以灵活控制HTTP请 求的发出速度，从而保证用户在中途退出时不会出现下载浪费。另外，因为采用分片的特点，HTTP streaming还可以实现媒体播放过程中的码率切换（码率自适应），结合网络带宽资源，为用户提供更好的体验。</li>
</ul>
<table>
<thead>
<tr>
<th><strong>HTTP streaming</strong></th>
<th><strong>Progressive download</strong></th>
</tr>
</thead>
<tbody><tr>
<td>支持点播、直播</td>
<td>仅支持点播</td>
</tr>
<tr>
<td>可对分片文件加密，保证数字版权</td>
<td>直接把媒体文件分割成多个小文件分片，无法保障版权所有</td>
</tr>
<tr>
<td>因为分片传输，故支持码率自适应</td>
<td>只支持固定码率</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th><strong>HTTP streaming</strong></th>
<th><strong>RTSP/RTP</strong></th>
</tr>
</thead>
<tbody><tr>
<td>基于TCP，更高可靠性，也可以直接利用TCP的流控机制来适应带宽的变化</td>
<td>基于UDP</td>
</tr>
<tr>
<td>可将播放过的内容保存在客户端</td>
<td>不能保存在客户端</td>
</tr>
<tr>
<td>使用80端口，能穿越防火墙</td>
<td>使用特殊端口</td>
</tr>
<tr>
<td>采用标准的HTTP协议来传输，只需要标准的HTTP服务器支撑</td>
<td>需要特殊的流媒体服务器</td>
</tr>
</tbody></table>
<ul>
<li>HTTP streaming的几个主流阵营：<ul>
<li>3GPP adaptive HTTP Streaming</li>
<li>Microsoft IIS Smooth Streaming</li>
<li>Adobe HTTP Dynamic Streaming (HDS)</li>
<li>Apple HTTP Live Streaming (HLS)</li>
</ul>
</li>
<li>HLS流化技术主要分三个部分：服务器组件、分发组件和客户端软件<ul>
<li>服务器组件主要负责从原始的音视频设备捕捉相应的音视频流，并对这些输入的媒体流进行编码，然后进行封装和分片，最后交付给分发组件来进行传送；</li>
<li>分发组件主要负责接收客户端发送的请求，然后将封装的流媒体分片文件连同相关的索引文件一起发送给客户端。对于没有采用CDN服务的源服务器，标准的 Web服务器就是一个分发组件，而对于大型的视频网站或者类似的大规模应用平台，分发组件还应包括支持RTMP协议的CDN；</li>
<li>客户端软件负责确定应该请求的具体媒体流，下载相关资源，并在下载后通过拼接分片将流媒体重新展现给用户</li>
</ul>
</li>
<li>HLS音视频流或流媒体文件在经过编码、封装和分片后，变成多个以.ts结尾的分片文件。流分割器产生的索引文件是以.M3U8为后缀的，用户可以直接通过Web访问来获取</li>
<li>分发组件负责将分片文件和索引文件通过HTTP的方式发送给客户端，无须对现有的Web服务器和Cache设备进行额外的扩展、配置和升级</li>
<li>客户端组件根据URL来获取这个视频的索引文件。索引文件包含了可提供分片文件的具体位置、解密密钥以及可用的替换流。</li>
<li>HDS，点播内容是通过一个简单的预编码生成MP4片段以及Manifest清单文件；直播的内容准备工作流程相对复杂一点，在播放的过程中生成MP4.（直播推荐用RTMP，使用FMS推流器）</li>
<li>MPEG-2 TS是指TS格式封装的、MPEG-2编码格式的媒体流。大多数IPTV系统使用这种内容源。H.264这一层完成原始文件的压缩编码，TS这一层负责音 视频的复用以及同步，RTP这一层负责流的顺序传输，UDP这一层负责数据包的交付，IP层负责传输路由选择</li>
<li>流媒体加速的回源要求：因为流媒体文件传送带宽需求高，而且往往需要维持TCP长连接，所以一旦CDN回源比例过高，源 站服务器I/O将不堪负荷。CDN对内容采取分发方式分为pull和push两种。Pull是被动下拉的方式，push是主动推送的方式。对于流媒体内 容，系统一般会选择对热点内容采取push方式的预分发，而普通的网页内容几乎100%是pull方式的。</li>
<li>在流媒体CDN系统中，用户访问的调度会更多考虑内容命中，主要是因为流媒体内容文件体积大，业务质量要求高，如果从其 他节点拉内容再向用户提供服务会带来额外的延迟，影响用户体验。为进一步提高命中率，流媒体CDN系统普遍采用了对热点内容实施预先push的内容分发策 略</li>
<li>在流媒体服务系统中，主要关注的技术是对不同流媒体协议、不同编码格式、不同播放器、不同业务质量要求等的适应。</li>
<li>流媒体CDN与Web CDN的对比（业务差异）</li>
</ul>
<table>
<thead>
<tr>
<th><strong>主要差异点</strong></th>
<th><strong>流媒体CDN</strong></th>
<th><strong>Web CDN</strong></th>
</tr>
</thead>
<tbody><tr>
<td>内容类型</td>
<td>大文件、实时流、QoS要求高</td>
<td>小文件、固定大小、QoS要求低</td>
</tr>
<tr>
<td>用户行为</td>
<td>拖曳、暂停等播放控制</td>
<td>下载后浏览</td>
</tr>
<tr>
<td>内容管理</td>
<td>内容冷热度差异明显（对命中率要求高），内容生命周期长</td>
<td>内容冷热度差异不明显，内容生命周期短</td>
</tr>
<tr>
<td>回源要求</td>
<td>回源比例小</td>
<td>回源比例大</td>
</tr>
</tbody></table>
<ul>
<li>现在已经投入商用的CDN系统，基本都是同时提供Web CDN能力和流媒体CDN能力的，而且这两种能力的实现在系统内部几乎都是互相隔离的，从调度系统到节点设备都没有交叉互用</li>
<li>流媒体CDN与Web CDN的设计差异（设计差异）</li>
</ul>
<table>
<thead>
<tr>
<th><strong>主要差异点</strong></th>
<th><strong>流媒体CDN</strong></th>
<th><strong>Web CDN</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Cache</td>
<td>支持多种流化协议，硬件配置大存储、高I/O</td>
<td>支持多协议（HTTP、FTP等）硬件配置小存储、高性能CPU</td>
</tr>
<tr>
<td>负载均衡</td>
<td>DNS+HTTP重定向方式</td>
<td>DNS方式</td>
</tr>
<tr>
<td>内容分发方式</td>
<td>热片PUSH，冷片PULL</td>
<td>全PULL方式</td>
</tr>
<tr>
<td>组网</td>
<td>多级组网，可能要求组播、单播混合组网</td>
<td>两级组网</td>
</tr>
</tbody></table>
<ul>
<li>流媒体CDN的Cache设备与Web Cache无论在软件实现还是硬件要求上差异都很大，我们很少看到这两种业务共用同一台设备</li>
<li>当用户请求的内容在Cache上命中时，Cache直接向用户提供流服务，此时Cache设备充当流媒体服务器的角色； 当用户请求内容未能在Cache上命中时，Cache会从上一级Cache（二级缓存设备或中间缓存设备）或者源站服务器获取内容，再提供给用户。 Cache在用户与另一个流媒体服务器之间扮演代理的角色</li>
<li>分布式存储技术因其大容量、低成本的特点，目前也被业界关注和研究作为流媒体CDN系统的存储解决方案之一。常用的分布 式存储技术包括分布式文件系统和分布式数据库，由于采用了数据副本冗余（每份数据复制2~3份）、磁盘冗余（Raid1、Raid10、Raid5）等技 术，通常可以提供良好的数据容错机制，当单台存储设备断电或者单个存储磁盘失效时，整个存储系统仍能正常工作</li>
<li>负载均衡设备在进行用户访问调度时，会综合考虑很多静态的、动态的参数，包括IP就近性、连接保持、内容命中、响应速 度、连接数等。但没有哪个CDN会考虑所有参数，而是会根据业务特点进行一些取舍，否则均衡系统就太复杂了。而流媒体CDN在进行用户访问调度时，会更多 考虑内容命中这一参数</li>
<li>有两种GSLB实现方式，一种是基于DNS的，一种是基于应用层重定向的</li>
<li>PUSH方式适合内容访问比较集中的情况，如热点的影视流媒体内容，PULL方式比较适合内容访问分散的情况</li>
<li>对使用CDN服务的SP来说，CDN的作用在于尽量就近为用户提供服务，帮助SP解决长距离IP传输和跨域传输带来的种 种业务质量问题（通过空间换取时间）。因此，为用户提供服务的Cache设备一定部署在离用户比较近的地方。另一方面，CDN的建设者从成本角度考虑，又 不能把所有内容都存放在这些离用户最近的节点中，这会消耗大量存储成本，所以这些提供服务的Cache设备会根据需要从源站服务器或者其他Cache获取 内容。这样就形成了CDN网络分层部署的概念。</li>
<li>从网络分层上看，Web CDN通常是两级架构（也有三级架构以减少回源），即中心-边缘。而流媒体CDN通常有三级以上架构，即中心-区域-边缘。产生这种区别的原因在于流媒体 回源成本比较高，源站服务器响应一次流媒体内容回源请求，要比Web内容回源消耗更多资源。尤其对于流媒体直播业务来说，只要直播节目没结束，服务器就需 要长时间持续吐流，如果没有第二层节点作为中继，那么中心节点的压力将是不可想象的。</li>
<li>分层部署的方式，对点播业务而言的主要意义是节省存储成本，对直播业务而言在于减少带宽成本。在点播业务中，边缘Cache只需存储用户访问量大的内容或者内容片断，其余内容存储在区域Cache中。</li>
<li>在直播业务中，边缘Cache从区域中心获取直播流，而不需要直接向中心节点（源站）获取，从而节省了区域中心到中心节点这一段的大部分带宽。因为直播流在各个Cache中都不需要占用很大的存储空间，只需少量缓存空间即可，所以直播业务方面并不用注重考虑存储成本</li>
<li>考虑到电信运营商的IP拓扑和流量模型，区域中心Cache通常部署在重点城市的城域网出口的位置，以保障向各个边缘 Cache的链路通畅。边缘Cache的位置选择则以整个节点能够提供的并发能力为主要依据，依据业务并发数收敛比，计算出单个Cache需要覆盖的用户 规模，从而选择一个合适的部署位置。当然，边缘Cache离用户越近，服务质量越好，但覆盖的用户数越少，部署成本越高。</li>
<li>内容文件预处理</li>
<li>是指视频内容进入CDN以后，进入内容分发流程之前，CDN系统对内容进行的一系列处理过程。这个预处理过程的目的有几个：<ul>
<li>为全网内容管理提供依据，比如对内容进行全网唯一标识，对内容基础信息进行记录等</li>
<li>为提高CDN服务效率或降低系统成本提供手段，比如内容切片</li>
<li>为满足业务要求提供能力，比如对同一内容进行多种码率的转换以满足动态带宽自适应或三屏互动业务要求</li>
</ul>
</li>
<li>视频转码(video transcoding)</li>
<li>码率转换<ul>
<li>空间分辨率转换</li>
<li>时间分辨率转换</li>
<li>编码格式转换。编码格式主要包括H.264、MPEG-4、MPEG-2、VC-1、REAL、H.263、WMV。通常是把其他编码格式转换成H.264</li>
</ul>
</li>
<li>文件切片<ul>
<li>是指按照一定的规则把一个完整的文件切成大小一致的若干个小文件；由于流媒体CDN需要提供的内容体积越来越大，传统整片存储带来的成本消耗超出了CDN服务商的承受范围；切片的另一个目的是，使边缘Cache能够支持自适应码率业务</li>
</ul>
</li>
<li>防盗链机制和实现<ul>
<li>基于IP的黑白名单</li>
<li>利用HTTP header的referer字段</li>
<li>使用动态密钥（随机生成的key通过算法生成新的url）</li>
<li>在内容中插入数据（对有版权内容进行加密（DRM），如Microsoft的playready，Google的Widevine）</li>
<li>打包下载：在原文件的基础上进一步封装，使得资源的hash 值改变</li>
</ul>
</li>
</ul>
<h1 id="动态内容加速服务的实现"><a href="#动态内容加速服务的实现" class="headerlink" title="动态内容加速服务的实现"></a>动态内容加速服务的实现</h1><ul>
<li>随着Web2.0的兴起，产生了动态网页、个性化内容、电子交易数据等内容的加速，这些就涉及了动态内容加速技术。</li>
<li>静态内容的加速，都是对于表现层的加速，对于动态页面等内容的加速，则要涉及逻辑层和数据访问层的加速技术</li>
<li>动态内容的提供不仅仅是HTML页面的设计及编辑，它还需要有后台数据库、应用逻辑程序的支持，以实现与用户的动态交互。</li>
<li>Web系统由表现层、业务逻辑层、数据访问层+用户数据层</li>
<li>表现层是Web系统与外部系统的交互界面，这一层通常由HTTP服务器组成，负责接收用户端的HTTP内容访问请求，从文件系统中读取静态文件</li>
<li>业务逻辑层负责处理所有业务逻辑和动态内容的生成</li>
<li>数据访问层位于系统的后端，负责管理Web系统的主要信息和数据存储，通常由数据库服务器和存储设备组成</li>
<li>用户数据层负责存储用户信息数据和关联关系，内容来自用户提供和用户行为分析结果</li>
<li>Web网站借助CDN技术能够获得更好的扩展性和高性能，核心在于CDN采用的缓存（caching）和复制（replication）机制，其中缓存是将最近经常被访问的源服务器拥有的内容复制到边缘服务器上，可被视为具有特定策略的复制。</li>
<li>CDN的复制机制是指将源Web系统逻辑架构的各个层次的相应功用复制到边缘服务器上实现，以缓解源系统的处理压力。<ul>
<li>Web系统表现层的复制，就是静态内容的复制。边缘服务器又被称为代理服务器，通过反向代理加速静态文件的交付</li>
<li>Web系统业务逻辑层的复制。CDN被用于改进动态生成内容的交付性能。即将应用程序和业务组件直接在CDN的边缘服务器中计算，从而直接在靠近用户的地方生成动态Web内容</li>
<li>– Akamai边缘计算部署模型，包括用户（使用浏览器）、企业J2EE应用系统（运行业务逻辑、原有系统、数据库等）、分布式网络服务器（Edge computing平台）运行支持J2EE应用编程模型的WebSphere或者Tomcat应用服务器</li>
<li>Web系统数据访问层复制。CDN边缘服务器能够具备生成动态内容和掌管内容生成数据的能力</li>
<li>– 利用边缘服务器代替源钻Web系统的后台数据访问层中的数据库系统，及时响应业务逻辑层提出的数据查询需求。</li>
<li>Web系统用户文件的复制。</li>
</ul>
</li>
<li>应用加速技术实际上是传统的网络负载均衡的升级和扩展，综合使用了负载均衡（智能调度）、TCP优化管理（TCP keep-alive connection，更激进的TCP窗口策略，基于HTTP1.1），链接管理（routing）、SSL VPN、压缩优化（代码压缩，图片压缩）、智能网络地址（NAT-公私网IP转换）、高级路由、智能端口镜像等技术。）</li>
<li>TCP的问题<ul>
<li>TCP窗口大小的限制（TCP窗口大小随传输成功而变大，而一旦发生传输失败，其窗口大小会立即缩小）</li>
<li>TCP协议慢启动（三握手）和拥塞控制</li>
</ul>
</li>
<li>广域网加速关键技术</li>
</ul>
<table>
<thead>
<tr>
<th><strong>针对层次</strong></th>
<th><strong>优化技术</strong></th>
<th><strong>优化原理</strong></th>
</tr>
</thead>
<tbody><tr>
<td>传输发起端</td>
<td>原始数据优化</td>
<td>通过压缩、重复数据删除和字典等技术，可节省绝大多数传输数据量，节约带宽，提高服务器性能</td>
</tr>
<tr>
<td>数据缓存技术</td>
<td>将类HTTP的业务、图片、文字等缓存在本地，只传输动态内容，减少带宽占用</td>
<td></td>
</tr>
<tr>
<td>物理层（硬件）</td>
<td>提升设备性能</td>
<td>基于现有TCP/IP，通过硬件方式提高性能，提高大量TCP并发连接和会话重组等处理能力</td>
</tr>
<tr>
<td>网络层（IP）</td>
<td>QoS和流量控制</td>
<td>通过协议识别，实现在同一端口中不同应用的真正区分，进而通过分流实现时延敏感应用的带宽保障</td>
</tr>
<tr>
<td>传输层（TCP）</td>
<td>代理设备</td>
<td>在传输两端各架设代理设备，所有的响应报文都在本地完成，只有真正发起请求时才通过链路，相当于同时在服务器和客户端进行协议欺骗</td>
</tr>
<tr>
<td></td>
<td>TCP协议优化</td>
<td>通过在广域网两端部署专用设备，在不影响基本传输情况下，通过各种手段对TCP窗口、响应、启动等机制进行改进，从而提高协议机制的效率</td>
</tr>
<tr>
<td>应用层</td>
<td>应用代理（缓存）</td>
<td>将常用的应用程序缓存在本地并配置好，用户可不用在本地等待类似于认证等会话过程，而是直接开始下一个应用，实现流水作业</td>
</tr>
</tbody></table>
<ul>
<li>数据碎片化，就是在应用层将数据分成一个个小的数据块，便于后续的数据比对使用。广域网加速设备在传输数据前会将缓存中的数据与数据切块进行对比，从而找出那些数据是重复数据，不再发送，哪些数据是新鲜的、需要传输的数据。</li>
<li>数据压缩和指针技术一般是放在一起使用的，在对数据分段后，会对每一段数据生成一个数据指针，对于重复内容，只传输指针。在压缩算法设计上，要求同时兼顾数据压缩比和压缩/解压缩时间。</li>
<li>高速TCP传输技术<ul>
<li>自适应拥塞窗口</li>
<li>有限制地快速重传</li>
<li>连接池：通过维护一个预先建立好的TCP连接池，当有数据传输需求时，从连接池中挑选一条可用连接传输。</li>
</ul>
</li>
<li>SSL加速技术<ul>
<li>SSL加密是一种处理器密集型加密算法，如果用服务器软件处理会消耗大量CPU资源，一般会在提供业务能力的服务器外围部署专门的SSL加速设备，采用硬解密方式实现</li>
<li>SSL加密分对称秘钥和非对称秘钥（计算资源消耗更大）</li>
</ul>
</li>
<li>SSL的基本原理和实现<ul>
<li>可认证性（authentication）</li>
<li>隐私性（privacy）</li>
<li>完整性（integrity）</li>
<li>不可抵赖性（undeniability）：发送者不能自称没有发出过接受者从他那里收到的内容</li>
</ul>
</li>
<li>SSL加速<ul>
<li>通常是基于硬件的SSL加速</li>
<li>通过在服务器上安装一块SSL加速板卡，可有效分担服务器CPU处理SSL事务的压力</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2019/08/12/Kubernetes-架构/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/12/Kubernetes-架构/" itemprop="url">Kubernetes 架构</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-12T18:51:51+08:00">
                2019-08-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/K8S/" itemprop="url" rel="index">
                    <span itemprop="name">K8S</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Kubernetes-架构"><a href="#Kubernetes-架构" class="headerlink" title="Kubernetes 架构"></a>Kubernetes 架构</h1><p> Kubernetes借鉴了Brog的设计理念，比如Pod、Service、Lables和单Pod单IP等。整体架构如下所示</p>
<p><img src="https://i.loli.net/2019/08/12/Wcv4PLCFgJ26imV.png" alt="assets_-LDAOok5ngY4pc1lEDes_-La8Wy3SQAP-8onLZ7uT_-La8X6lf7UjRFyBsrhpw_components.png"></p>
<ul>
<li><p>etcd保存了集群的状态信息</p>
</li>
<li><p>kube-apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现机制</p>
</li>
<li><p>kube-controller-manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等</p>
</li>
<li><p>kube-scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上</p>
</li>
<li><p>kubelet 负责维持容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理</p>
</li>
<li><p>Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI），默认的容器运行时为 Docker；</p>
</li>
<li><p>kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/12/WyA6Jpo52BINwm8.png" alt="assets_-LDAOok5ngY4pc1lEDes_-La8Wy3SQAP-8onLZ7uT_-La8X6ljrf3pM1bbtQ_0_core-packages.png"></p>
<p>除了核心组件，还有一些推荐的 Add-ons：</p>
<ul>
<li><p>kube-dns 负责为整个集群提供 DNS 服务</p>
</li>
<li><p>Ingress Controller 为服务提供外网入口</p>
</li>
<li><p>Heapster 提供资源监控</p>
</li>
<li><p>Dashboard 提供 GUI</p>
</li>
<li><p>Federation 提供跨可用区的集群</p>
</li>
<li><p>Fluentd-elasticsearch 提供集群日志采集、存储与查询</p>
</li>
</ul>
<h2 id="分层架构"><a href="#分层架构" class="headerlink" title="分层架构"></a>分层架构</h2><p><img src="https://i.loli.net/2019/08/12/xEBCcQ1YHAhqgP3.png" alt="assets_-LDAOok5ngY4pc1lEDes_-La8Wy3SQAP-8onLZ7uT_-La8X6lnG-e8vfXmSbwl_core-ecosystem.png"></p>
<ul>
<li>核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境</li>
<li>应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等）</li>
<li>管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等）</li>
<li>接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦</li>
<li>生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴<ul>
<li>Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS 应用、ChatOps 等</li>
<li>Kubernetes 内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等</li>
</ul>
</li>
</ul>
<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><p><img src="https://i.loli.net/2019/08/12/vmC7XRnEJpOoHzb.png" alt="assets_-LDAOok5ngY4pc1lEDes_-La8Wy3SQAP-8onLZ7uT_-La8X6llZfuE65sGzUkc_core-apis.png"></p>
<h2 id="核心API"><a href="#核心API" class="headerlink" title="核心API"></a>核心API</h2><p><img src="https://i.loli.net/2019/08/12/BXUHhx5ofuyJsL2.jpg" alt="assets_-LDAOok5ngY4pc1lEDes_-La8Wy3SQAP-8onLZ7uT_-La8X6lhezSJ3ufUcmlH_14937095836427.jpg"></p>
<h2 id="生态系统"><a href="#生态系统" class="headerlink" title="生态系统"></a>生态系统</h2><p><img src="https://i.loli.net/2019/08/12/PO76KHBXYpUveaW.png" alt="assets_-LDAOok5ngY4pc1lEDes_-La8Wy3SQAP-8onLZ7uT_-La8X6ldNQWZNfl3OJ9M_architecture.png"></p>
<h1 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a>设计理念</h1><h2 id="API设计原则"><a href="#API设计原则" class="headerlink" title="API设计原则"></a>API设计原则</h2><ol>
<li><p>所有API都是声明式</p>
</li>
<li><p>API对象比西湖不而且可组合</p>
</li>
<li><p>高层API以操作意图为基础设计</p>
</li>
<li><p>底层API更具高层API的控制需要设计</p>
</li>
<li><p>尽量避免简单封装，不要有外部API无法显式知道内部隐藏的机制</p>
</li>
<li><p>API操作复杂度与对象数量成正比</p>
</li>
<li><p>API对象状态不能依赖于网络连接状态</p>
</li>
<li><p>尽量避免让操作机制依赖于全局状态，分布式系统中要保证全局状态的同步是非常困难的</p>
<h2 id="控制设计原则"><a href="#控制设计原则" class="headerlink" title="控制设计原则"></a>控制设计原则</h2></li>
</ol>
<ul>
<li>控制逻辑只依赖于当前状态</li>
<li>假设任何错误的可能，并做容错助理</li>
<li>尽量避免复杂状态机制，控制逻辑不要依赖无法监控的内部状态</li>
<li>假设任何操作都可能被任何操作对象拒绝，甚至错误解析</li>
<li>每个模块都可以在出错之后自动恢复</li>
<li>每个模块都可以在必要时优雅地降级服务<h2 id="架构设计原则"><a href="#架构设计原则" class="headerlink" title="架构设计原则"></a>架构设计原则</h2></li>
<li>只有apiserver可以直接访问etcd存储，其他服务必须通过KubernetesAPI来访问集群状态</li>
<li>单点故障不影响集群状态</li>
<li>在没有新请求的情况下，所有组件应该在故障恢复后继续执行上次最后收到的请求（比如网络分区或服务重启等）</li>
<li>所有组件都应该在内存中保持所需要的状态，apiserver将状态写入etcd存储，而其他组件则通过apiserver更新并监听所有的变化</li>
<li>优先使用事件监听而不是轮询<h1 id="核心技术概念和API对象"><a href="#核心技术概念和API对象" class="headerlink" title="核心技术概念和API对象"></a>核心技术概念和API对象</h1></li>
</ul>
<p>API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。</p>
<p>每个API对象都有3大类属性：<font color="red"><strong>元数据metadata、规范spec和状态status</strong></font>。元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。规范描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3；status描述了系统实际当前达到的状态（Status），例如系统当前实际的Pod副本数为2；那么复本控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3。</p>
<p>K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。</p>
<h2 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h2><ul>
<li>Master是集群控制节点，每个K8S集群里需要Master负责整个集群的管理和控制，K8S所有控制命令都是发给它，由其负责具体执行和调度。</li>
<li>Master上运行的关键进程<ul>
<li>Kubernetes API Server（Kube-apiserver）,提供哦你了HTTP Rest接口的关键服务进程，是K8S里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程</li>
<li>Kubernetes Controller Manager（kube-controller-manager）,K8S里所有资源对象的自动化控制中心，资源对象的“大总管”</li>
<li>Kubernetes Scheduler（kube-scheduler）,负责资源调度（Pod调度）的进程，相当于“调度室；</li>
<li>etcd Server进程，K8S集群资源对象的数据全部保存在etcd中<h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2></li>
</ul>
</li>
<li>Node节点是K8S集群中工作负载节点，每个Node都会被Master分配相应的工作负载</li>
<li>Node节点上运行的关键进程<ul>
<li>kubelet:负责Pod对应的容器创建、启停等任务，同时与Master节点密切协作，实现集群管理的基本功能</li>
<li>kube-proxy:实现K8S Service的通信与负载均衡机制的重要组件</li>
<li>Docker Engine: Docker引擎，负责容器的创建和管理工作<h2 id="POD"><a href="#POD" class="headerlink" title="POD"></a>POD</h2></li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/12/3TdApJyxroRDPg9.png" alt="assets_-LDAOok5ngY4pc1lEDes_-LM_rqip-tinVoiFZE0I_-LM_sIwx4E3-69Ml-jNW_pod.png"></p>
<ul>
<li>Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod对多容器的支持是K8s最基础的设计理念。</li>
<li>Pod是K8s集群中所有业务类型的基础，可以看作运行在K8s集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。</li>
<li>目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、DaemonSet和StatefulSet。<h2 id="Replication-controller（RC）"><a href="#Replication-controller（RC）" class="headerlink" title="Replication controller（RC）"></a>Replication controller（RC）</h2></li>
<li>RC是K8S系统中核心概念之一，定义了一个期望的场景，即生命某种Pod的副本数量在任意时刻都符合某个预期值。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。</li>
<li>RC的都能够以包括如下<ul>
<li>Pod期待的副本数（replicas）</li>
<li>用于筛选目标Pod的Label Selector</li>
<li>当Pod的副本书两小于预期数量的时候，用于创建新Pod的Pod模板（template）</li>
</ul>
</li>
<li>RC是K8s较早期的技术概念，<font color="green"><strong>只适用于长期伺服型的业务类型</strong></font>，比如控制小机器人提供高可用的Web服务。<h2 id="Replica-Set（RS）"><a href="#Replica-Set（RS）" class="headerlink" title="Replica Set（RS）"></a>Replica Set（RS）</h2></li>
<li>RS 是新一代RC，提供同样的高可用能力，能够支持更多种类的匹配模式。RS对象一般不单独使用，而是作为Deployment的理想状态参数使用。<h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2></li>
<li>解决Pod的编排问题</li>
<li>部署是一个比RS应用模式更广的API对象，可以是创建一个服务，更新一个服务，或者滚动升级一个服务（滚动升级实际是创建新的RS，然后逐渐在新的RS中副本增加到理想状态，然后逐步将旧的RS中的副本减少到0）</li>
</ul>
<h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p> <img src="https://i.loli.net/2019/08/12/wlO8dfNvkq3r9sy.png" alt="assets_-LDAOok5ngY4pc1lEDes_-LM_rqip-tinVoiFZE0I_-LM_sIx5Qc9S275Z49_6_14731220608865.png"></p>
<ul>
<li>RC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。</li>
<li>要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的后端服务实例。在K8S集群中，客户端需要访问的服务就是Service对象。</li>
<li><strong>每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务</strong></li>
<li>*<em>在K8S集中微服务的负载均衡是由Kube-proxy实现的 *</em></li>
<li>Kube-proxy是K8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多<h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2></li>
<li>Job是K8s用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job管理的Pod根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。<h2 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h2></li>
<li>后台支撑型服务的核心关注点在K8s集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类Pod运行。节点可能是所有集群节点也可能是通过nodeSelector选定的一些特定节点。</li>
<li>典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支撑K8s集群运行的服务<h2 id="StatefulSett"><a href="#StatefulSett" class="headerlink" title="StatefulSett"></a>StatefulSett</h2></li>
<li>RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了、名字和启动在哪儿都不重要，重要的只是Pod总数。</li>
<li>StatefulSet是用来控制有状态服务，StatefulSet中的每个Pod的名字都是事先确定的，不能更改</li>
<li>对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）</li>
<li>对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务</li>
<li>适合于StatefulSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务</li>
</ul>
<h2 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h2><ul>
<li>Pod中能够被多个容器访问的共享目录，与Pod生命周期相同，与容器生命周期不相关</li>
<li>类型：<ol>
<li>emptyDir ：初始内容为空，无需指定宿主机上对应目录文件，临时空间</li>
<li>hostpath ：Pod上挂载宿主机上的文件或目录</li>
<li>gcePersistentDisk:Pod上的内容会被永久保存，即使Pod删除（node节点在GCE环境）</li>
<li>awsElasticBlockStore:同上，AWS云环境</li>
<li>NFS</li>
<li>其他类型：iscsi、glusterfs、rbd、gitRepo、flocker、secret(加密)</li>
</ol>
</li>
</ul>
<h2 id="Persistent-Volume"><a href="#Persistent-Volume" class="headerlink" title="Persistent Volume"></a>Persistent Volume</h2><ul>
<li>PV只能是网络存储，不属于任何Node，但可以在每个Node上访问</li>
<li>PV并不定义在Pod上，而是独立于Pod之外定义</li>
<li>PV目前只有几种类型：GCE Persistent Disks、NFS、RBD、iSCSI、AWSEBS、GlusterFS等</li>
</ul>
<h2 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h2><ul>
<li>集群内部的资源对象”分配“到不同的Namespace中，形成逻辑上分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源同时还能被分别管理</li>
</ul>
<h2 id="Label（标签）"><a href="#Label（标签）" class="headerlink" title="Label（标签）"></a>Label（标签）</h2><ul>
<li>Label是一个Key=value的键值对，key和value由用户自己定义。Label可以附加到各种资源对象上，例如Node、Pod、Service、RS等，一个资源对象可以定义任意数量的Label，同一label也可以被添加到任意数量的资源对象上，label可以在定义资源对象是创建，也可以创建后动态添加或删除</li>
<li>通过LabelSelector(标签选择器)查询和筛选，K8S通过这种方式实现类似SQL的简单又通用的对象查询机制</li>
</ul>
<h2 id="Horizontal-Pod-Autoscaler（HPA）"><a href="#Horizontal-Pod-Autoscaler（HPA）" class="headerlink" title="Horizontal Pod Autoscaler（HPA）"></a>Horizontal Pod Autoscaler（HPA）</h2><ul>
<li>Pod横向自动扩容，通过追踪分析RS控制的所有目标Pod的负载变化情况，确定是否需要针对性调整目标Pod的副本数</li>
<li>度量指标<ul>
<li>CPUUtilizationPercentage （需要部署安装Heapster）</li>
<li>应用程序自定义的度量指标，比如服务在每秒内的相应的请求数（TPS或QPS）</li>
</ul>
</li>
</ul>
<h2 id="RBAC访问授权"><a href="#RBAC访问授权" class="headerlink" title="RBAC访问授权"></a>RBAC访问授权</h2><ul>
<li>基于角色的访问控制（Role-based Access Control，RBAC）的授权模式</li>
</ul>
<h1 id="核心组件-1"><a href="#核心组件-1" class="headerlink" title="核心组件"></a>核心组件</h1><p> ##组件通信<br>   K8S多组件之间通信原理</p>
<p><img src="https://i.loli.net/2019/08/12/GzcCtOFyV7pnHBX.png" alt="assets_-LDAOok5ngY4pc1lEDes_-LM_rqip-tinVoiFZE0I_-LM_sEq_NuMALezRGMtG_workflow.png"></p>
<ul>
<li>apiserver 负责etcd存储的所有操作，且只有apiserver才直接操作etcd集群</li>
<li>apiserver 对内（集群中的其他组件）和对外（用户）提供统一的REST API，其他组件均通过apiserver进行通信<ul>
<li>controller manager、scheduler、kube-proxy和kubelet等均通过apiserver watch API检测资源变化情况，并对资源作相应的操作</li>
<li>所有需要更新资源状态的操作均通过apiserver的REST API进行</li>
</ul>
</li>
<li>apiserver也会直接调用kubelet API（如logs,exec,attach等），默认不校验kubelet证书，看可以通过–kubelet-certificate-autprity开启开启（GKE通过SSH隧道保护他们之间通信）<br>比如典型的创建Podcast的流程为</li>
</ul>
<p><img src="https://i.loli.net/2019/08/12/eg53NY8LvBa91q7.png" alt="assets_-LDAOok5ngY4pc1lEDes_-LM_rqip-tinVoiFZE0I_-LM_sEqUTx_-gUuaPyw1_components.png"></p>
<ol>
<li><p>用户通过REST API创建一个Pod </p>
</li>
<li><p>apiserver 将其写入etcd</p>
</li>
<li><p>scheduler检测到未绑定Node的Pod，开始调度并更新Pod的Node绑定</p>
</li>
<li><p>kubelet检测到有新的Podcast调度过来，通过container runtime运行该Pod</p>
</li>
<li><p>kubelet通过container runtime取到Pod状态，并更新到apiserver中</p>
<h2 id="端口号"><a href="#端口号" class="headerlink" title="端口号"></a>端口号</h2></li>
</ol>
<h3 id="Master-1"><a href="#Master-1" class="headerlink" title="Master"></a>Master</h3><table>
<thead>
<tr>
<th align="center"><strong>Protocol</strong></th>
<th align="center"><strong>Direction</strong></th>
<th align="center"><strong>PortRange</strong></th>
<th align="center"><strong>Purpose</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">6443*</td>
<td align="center">Kubernetes API server</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">8080</td>
<td align="center">Kubernetes API insecure server</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">2379-2380</td>
<td align="center">etcd server client API</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10250</td>
<td align="center">Kubelet API</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10251</td>
<td align="center">kube-scheduler healthz</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10252</td>
<td align="center">kube-controller-manager healthz</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10253</td>
<td align="center">cloud-controller-manager healthz</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10255</td>
<td align="center">Read-only Kubelet API</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10256</td>
<td align="center">kube-proxy healthz</td>
</tr>
</tbody></table>
<h3 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h3><table>
<thead>
<tr>
<th align="center"><strong>Protocol</strong></th>
<th align="center"><strong>Direction</strong></th>
<th align="center"><strong>PortRange</strong></th>
<th align="center"><strong>Purpose</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">4194</td>
<td align="center">Kubelet cAdvisor</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10248</td>
<td align="center">Kubelethealthz</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10249</td>
<td align="center">kube-proxy metrics</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10250</td>
<td align="center">Kubelet API</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10255</td>
<td align="center">Read-only Kubelet API</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">10256</td>
<td align="center">kube-proxy healthz</td>
</tr>
<tr>
<td align="center">TCP</td>
<td align="center">Inbound</td>
<td align="center">30000-32767</td>
<td align="center">NodePort Services**</td>
</tr>
</tbody></table>
<h2 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h2><p>Etcd 是 CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。</p>
<h2 id="kube-controller-manager"><a href="#kube-controller-manager" class="headerlink" title="kube-controller-manager"></a>kube-controller-manager</h2><p><img src="https://i.loli.net/2019/08/12/RyPTzkrf1qw5VKg.png" alt="assets_-LDAOok5ngY4pc1lEDes_-L_kZB_hPn0h_fskH43w_-L_kZGkSetdfkFl6OL6N_post-ccm-arch.png"></p>
<p>Controller Manager 由 kube-controller-manager 和 cloud-controller-manager 组成，是 Kubernetes 的大脑，它通过 apiserver 监控整个集群的状态，并确保集群处于预期的工作状态。<br>Kube-controller-manager 由一系列的控制器组成<br><img src="https://i.loli.net/2019/08/13/5GwXKSWfODuFaVt.png" alt="20170721232653797.png"></p>
<ul>
<li>Replication Controller</li>
<li>Node Controller</li>
<li>CronJob Controller</li>
<li>Daemon Controller</li>
<li>Deployment Controller</li>
<li>Endpoint Controller</li>
<li>Garbage Collector</li>
<li>Namespace Controller</li>
<li>Job Controller</li>
<li>Pod AutoScaler</li>
<li>RelicaSet</li>
<li>Service Controller</li>
<li>ServiceAccount Controller</li>
<li>StatefulSet Controller</li>
<li>Volume Controller</li>
<li>Resource quota Controller<br>cloud-controller-manager 在 Kubernetes 启用 Cloud Provider 的时候才需要，用来配合云服务提供商的控制，也包括一系列的控制器，如</li>
<li>Node Controller</li>
<li>Route Controller</li>
<li>Service Controller<h2 id="kube-scheduler"><a href="#kube-scheduler" class="headerlink" title="kube-scheduler"></a>kube-scheduler</h2>kube-scheduler 负责分配调度 Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配 Node 的 Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod 的 NodeName 字段）<br>调度器需要充分考虑诸多的因素：</li>
<li>公平调度</li>
<li>资源高效利用</li>
<li>QoS</li>
<li>affinity 和 anti-affinity</li>
<li>数据本地化（data locality）</li>
<li>内部负载干扰（inter-workload interference）</li>
<li>deadlines<br>有三种方式指定 Pod 只运行在指定的 Node 节点上</li>
<li>nodeSelector：只调度到匹配指定 label 的 Node 上</li>
<li>nodeAffinity：功能更丰富的 Node 选择器，比如支持集合操作</li>
<li>podAffinity：调度到满足条件的 Pod 所在的 Node 上<h2 id="kube-apiserver"><a href="#kube-apiserver" class="headerlink" title="kube-apiserver"></a>kube-apiserver</h2></li>
<li>提供集群管理的 REST API 接口，包括认证授权、数据校验以及集群状态变更等</li>
<li>提供其他模块之间的数据交互和通信的枢纽（其他模块通过 API Server 查询或修改数据，只有 API Server 才直接操作 etcd）<h2 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h2>每个节点上都运行一个 kubelet 服务进程，默认监听 10250 端口，接收并执行 master 发来的指令，管理 Pod 及 Pod 中的容器。每个 kubelet 进程会在 API Server 上注册节点自身信息，定期向 master 节点汇报节点的资源使用情况，并通过 cAdvisor 监控节点和容器的资源。<h3 id="节点管理"><a href="#节点管理" class="headerlink" title="节点管理"></a>节点管理</h3>节点管理主要是节点自注册和节点装填更新：<ul>
<li>Kubelet可以通过设置启动参数–register-node来确定是否向API Server注册</li>
<li>如果Kubelet没有选择自注册模式，则需要用户自己配置Node资源信息，同时需要告知Kubelet集群上的API Server的位置</li>
<li>Kubelet在启动时通过API Server注册节点信息，并定时向API Server发送节点新消息，API Server在接受到新消息后，将信息写入etcd<h3 id="Pod管理"><a href="#Pod管理" class="headerlink" title="Pod管理"></a>Pod管理</h3></li>
<li>Kubelet 以 PodSpec 的方式工作。PodSpec 是描述一个 Pod 的 YAML 或 JSON 对象。 kubelet 采用一组通过各种机制提供的 PodSpecs（主要通过 apiserver），并确保这些 PodSpecs 中描述的 Pod 正常健康运行<h3 id="Static-Pod"><a href="#Static-Pod" class="headerlink" title="Static Pod"></a>Static Pod</h3></li>
<li>静态Pod是由kubelet进行管理的仅存在于特定Node上的Pod。不能通过API Server进行管理，无法与RC、Deployment或者DaemonSet进行管理，并且kubelet无法对其进行健康检查。</li>
<li>静态Pod总是由kubelet进行创建，并且总是在kuelet所在的Node上运行</li>
<li>所有以非 API Server 方式创建的 Pod 都叫 Static Pod。Kubelet 将 Static Pod 的状态汇报给 API Server，API Server 为该 Static Pod 创建一个 Mirror Pod 和其相匹配。Mirror Pod 的状态将真实反映 Static Pod 的状态。当 Static Pod 被删除时，与之相对应的 Mirror Pod 也会被删除。</li>
<li><h3 id="容器健康检查"><a href="#容器健康检查" class="headerlink" title="容器健康检查"></a>容器健康检查</h3></li>
</ul>
</li>
</ul>
<ol>
<li>LivenessProbe 探针：用于判断容器是否健康，告诉 Kubelet 一个容器什么时候处于不健康的状态。如果 LivenessProbe 探针探测到容器不健康，则 Kubelet 将删除该容器，并根据容器的重启策略做相应的处理。如果一个容器不包含 LivenessProbe 探针，那么 Kubelet 认为该容器的 LivenessProbe 探针返回的值永远是 “Success”</li>
<li>ReadinessProbe：用于判断容器是否启动完成且准备接收请求。如果 ReadinessProbe 探针探测到失败，则 Pod 的状态将被修改。Endpoint Controller 将从 Service 的 Endpoint 中删除包含该容器所在 Pod 的 IP 地址的 Endpoint 条目。<br>livenessProbe 包含如下三种实现方式：</li>
</ol>
<ul>
<li><p>ExecAction：在容器内部执行一个命令，如果该命令的退出状态码为 0，则表明容器健康；</p>
</li>
<li><p>TCPSocketAction：通过容器的 IP 地址和端口号执行 TCP 检查，如果端口能被访问，则表明容器健康；</p>
</li>
<li><p>HTTPGetAction：通过容器的 IP 地址和端口号及路径调用 HTTP GET 方法，如果响应的状态码大于等于 200 且小于 400，则认为容器状态健康。</p>
<h3 id="cAdvisor-资源监控"><a href="#cAdvisor-资源监控" class="headerlink" title="cAdvisor 资源监控"></a>cAdvisor 资源监控</h3><p>Kubernetes集群中，应用程序的执行情况可以在不同的级别上检测到，包括：容器、Pod、Servvice和整个集群。Heapster项目为K8S提供了一个基本监控平台，它是集群级别的监控事件数据集成器（Aggregator）.Heapster以Pod的方式运行在集群中，Heapster通过Kubelet发现所有运行在集群中的节点，并查看这些节点的资源使用情况。Kubelet通过cAdvisor获取其所在节点及容器的数据。</p>
<ul>
<li>cAdvisor 是一个开源的分析容器资源使用率和性能特性的代理工具，已集成到 Kubernetes 代码中。</li>
<li>cAdvisor 自动查找所有在其所在节点上的容器，自动采集 CPU、内存、文件系统和网络使用的统计信息。</li>
<li>cAdvisor 通过它所在节点机的 Root 容器，采集并分析该节点机的全面使用情况。</li>
<li>cAdvisor 通过其所在节点机的 4194 端口暴露一个简单的 UI<h3 id="Container-Runtime"><a href="#Container-Runtime" class="headerlink" title="Container Runtime"></a>Container Runtime</h3>负责真正管理镜像和容器的生命周期。Kubelet通过Container Runtime Interface（CRI）与容器云形式交互，以管理镜像和容器。</li>
<li>拆分成Sandbox和Container的gPRC接口，并将镜像管理和容器管理分离到不同的服务</li>
</ul>
<p><img src="https://i.loli.net/2019/08/13/FDv8ZWuRi3X5SCg.png" alt="assets_-LDAOok5ngY4pc1lEDes_-LOmrfw3UHfIY1g0xnLo_-LOmroTRjl0FBixIYL5x_cri.png"></p>
<h2 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h2></li>
</ul>
<ul>
<li>每台node上都运行一个kube-proxy服务，监听API server和endpoint的变化情况，并通过iptables等来为服务配置负载均衡（仅支持TCP和UDP）</li>
<li>kube-proxy可以直接运行在物理机上，也可以在static pod或者daemonset的方式运行</li>
<li>实现方式<ul>
<li>userspace:用户控件监听端口，所有服务通过iptables转发到这个端口，然后在其内部负载均衡到实际pod</li>
<li>iptables:完全以iptables规则的方式实现service负载均衡（服务多的时候，产生太多iptables规则）</li>
<li>ipvs：解决iptables模式性能问题，增量式更新，保证service更新期间连接不断开</li>
<li>应用层的转发机制通过services是无法实现的，需要借助Ingress将不同URL的访问请求转发到后端不同的service<h2 id="kube-dns"><a href="#kube-dns" class="headerlink" title="kube-dns"></a>kube-dns</h2></li>
</ul>
</li>
<li>目前推荐CoreDNS替代kube-dns(skydns)</li>
<li>DNS 格式<ul>
<li>Service <ul>
<li>A record (cluster IP 或Pod IP列表)</li>
<li>SRV record</li>
</ul>
</li>
<li>Pod<ul>
<li>A record:</li>
<li>指定hostname和subdomain</li>
</ul>
</li>
</ul>
</li>
<li>工作原理<ul>
<li>kube-dns：DNS 服务的核心组件，主要由 KubeDNS 和 SkyDNS 组成<ul>
<li>KubeDNS 负责监听 Service 和 Endpoint 的变化情况，并将相关的信息更新到 SkyDNS 中</li>
<li>SkyDNS 负责 DNS 解析，监听在 10053 端口 (tcp/udp)，同时也监听在 10055 端口提供 metrics</li>
<li>kube-dns 还监听了 8081 端口，以供健康检查使用</li>
</ul>
</li>
<li>dnsmasq-nanny：负责启动 dnsmasq，并在配置发生变化时重启 dnsmasq<ul>
<li>dnsmasq 的 upstream 为 SkyDNS，即集群内部的 DNS 解析由 SkyDNS 负责</li>
</ul>
</li>
<li>sidecar：负责健康检查和提供 DNS metrics（监听在 10054 端口）</li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/13/kV82oOjNUX6dQzC.png" alt="assets_-LDAOok5ngY4pc1lEDes_-LM_tX5rWaCuzsx7xEcz_-LM_t_Oxw2Twn_BSf7GQ_kube-dns.png"></p>
<h1 id="资源管理对象"><a href="#资源管理对象" class="headerlink" title="资源管理对象"></a>资源管理对象</h1><h2 id="Node-1"><a href="#Node-1" class="headerlink" title="Node"></a>Node</h2><p>Node 是 Pod 真正运行的主机，可以是物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 container runtime（比如 docker 或者 rkt）、kubelet 和 kube-proxy 服务。</p>
<h3 id="Node管理"><a href="#Node管理" class="headerlink" title="Node管理"></a>Node管理</h3><ul>
<li>不像其他的资源（如 Pod 和 Namespace），Node 本质上不是 Kubernetes 来创建的，Kubernetes 只是管理 Node 上的资源。虽然可以通过 Manifest 创建一个 Node 对象（如下 yaml 所示），但 Kubernetes 也只是去检查是否真的是有这么一个 Node，如果检查失败，也不会往上调度 Pod。</li>
<li>这个检查是由 Node Controller 来完成的。Node Controller 负责<ul>
<li>维护 Node 状态 </li>
<li>与 Cloud Provider 同步 Node</li>
<li>给 Node 分配容器 CIDR</li>
<li>删除带有 NoExecute taint 的 Node 上的 Pods<h3 id="Node状态"><a href="#Node状态" class="headerlink" title="Node状态"></a>Node状态</h3></li>
</ul>
</li>
<li>地址：包括 hostname、外网 IP 和内网 IP</li>
<li>条件（Condition）：包括 OutOfDisk、Ready、MemoryPressure 和 DiskPressure</li>
<li>容量（Capacity）：Node 上的可用资源，包括 CPU、内存和 Pod 总数</li>
<li>基本信息（Info）：包括内核版本、容器引擎版本、OS 类型等</li>
</ul>
<h2 id="PersistentVolume"><a href="#PersistentVolume" class="headerlink" title="PersistentVolume"></a>PersistentVolume</h2><p>PersistentVolume (PV) 和 PersistentVolumeClaim (PVC) 提供了方便的持久化卷：<strong>PV 提供网络存储资源，而 PVC 请求存储资源</strong>。这样，设置持久化的工作流包括配置底层文件系统或者云数据卷、创建持久性数据卷、最后创建 PVC 来将 Pod 跟数据卷关联起来。PV 和 PVC 可以将 pod 和数据卷解耦，pod 不需要知道确切的文件系统或者支持它的持久化引擎。</p>
<h3 id="Volume-生命周期"><a href="#Volume-生命周期" class="headerlink" title="Volume 生命周期"></a>Volume 生命周期</h3><p>Volume 的生命周期包括 5 个阶段</p>
<ol>
<li>Provisioning，即 PV 的创建，可以直接创建 PV（静态方式），也可以使用 StorageClass 动态创建</li>
<li>Binding，将 PV 分配给 PVC</li>
<li>Using，Pod 通过 PVC 使用该 Volume，并可以通过准入控制 StorageObjectInUseProtection（1.9 及以前版本为 PVCProtection）阻止删除正在使用的 PVC</li>
<li>Releasing，Pod 释放 Volume 并删除 PVC</li>
<li>Reclaiming，回收 PV，可以保留 PV 以便下次使用，也可以直接从云存储中删除</li>
<li>Deleting，删除 PV 并从云存储中删除后段存储<br>根据这 5 个阶段，Volume 的状态有以下 4 种</li>
</ol>
<ul>
<li>Available：可用</li>
<li>Bound：已经分配给 PVC</li>
<li>Released：PVC 解绑但还未执行回收策略</li>
<li>Failed：发生错误<h3 id="PV"><a href="#PV" class="headerlink" title="PV"></a>PV</h3>PersistentVolume（PV）是集群之中的一块网络存储。跟 Node 一样，也是集群的资源。PV 跟 Volume (卷) 类似，不过会有独立于 Pod 的生命周期。比如一个 NFS 的 PV 可以定义为<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: pv0003</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 5Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  persistentVolumeReclaimPolicy: Recycle</span><br><span class="line">  nfs:</span><br><span class="line">    path: /tmp</span><br><span class="line">    server: 172.17.0.2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>PV 的访问模式（accessModes）有三种：</p>
<ul>
<li>ReadWriteOnce（RWO）：是最基本的方式，可读可写，但只支持被单个节点挂载。</li>
<li>ReadOnlyMany（ROX）：可以以只读的方式被多个节点挂载。</li>
<li>ReadWriteMany（RWX）：这种存储可以以读写的方式被多个节点共享。不是每一种存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是 NFS。在 PVC 绑定 PV 时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。<br>PV 的回收策略（persistentVolumeReclaimPolicy，即 PVC 释放卷的时候 PV 该如何操作）也有三种</li>
<li>Retain，不清理, 保留 Volume（需要手动清理）</li>
<li>Recycle，删除数据，即 rm -rf /thevolume/*（只有 NFS 和 HostPath 支持）</li>
<li>Delete，删除存储资源，比如删除 AWS EBS 卷（只有 AWS EBS, GCE PD, Azure Disk 和 Cinder 支持）<h3 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h3>StorageClass 包括四个部分</li>
<li>provisioner：指定 Volume 插件的类型，包括内置插件（如 kubernetes.io/glusterfs）和外部插件（如 external-storage 提供的 ceph.com/cephfs）。</li>
<li>mountOptions：指定挂载选项，当 PV 不支持指定的选项时会直接失败。比如 NFS 支持 hard 和 nfsvers=4.1 等选项。</li>
<li>parameters：指定 provisioner 的选项，比如 kubernetes.io/aws-ebs 支持 type、zone、iopsPerGB 等参数。</li>
<li>reclaimPolicy：指定回收策略，同 PV 的回收策略。</li>
</ul>
<h3 id="PVC"><a href="#PVC" class="headerlink" title="PVC"></a>PVC</h3><p>PV 是存储资源，而 PersistentVolumeClaim (PVC) 是对 PV 的请求。PVC 跟 Pod 类似：Pod 消费 Node 资源，而 PVC 消费 PV 资源；Pod 能够请求 CPU 和内存资源，而 PVC 请求特定大小和访问模式的数据卷。</p>
<h2 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h2><p>Pod 是一组紧密关联的容器集合，它们共享 IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。</p>
<p>Pod 的特征</p>
<ul>
<li>包含多个共享 IPC、Network 和 UTC namespace 的容器，可直接通过 localhost 通信</li>
<li>所有 Pod 内容器都可以访问共享的 Volume，可以访问共享数据</li>
<li>无容错性：直接创建的 Pod 一旦被调度后就跟 Node 绑定，即使 Node 挂掉也不会被重新调度（而是被自动删除），因此推荐使用 Deployment、Daemonset 等控制器来容错</li>
<li>优雅终止：Pod 删除的时候先给其内的进程发送 SIGTERM，等待一段时间（grace period）后才强制停止依然还在运行的进程</li>
<li>特权容器（通过 SecurityContext 配置）具有改变系统配置的权限（在网络插件中大量应用）<h3 id="Pod-定义"><a href="#Pod-定义" class="headerlink" title="Pod 定义"></a>Pod 定义</h3>Pod 生命周期<br>Kubernetes 以 PodStatus.Phase 抽象 Pod 的状态（但并不直接反映所有容器的状态）。可能的 Phase 包括</li>
<li>Pending: Pod 已经在 apiserver 中创建，但还没有调度到 Node 上面</li>
<li>Running: Pod 已经调度到 Node 上面，所有容器都已经创建，并且至少有一个容器还在运行或者正在启动</li>
<li>Succeeded: Pod 调度到 Node 上面后成功运行结束，并且不会重启</li>
<li>Failed: Pod 调度到 Node 上面后至少有一个容器运行失败（即退出码不为 0 或者被系统终止）</li>
<li>Unknonwn: 状态未知，通常是由于 apiserver 无法与 kubelet 通信导致</li>
</ul>
<p>PodSpec 中的 restartPolicy 可以用来设置是否对退出的 Pod 重启，可选项包括 Always、OnFailure、以及 Never。</p>
<ul>
<li>Always：只要退出就重启</li>
<li>OnFailure：失败退出（exit code 不等于 0）时重启</li>
<li>Never：只要退出就不再重启</li>
</ul>
<h3 id="容器生命周期钩子"><a href="#容器生命周期钩子" class="headerlink" title="容器生命周期钩子"></a>容器生命周期钩子</h3><p>容器生命周期钩子（Container Lifecycle Hooks）监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数。支持两种钩子：</p>
<ul>
<li>postStart： 容器创建后立即执行，注意由于是异步执行，它无法保证一定在 ENTRYPOINT 之前运行。如果失败，容器会被杀死，并根据 RestartPolicy 决定是否重启</li>
<li>preStop：容器终止前执行，常用于资源清理。如果失败，容器同样也会被杀死<br>而钩子的回调函数支持两种方式：</li>
<li>exec：在容器内执行命令，如果命令的退出状态码是 0 表示执行成功，否则表示失败</li>
<li>httpGet：向指定 URL 发起 GET 请求，如果返回的 HTTP 状态码在 [200, 400) 之间表示请求成功，否则表示失败</li>
</ul>
<h2 id="ReplicaSet"><a href="#ReplicaSet" class="headerlink" title="ReplicaSet"></a>ReplicaSet</h2><ul>
<li>ReplicationController（也简称为 rc）用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而异常多出来的容器也会自动回收。ReplicationController 的典型应用场景包括确保健康 Pod 的数量、弹性伸缩、滚动升级以及应用多版本发布跟踪等。</li>
<li><ul>
<li>在新版本的 Kubernetes 中建议使用 ReplicaSet（也简称为 rs）来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector（ReplicationController 仅支持等式）。</li>
</ul>
</li>
<li><ul>
<li>虽然也 ReplicaSet 可以独立使用，但建议使用 Deployment 来自动管理 ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持 rolling-update 但 Deployment 支持），并且还支持版本记录、回滚、暂停升级等高级特</li>
</ul>
</li>
</ul>
<h2 id="Resource-Quota"><a href="#Resource-Quota" class="headerlink" title="Resource Quota"></a>Resource Quota</h2><p>资源配额（Resource Quotas）是用来限制用户资源用量的一种机制。<br>它的工作原理为</p>
<ul>
<li><strong>资源配额应用在 Namespace 上，并且每个 Namespace 最多只能有一个 ResourceQuota 对象</strong></li>
<li>开启计算资源配额后，创建容器时必须配置计算资源请求或限制（也可以用 LimitRange 设置默认值）</li>
<li>用户超额后禁止创建新的资源<h3 id="资源配额的类型"><a href="#资源配额的类型" class="headerlink" title="资源配额的类型"></a>资源配额的类型</h3></li>
<li>计算资源，包括 cpu 和 memory<ul>
<li>cpu, limits.cpu, requests.cpu</li>
<li>memory, limits.memory, requests.memory</li>
</ul>
</li>
<li>存储资源，包括存储资源的总量以及指定 storage class 的总量<ul>
<li>requests.storage：存储资源总量，如 500Gi</li>
<li>persistentvolumeclaims：pvc 的个数</li>
<li>.storageclass.storage.k8s.io/requests.storage</li>
<li>.storageclass.storage.k8s.io/persistentvolumeclaims</li>
<li>requests.ephemeral-storage 和 limits.ephemeral-storage （需要 v1.8+）</li>
</ul>
</li>
<li>对象数，即可创建的对象的个数<ul>
<li>pods, replicationcontrollers, configmaps, secrets</li>
<li>resourcequotas, persistentvolumeclaims</li>
<li>services, services.loadbalancers, services.nodeports<h3 id="LimitRange"><a href="#LimitRange" class="headerlink" title="LimitRange"></a>LimitRange</h3>默认情况下，Kubernetes 中所有容器都没有任何 CPU 和内存限制。LimitRange 用来给 Namespace 增加一个资源限制，包括最小、最大和默认资源。</li>
</ul>
</li>
</ul>
<h2 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h2><p>StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为了无状态服务设计），其应用场景包括</p>
<ul>
<li>稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC实现</li>
<li>稳定的网络标识，即Pod重新调度后期PodName和HostName不变，基于Headless Service（即没有Cluster IP的service）来实现</li>
<li>有序部署，有序扩展，即Pod是由顺序的，在部署或者扩展的时候要依据定义的顺序依次次序进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于init containers来实现</li>
<li>有序收缩，有序删除（即从 N-1 到 0）</li>
</ul>
<p>从上面的应用场景可以发现，StatefulSet 由以下几个部分组成：</p>
<ul>
<li>用于定义网络标志（DNS domain）的 Headless Service</li>
<li>用于创建 PersistentVolumes 的 volumeClaimTemplates</li>
<li>定义具体应用的 StatefulSet</li>
</ul>
<h2 id="Volume-1"><a href="#Volume-1" class="headerlink" title="Volume"></a>Volume</h2><blockquote>
<ul>
<li>默认情况下容器的数据都是非持久化的，在容器消亡以后数据也跟着丢失，所以 Docker 提供了 Volume 机制以便将数据持久化存储。类似的，Kubernetes 提供了更强大的 Volume 机制和丰富的插件，解决了容器数据持久化和容器间共享数据的问题。</li>
</ul>
</blockquote>
<p>与 Docker 不同，<strong>Kubernetes Volume 的生命周期与 Pod 绑定</strong></p>
<ul>
<li>容器挂掉后 Kubelet 再次重启容器时，Volume 的数据依然还在</li>
<li>而 Pod 删除时，Volume 才会清理。数据是否丢失取决于具体的 Volume 类型，比如 emptyDir 的数据会丢失，而 PV 的数据则不会丢<h3 id="Volume-类型"><a href="#Volume-类型" class="headerlink" title="Volume 类型"></a>Volume 类型</h3></li>
</ul>
<h4 id="emptyDir"><a href="#emptyDir" class="headerlink" title="emptyDir"></a>emptyDir</h4><p>如果 Pod 设置了 emptyDir 类型 Volume， Pod 被分配到 Node 上时候，会创建 emptyDir，只要 Pod 运行在 Node 上，emptyDir 都会存在（容器挂掉不会导致 emptyDir 丢失数据），但是如果 Pod 从 Node 上被删除（Pod 被删除，或者 Pod 发生迁移），emptyDir 也会被删除，并且永久丢失。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pd</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: gcr.io/google_containers/test-webserver</span><br><span class="line">    name: test-container</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /cache</span><br><span class="line">      name: cache-volume</span><br><span class="line">  volumes:</span><br><span class="line">  - name: cache-volume</span><br><span class="line">    emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure>

<h4 id="hostPath"><a href="#hostPath" class="headerlink" title="hostPath"></a>hostPath</h4><p>hostPath 允许挂载 Node 上的文件系统到 Pod 里面去。如果 Pod 需要使用 Node 上的文件，可以使用 hostPath</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pd</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: gcr.io/google_containers/test-webserver</span><br><span class="line">    name: test-container</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /test-pd</span><br><span class="line">      name: test-volume</span><br><span class="line">  volumes:</span><br><span class="line">  - name: test-volume</span><br><span class="line">    hostPath:</span><br><span class="line">      path: /data</span><br></pre></td></tr></table></figure>

<h4 id="gcePersistentDisk"><a href="#gcePersistentDisk" class="headerlink" title="gcePersistentDisk"></a>gcePersistentDisk</h4><p>gcePersistentDisk 可以挂载 GCE 上的永久磁盘到容器，需要 Kubernetes 运行在 GCE 的 VM 中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - name: test-volume</span><br><span class="line">    # This GCE PD must already exist.</span><br><span class="line">    gcePersistentDisk:</span><br><span class="line">      pdName: my-data-disk</span><br><span class="line">      fsType: ext4</span><br></pre></td></tr></table></figure>

<h4 id="awsElasticBlockStore"><a href="#awsElasticBlockStore" class="headerlink" title="awsElasticBlockStore"></a>awsElasticBlockStore</h4><p>同gcePersistentDisk，环境为AWS</p>
<h4 id="nfs"><a href="#nfs" class="headerlink" title="nfs"></a>nfs</h4><p>NFS 是 Network File System 的缩写，即网络文件系统。Kubernetes 中通过简单地配置就可以挂载 NFS 到 Pod 中，而 NFS 中的数据是可以永久保存的，同时 NFS 支持同时写操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">- name: nfs</span><br><span class="line">  nfs:</span><br><span class="line">    # FIXME: use the right hostname</span><br><span class="line">    server: 10.254.234.223</span><br><span class="line">    path: &quot;/&quot;</span><br></pre></td></tr></table></figure>

<h4 id="gitRepo"><a href="#gitRepo" class="headerlink" title="gitRepo"></a>gitRepo</h4><p>gitRepo volume 将 git 代码下拉到指定的容器路径中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">- name: git-volume</span><br><span class="line">  gitRepo:</span><br><span class="line">    repository: &quot;git@somewhere:me/my-git-repository.git&quot;</span><br><span class="line">    revision: &quot;22f1d8406d464b0c0874075539c1f2e96c253775&quot;</span><br></pre></td></tr></table></figure>

<h4 id="使用subPath"><a href="#使用subPath" class="headerlink" title="使用subPath"></a>使用subPath</h4><p>Pod 的多个容器使用同一个 Volume 时，subPath 非常有用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: my-lamp-site</span><br><span class="line">spec:</span><br><span class="line">    containers:</span><br><span class="line">    - name: mysql</span><br><span class="line">      image: mysql</span><br><span class="line">      volumeMounts:</span><br><span class="line">      - mountPath: /var/lib/mysql</span><br><span class="line">        name: site-data</span><br><span class="line">        subPath: mysql</span><br><span class="line">    - name: php</span><br><span class="line">      image: php</span><br><span class="line">      volumeMounts:</span><br><span class="line">      - mountPath: /var/www/html</span><br><span class="line">        name: site-data</span><br><span class="line">        subPath: html</span><br><span class="line">    volumes:</span><br><span class="line">    - name: site-data</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: my-lamp-site-data</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2019/08/11/AWS-Direct-Connect-Overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/11/AWS-Direct-Connect-Overview/" itemprop="url">AWS Direct Connect Overview</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-11T13:45:05+08:00">
                2019-08-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS/" itemprop="url" rel="index">
                    <span itemprop="name">AWS</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Direct-Connect-Overview"><a href="#Direct-Connect-Overview" class="headerlink" title="Direct Connect Overview"></a>Direct Connect Overview</h2><ul>
<li>AWS Direct Connect是一种网络服务，它是一种替代Internet来利用AWS云服务的一种方案</li>
<li>通过标准的以太网光纤电缆将内部网络链接到 AWS Direct Connect 位置。电缆的一端接到用户侧路由器，另一端接到 AWS Direct Connect 路由器</li>
<li>可以使用1Gbps和10Gbps端口建立直接连接。可以从任何支持AWS Direct Connect的APN合作伙伴订购50Mbps，100Mbps，200Mbps，300Mbps，400Mbps和500Mbps的速度</li>
<li>有了此连接以后，可以创建直接连接到公共 AWS 服务（如 Amazon S3）或 Amazon VPC 的虚拟接口，从而绕过网络路径中的 Internet 服务提供商</li>
<li>AWS Direct Connect位置提供对与其关联的区域中的Amazon Web Services的访问，以及对其他美国区域的访问（如果是美国地区的Direct Connect）。例如，可以为美国的任何AWS Direct Connect位置配置单个连接，并使用它来访问所有美国区域和AWS GovCloud（美国）中的公共AWS服务。</li>
<li>每个AWS Direct Connect位置都可以连接到地理位置最近的AWS区域内的所有可用区</li>
<li>公有区域或AWS GovCloud (US)中的 AWS Direct Connect 位置可以访问任何其他公有区域（不包括中国(北京和宁夏)）中的公有服务<br><img src="https://i.loli.net/2019/08/11/ab2uJTrhVCylmHW.png" alt="direct_connect_overview.png"></li>
</ul>
<h2 id="Direct-Connect-Advantages"><a href="#Direct-Connect-Advantages" class="headerlink" title="Direct Connect Advantages"></a>Direct Connect Advantages</h2><ul>
<li>降低带宽成本<ul>
<li>所有通过专用连接传输的数据均按AWS直接连接数据传输速率(而不是Internet数据传输速率)的，从而节省成本</li>
<li>在AWS之间传输数据直接减少了Internet服务提供商的带宽承诺</li>
</ul>
</li>
<li>一致的网络性能<ul>
<li>与互联网的网络变化（抖动、延迟）相比，Direct Connect提供专用连接和更一致的网络性能体验</li>
</ul>
</li>
<li>AWS 服务兼容性<ul>
<li>Direct Connect是一种网络服务，可与S3，EC2和VPC等所有AWS服务配合使用</li>
</ul>
</li>
<li>VPC的专用连接<ul>
<li>使用直接连接专用虚拟接口（Private virtual interface）可以在网络和VPC之间建立专用的专用高带宽网络连接</li>
</ul>
</li>
<li>弹性<ul>
<li>使用更高带宽的连接或建立多个连接，可以轻松扩展直接连接以满足需求</li>
</ul>
</li>
</ul>
<h1 id="Direct-Connect-vs-IPSec-VPN-Connections"><a href="#Direct-Connect-vs-IPSec-VPN-Connections" class="headerlink" title="Direct Connect vs IPSec VPN Connections"></a>Direct Connect vs IPSec VPN Connections</h1><ul>
<li>VPC VPN连接通过IPSec在Internet上建立内部网和Amazon VPC之间的加密网络连接</li>
<li>VPN连接可以在几分钟内进行配置，而对于需要一个具有低到适度的带宽需求，并能容忍在基于互联网连接的内在变化</li>
<li>AWS Direct Connect不涉及Internet; 相反，它使用Intranet和Amazon VPC之间的专用专用网络连接</li>
<li>与Direct Connect连接相比，VPN连接非常便宜，因为它需要实际的硬件和基础设施，可能需要数千个</li>
</ul>
<h1 id="Direct-Connect-Anatomy"><a href="#Direct-Connect-Anatomy" class="headerlink" title="Direct Connect Anatomy"></a>Direct Connect Anatomy</h1><p><img src="https://i.loli.net/2019/08/11/GumT4j7R1sLoiHE.png" alt="screen-shot-2016-05-17-at-1-56-15-pm.png"></p>
<ul>
<li>亚马逊在不同地点维护AWS Direct Connect PoP（称为主机托管设施），这与AWS区域不同</li>
<li>AWS本身维护从AWS Direct Connect PoP到AWS区域的连接</li>
<li>消费者，既可以购买机架空间，也可以使用任何已在主机托管设施中拥有基础架构的AWS APN合作伙伴并配置客户网关</li>
<li>Direct Connect PoP与Colocation Facility内的Customer网关之间的连接称为Cross Connect</li>
<li>可以使用任何服务提供商网络建立从客户网关到客户数据中心的连接</li>
<li>使用AWS创建直接连接连接后，将收到LOA-CFA（授权书 - 连接设施分配）。</li>
<li>LOA-CFA可以切换到主机托管设施或APN合作伙伴以建立交叉连接</li>
<li>一旦建立了Cross Connect以及CGW和Customer DataCenter之间的连接，就可以创建虚拟接口</li>
<li>AWS Direct Connect需要VGW才能访问AWS VPC</li>
<li>虚拟接口<ul>
<li>每个AWS Direct Connect连接都需要一个虚拟接口</li>
<li>每个AWS Direct Connect连接都可以配置一个或多个虚拟接口。</li>
<li>可以创建公共虚拟接口（public virtual interface）以连接到例如公共资源。 SQS，S3，EC2，Glacier等只能公开到达</li>
<li>可以创建专用虚拟接口以连接到例如VPC私有IP地址的实例</li>
<li>每个虚拟接口都需要VLAN ID，接口IP地址，ASN和BGP密钥</li>
</ul>
</li>
<li>要将AWS Direct Connect连接与其他AWS账户一起使用，可以为该账户创建托管虚拟接口。 这些托管虚拟接口与标准虚拟接口的工作方式相同，可以连接到公共资源或VPC。</li>
</ul>
<h1 id="Direct-Connect-Redundancy"><a href="#Direct-Connect-Redundancy" class="headerlink" title="Direct Connect Redundancy"></a>Direct Connect Redundancy</h1><p><img src="https://i.loli.net/2019/08/11/WU7kjK2xcd3uslJ.png" alt="screen-shot-2016-05-17-at-1-57-22-pm.png"></p>
<ul>
<li>直接连接不提供冗余，并且有多个单点故障，因为每个连接都包含路由器端口和Amazon路由器之间的单个专用连接</li>
<li>冗余连接<ul>
<li>建立第二个DX，最好使用不同的路由器和AWS Direct Connect PoP在不同的主机托管设施中建立连接</li>
<li>客户DC与VGW之间的IPsec VPN连接</li>
</ul>
</li>
<li>对于在同一AWS Direct Connect位置中请求的多个端口，Amazon本身确保在冗余Amazon路由器上配置它们以防止硬件故障的影响</li>
</ul>
<h1 id="Direct-Connect-LAG"><a href="#Direct-Connect-LAG" class="headerlink" title="Direct Connect LAG"></a>Direct Connect LAG</h1><ul>
<li>链接聚合组 (LAG) 是一个逻辑接口，使用链接聚合控制协议 (LACP) 在一个 AWS Direct Connect 终端节点处聚合多个连接，从而允许将这些连接视为一个托管连接</li>
<li>可从现有连接创建 LAG，也可配置新连接</li>
<li>在创建 LAG 之后，可将现有连接 (无论是独立连接还是其他 LAG 的一部分) 与 LAG 关联</li>
<li>LAG遵循以下规则<ul>
<li>LAG 中的所有连接都必须使用相同的带宽</li>
<li>LAG 中最多可有 4 个连接。LAG 中的每个连接都会计入区域的整体连接限制</li>
<li>LAG 中的所有连接都必须终止于同一 AWS Direct Connect 终端节点<h4 id="refer"><a href="#refer" class="headerlink" title="refer"></a>refer</h4></li>
</ul>
</li>
</ul>
<ul>
<li>AWS Direct Connect user guide</li>
<li><a href="http://jayendrapatil.com/aws-direct-connect-dx" target="_blank" rel="noopener">http://jayendrapatil.com/aws-direct-connect-dx</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2019/08/11/AWS地理组件/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/11/AWS地理组件/" itemprop="url">AWS地理组件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-11T10:11:31+08:00">
                2019-08-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS/" itemprop="url" rel="index">
                    <span itemprop="name">AWS</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><font size="1">&gt; <em>前沿：本文主要描述AWS各种服务自身的作用范围（地理），AWS的各种服务所覆盖的地理区域包含全区域、Region、AZ等</em></font></p>
<h3 id="优先解释几个概念"><a href="#优先解释几个概念" class="headerlink" title="优先解释几个概念"></a>优先解释几个概念</h3><p><font color="red">AWS Regions &amp; Availability Zones</font></p>
<ul>
<li>Amazon服务托管在全球多个位置。Amazon提供将资源和数据放置在多个位置的能力，以提高性能、提供容错、高可用性和成本优化</li>
</ul>
<p><font color="red">Region</font></p>
<ul>
<li>AWS为客户提供了在多个地理区域(称为区域)中放置实例和存储数据的灵活性。每个区域都是一个定义地理位置上AWS资源的独立集合。</li>
<li>每个地区都是一个独立的地理区域，是完全独立的。</li>
<li>每个Amazon区域都被设计成与其他区域完全隔离，并帮助实现尽可能大的容错性和稳定性</li>
<li>区域间的通信是通过公共互联网进行的，应该采取适当的措施使用加密技术来保护数据</li>
<li>区域间的数据传输按Internet数据传输速率对发送和接收实例进行收费</li>
<li>除非显式复制，否则不会跨区域复制资源</li>
<li>一个地区的选择可以由许多因素驱动<ul>
<li>延迟—可以选择贴近目标用户群的区域，以减少数据延迟成本</li>
<li>AWS在所有地区提供相同的服务，但通常情况下，由于亚马逊的成本(由于土地、电力、带宽等)不同，服务的成本也会有所不同，因此在一个地区的服务成本可能会低于另一个地区</li>
</ul>
</li>
<li>法律遵从性——许多国家强制执行数据驻留在区域内的遵从性和法规要求（如欧盟的GDPR）</li>
<li>特性——由于并非所有区域都提供所有AWS特性和服务，因此区域选择可以依赖于该区域支持的服务</li>
</ul>
<p><font color="red"> Availability Zones</font></p>
<ul>
<li>每个区域由多个孤立的位置组成，这些位置称为可用性区域，每个可用性区域都运行在其物理上不同的、独立的基础设施上（风火水电完全独立），并且设计得非常可靠。</li>
<li>每个Region都有多个可用性区域。</li>
<li>每个AZ在物理上是相互隔离的（距离在KM以上），因此一个不常见的灾难，如火灾、地震，只会影响一个AZ</li>
<li>AZs在地理上相互分离，位于同一区域内，是一个独立的失效区。</li>
<li>AZ冗余地连接到多个运营商的链路。<br>区域中的AZ使用低延迟的私有链接连接，而不是通过公共internet。</li>
<li>多AZ，跨多个可用性区域的资源分布，特性可用于跨多个AZ分布实例，以提供高可用性。</li>
<li>AWS通过将每个帐户的可用性区域独立映射到标识符，确保资源分布在一个区域的可用性区域之间。例如，us-east-1区域的us-east-1a AZ可能与us-east-1a AZ位于不同的位置。无法协调帐户之间的可用性区域。</li>
</ul>
<p><font color="red">Edge Locations</font></p>
<ul>
<li>Edge location是AWS Cloud Front (CDN)面向全球提供数据加速访问的节点。</li>
<li>主要位于世界上大多数主要城市，CloudFront (CDN)使用这些位置向终端用户分发内容，以减少延迟。<h1 id="1-AWS-地理组件"><a href="#1-AWS-地理组件" class="headerlink" title="1. AWS 地理组件"></a>1. AWS 地理组件</h1></li>
</ul>
<ul>
<li>AWS 提供三种地理性组件：（<a href="https://infrastructure.aws/" target="_blank" rel="noopener">https://infrastructure.aws/</a> 获取最新信息，可查阅这个网站，具体网络连接，region内AZ数量，区域内的PoP节点都可以即时获得。）</li>
<li><strong>Regions</strong>：区域，即AWS提供云服务的一个区域，其目的是为了用户能就近接入，降低网络延迟。通常是一个城市的若干个AZ组成一个region。2016年，AWS 宣布在其全球region之间建设了100GbE 私有环网。</li>
<li><strong>Availability Zones</strong>：一个 region 内至少两个通常三个可用区，其用途是为了搭建高可用架构。一个比较常见的看法是一个AZ是一个数据中心。（其实这不尽然，有时候靠得非常近的几个数据中心也可以组成一个AZ。弗吉尼亚有6个AZ。部分AZ 超过30万台服务器。AZ拥有独立的包括电力和网络在内的基础设施等。）AZ 之间利用低延迟光纤网络互联，延迟控制在3ms以内，AZ内低于0.3ms。（目前所有新建区域的AZ都会保持3个及以上，北京区域属于特殊情况。）</li>
<li><strong>Edge Locations</strong>：指往往部署在大城市，以及主要人口汇聚区域的AWS 站点。它的主要作用是缓存数据，降低延迟。它们独立于region 和 AZ，数量比AZ多很多。它被多个AWS服务利用，比如AWS CloudFront 和 AWS Lambda@Edge。CloudFront 利用它来作为提供给用户分布在全球的接入点，通常称为Edge POP点。</li>
<li><strong>目前全球有21个区域，66个可用区，180个接入点（2019/07/23）</strong></li>
</ul>
<p><img src="https://i.loli.net/2019/08/11/PRfSLEvcx2Fe1h5.jpg" alt="001.jpg"><br><img src="https://i.loli.net/2019/08/11/vaZy2ekGKf37jiM.jpg" alt="003.jpg"></p>
<h1 id="2-AWS基础服务与地理组件关系"><a href="#2-AWS基础服务与地理组件关系" class="headerlink" title="2. AWS基础服务与地理组件关系"></a>2. AWS基础服务与地理组件关系</h1><p><img src="https://i.loli.net/2019/08/11/x1HuiApjYXPF6cR.jpg" alt="004.jpg"></p>
<p>例如：</p>
<ol>
<li>AWS 只有极少数全区域性的，不限于特定region，比如IAM、SES、S3 和 CloudFront（特殊区域除外，如AWS USA GOV、中国区域）</li>
<li>一些服务是覆盖区域的，其作用范围在某个特定区域内，比如经常使用的的S3、AMI</li>
<li>一些服务是覆盖可用区的，其作用范围在某可用区内，比如最常使用的 EC2和EBS</li>
</ol>
<p><img src="https://i.loli.net/2019/08/11/MxZpr82NsRJGl9O.jpg" alt="002.jpg"></p>
<h1 id="3-具体覆盖区域"><a href="#3-具体覆盖区域" class="headerlink" title="3. 具体覆盖区域"></a>3. 具体覆盖区域</h1> <style>
table {
    width: 100%; /*表格宽度*/
    max-width: 65em; /*表格最大宽度，避免表格过宽*/
    border: 1px solid #dedede; /*表格外边框设置*/
    margin: 15px auto; /*外边距*/
    border-collapse: collapse; /*使用单一线条的边框*/
    empty-cells: show; /*单元格无内容依旧绘制边框*/
}
table th,

table td {
  height: 35px; /*统一每一行的默认高度*/
  border: 1px solid #dedede; /*内部边框样式*/
  padding: 0 10px; /*内边距*/
          }
table tbody tr:nth-child(2n) {
    background: rgba(158,188,226,0.12); 
}

table tr:hover {
    background: #efefef; 
}




.table-area {
    overflow: auto;
}

  </style>






<table>
<thead>
<tr>
<th align="center">Service</th>
<th align="center">Global</th>
<th align="center">Region</th>
<th align="center">AZ</th>
<th align="center">subservices</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center">IAM</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Users, Groups, Roles, Accounts</td>
<td align="left">所有区域都可以使用相同的AWS帐户、用户、组和角色</td>
</tr>
<tr>
<td align="center">IAM</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Key Pairs</td>
<td align="left">关键对-全球或地区</td>
</tr>
<tr>
<td align="center">IAM</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Key Pairs</td>
<td align="left">IAM users 是与 AWS account 绑定的，不受限于某个region。</td>
</tr>
<tr>
<td align="center">IAM</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Key Pairs</td>
<td align="left">注：中国/美国 GOV区域除外，其都有自己独立的IAM账户体系，不与全球共用。</td>
</tr>
<tr>
<td align="center">IAM</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">Key Pairs</td>
<td align="left">Amazon EC2创建的密钥对是特定于该区域的</td>
</tr>
<tr>
<td align="center">IAM</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">RSA key pair</td>
<td align="left">RSA密钥对可以创建和上传，可以在所有区域使用。</td>
</tr>
<tr>
<td align="center">KMS</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">存储在创建区域，并且仅在该区域使用。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">VPC</td>
<td align="left">VPC是在一个区域内创建的</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">VPC 位于一个reigon内，且分布在该region的所有AZ内。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">VPC不能迁移至其它region，而只能新建。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">Subnet</td>
<td align="left">子网只能率属于AZ。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">Security groups</td>
<td align="left">安全组绑定到一个区域，并且只能分配给同一区域中的实例。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">VPC Endpoints</td>
<td align="left">您不能在不同区域的VPC和AWS服务之间创建端点。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">VPC Peering</td>
<td align="left">支持region内和跨region</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">-</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">NAT gateway</td>
<td align="left">NAT网关运行在AZ中</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">Virtual private gateway (VGW)</td>
<td align="left">VGW在Region范围内自身实现冗余度设计。但是，VPN 服务和DX 需要用户自配置其高可用设计。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">Internet gateway</td>
<td align="left">IGW在Region范围内自身实现高可用、冗余度设计，等同于有多条链路到达IGW。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">Elastic IP Address</td>
<td align="left">在区域内创建的弹性IP地址只能分配给区域内的实例。</td>
</tr>
<tr>
<td align="center">VPC</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">每个region有它自己的地址池，EIP 从该池中分配。</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">Resource Identifiers</td>
<td align="left">资源标识符属于Region范围内，每个资源标识符(如AMI ID、实例ID、EBS卷ID或EBS快照ID)都绑定到其区域，并且只能在创建资源的区域中使用。</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">Instances</td>
<td align="left">实例绑定到启动它的可用性区域。但是，请注意它的实例ID绑定到该区域。</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">EBS Volumes</td>
<td align="left">Amazon EBS卷绑定到它的可用性区域，并且只能附加到同一可用性区域（AZ）中的实例。</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">EBS Snapshot</td>
<td align="left">EBS快照绑定到它的区域，并且只能用于在同一区域创建卷，如果需要，必须从一个区域复制到另一个区域，可利用 Snapshot Copy 功能将其拷贝至其它region</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">AMIs （Aamzon Machine Images）</td>
<td align="left">AMI提供了启动EC2实例的模板</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">AMI与Amazon S3中文件所在的区域绑定。对于在不同区域使用AMI，可以将AMI复制到其他区域</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">AWS 提供 AMI Copy 功能来将某AMI 拷贝至其它region。</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">Auto Scaling</td>
<td align="left">自动扩展跨越同一区域内的多个可用性区域，但不能跨区域</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">ELB(Elastic Load Balancer)</td>
<td align="left">弹性负载均衡器在同一区域的多个可用性区域的多个实例之间分配流量</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">无法将 ELB 迁移至其它region，你只能在其它region中新建ELB实例。</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">SSH Public Keys</td>
<td align="left">保存在region内，AWS不跨region复制或同步keys。</td>
</tr>
<tr>
<td align="center">EC2</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">Placement Groups</td>
<td align="left">集群放置组部署在同一可用性区域内的实例组</td>
</tr>
<tr>
<td align="center">S3</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">S3 bucket  name是全球性，但数据是区域性的</td>
</tr>
<tr>
<td align="center">S3</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">S3桶是在选定的区域内创建的Bucket 中的数据物理地位于一个region内，但是可以从其它region上访问它，此时需要考虑到延时问题。</td>
</tr>
<tr>
<td align="center">S3</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">存储的对象跨可用性区域复制，以提供高持久性，但除非显式地进行跨区域复制，否则不会跨区域复制</td>
</tr>
<tr>
<td align="center">Glacier</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">要迁移 Glacier 中的数据的话，需要经过几个步骤：1. 将 Glacier 中的数据restore到 S3 中。2. 利用 S3 Copy 功能将数据拷贝至另一个region 3. 利用 S3 lifecycle policy 将 S3 中的数据转移到新的region的 Glacier 内 4. 将原region的 Glacier 中的数据删除。</td>
</tr>
<tr>
<td align="center">Glacier</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">注：目前S3可以向S3 Deep Archive 直接归档数据（如北京region到宁夏）</td>
</tr>
<tr>
<td align="center">EFS</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">有两种数据在region间的迁移途径。1. 将EFS中的数据拷贝至 EBS，然后利用 EBS Snapshot Copy 功能将数据拷贝至另一个region内，再将数据从 EBS 拷贝到 EFS 内。 2. 将 EFS 中的数据拷贝到 S3 中，然后将利用 S3 Cross-region Replication 功能将数据拷贝至另一个region，再从S3 拷贝到EFS。</td>
</tr>
<tr>
<td align="center">Route53</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">Route53服务是在AWS的边缘位置提供的，并且是全球性的</td>
</tr>
<tr>
<td align="center">RDS</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="left">RDS 实例有单可用区的，也有跨多AZ 的</td>
</tr>
<tr>
<td align="center">RDS</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">可利用 AWS Database Migration Serivce 进行跨区域迁移</td>
</tr>
<tr>
<td align="center">RDS</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">跨区域的手工数据迁移步骤：1. 停止transactions 2. 在一个临时 EC2 将 DB 中的数据导出为文件 3. 利用工具将文件拷贝至远端region的EC2内 4. 创建RDS实例 5. 导入数据文件</td>
</tr>
<tr>
<td align="center">ElastiCache</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">支持 Redis 和 Memcached</td>
</tr>
<tr>
<td align="center">ElastiCache</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">Redis 迁移方法：1. 给集群手工创建一个 backup 2. 将backup 导入 S3. 4. S3 bucket 复制到另一个region。 5. 在新的region 内从 S3 restore 数据，其过程包括创建一个新的 Redis cluster 然后导入数据。</td>
</tr>
<tr>
<td align="center">ElastiCache</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">Memcached 数据跨region 迁移方法：在新的region 内创建一个 cluster，然后从应用层做数据复制。</td>
</tr>
<tr>
<td align="center">RedShift</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">集群迁移：利用 RedShift cross-region snapshot 功能创建snapshot 并将它拷贝到新的region内，然后将snapshot restore 到集群。</td>
</tr>
<tr>
<td align="center">RedShift</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">表迁移：利用 RedShift Upload 功能将数据导入 S3，再利用 S3 Cross-region Replication 功能将数据复制到另一个region，再在另一个region内创建 RedShift 集群并利用 COPY 功能从S3 中导入数据。</td>
</tr>
<tr>
<td align="center">EMR</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">跨region 迁移 EMR：在新的 region 内新建 EMR Cluster，然后导入数据</td>
</tr>
<tr>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">如果数据在 S3 中，则利用 S3 cross-region replication 功能将数据迁移到新的 region 内</td>
</tr>
<tr>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">如果数据在 HDFS 内，择利用 S3DistCp 命令将HDFS 内的数据拷贝到 S3， 然后再利用 S3DistCp 命令将S3 中的数据拷贝到目标 HDFS 内。</td>
</tr>
<tr>
<td align="center">Elasticsearch</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">为 ES domain 创建一个 snapshot，它会被保存到 S3 内。再利用 S3 做跨region 复制。再在新region内将数据从S3 恢复到 Elasticsearch 中。</td>
</tr>
<tr>
<td align="center">SQS</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">SQS queues 位于region内。需要利用应用，将消息从源region 的 queues 中导入目的 region的 queues 内。</td>
</tr>
<tr>
<td align="center">SNS</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">SNS topics 位于region 内</td>
</tr>
<tr>
<td align="center">Auroa</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">在另一个region 内创建一个 Aurora Cluster 作为 Read Replica。一旦创建后，Amazon RDS 对原 Aurora cluster 做snapshot，然后将 snapshot 发送只 Read Replica。</td>
</tr>
<tr>
<td align="center">DynamoDB</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">所有数据对象都存储在同一区域内，并跨同一区域中的多个可用性区域复制</td>
</tr>
<tr>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">可以使用跨区域复制显式地跨区域复制数据对象</td>
</tr>
<tr>
<td align="center">WAF</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">WAF服务保护Web应用程序免受常见Web攻击，它是在AWS边缘位置提供的，并且是全局的</td>
</tr>
<tr>
<td align="center">CloudFront</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">CloudFront是全球内容交付网络(CDN)服务，</td>
</tr>
<tr>
<td align="center">Storage Gateway</td>
<td align="center">-</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">AWS存储网关适用范围在Region范围内，提供存储卷、快照和磁带数据的网关。</td>
</tr>
<tr>
<td align="center">SES</td>
<td align="center">Y</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="center">-</td>
<td align="left">SES 有 regional endpoint。你的应用既可以使用与它相同region内的 SES服务，也可以利用其它region内的SES服务。当然了，这里面需</td>
</tr>
</tbody></table>
<h4 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h4><ul>
<li>《Using Amazon Web Services for Disaster Recovery》</li>
<li>《Migrating AWS Resources to a New AWS Region》</li>
<li>《Building Fault-Tolerant Application on AWS》</li>
<li>刘世民 云计算 博客</li>
<li><a href="http://jayendrapatil.com/aws-global-vs-regional-vs-az-resources/" target="_blank" rel="noopener">http://jayendrapatil.com/aws-global-vs-regional-vs-az-resources/</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2019/08/10/AWS-VPC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/10/AWS-VPC/" itemprop="url">AWS VPC</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-10T11:30:44+08:00">
                2019-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AWS/" itemprop="url" rel="index">
                    <span itemprop="name">AWS</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><font size="2"> &gt; <em>前言：在 Amazon Web Services (AWS) 云中预置一个逻辑隔离的部分，可以在自己定义的虚拟网络中启动 AWS 资源。完全掌控您的虚拟联网环境，包括选择自己的 IP 地址范围、创建子网以及配置路由表和网络网关。在 VPC 中可以使用 IPv4 和 IPv6，因此能够轻松安全地访问资源和应用程序。</em></font></p>
<h2 id="VPC-Overview-amp-Components"><a href="#VPC-Overview-amp-Components" class="headerlink" title="VPC Overview &amp; Components"></a>VPC Overview &amp; Components</h2><font face="微软雅黑">

<ul>
<li>虚拟私有云（VPC）是专用于AWS账户的虚拟网络， 它在逻辑上与AWS云中的其他虚拟网络隔离</li>
<li>VPC允许用户选择IP地址范围，创建子网，以及配置路由表，网络网关和安全设置</li>
<li>VPC sizing<ul>
<li>创建 VPC 时将单个无类别互联网域路由 (CIDR) IP 地址范围指定为主 CIDR 块，默认 VPC 分配有 172.31.0.0/16 的 CIDR 范围（允许2 ^ 16（65536）IP地址可用）</li>
<li>允许的CIDR块大小介于以下两者之间<ol>
<li>/ 28网络掩码（最小值为2 ^ 4  -  16个可用IP地址）</li>
<li>/ 16网络掩码（最大为2 ^ 16  -  65536 IP地址）</li>
</ol>
</li>
</ul>
</li>
<li>可以分配来自专用（非公共路由）IP地址的CIDR块<ol>
<li>10.0.0.0  -  10.255.255.255（10/8前缀）</li>
<li>172.16.0.0  -  172.31.255.255（172.16 /12前缀）</li>
<li>192.168.0.0  -  192.168.255.255（192.168/16前缀）<ul>
<li>可以指定一系列可公开路由的IP地址;然而，VPC中公开路由的CIDR块目前不支持直接访问Internet(public subnet)</li>
<li>现在可以调整VPC的size</li>
<li>每个VPC都与使用相同CIDR块创建的任何其他VPC分开，即使它位于同一AWS账户中</li>
</ul>
</li>
</ol>
</li>
<li>VPC允许VPC对等连接与相同或不同AWS账户中的其他VPC</li>
<li>可以建立VPC与公司或家庭办公网络之间的连接，但是CIDR块不应重叠，例如： 具有CIDR 10.0.0.0/16的VPC可以与10.1.0.0/16公司网络通信，但如果尝试连接到10.0.37.0/16公司网络导致重叠IP地址，则连接将被丢弃。</li>
<li>VPC允许其中启动的实例设置租期选项。 默认情况下，租赁选项是共享的。 如果选择了专用选项，则其中的所有实例都将在覆盖单个实例租用设置的专用硬件上启动</li>
<li>只有在终止VPC中的所有实例并删除具有VPC的所有组件之后，才可能删除VPC，包括子网，安全组，网络ACL，路由表，Internet网关，VPC对等连接和DHCP选项</li>
<li>AWS VPC组件<br><img src="https://i.loli.net/2019/08/10/fLeUIt8TSy6vD5Q.png" alt="AWS-VPC-Components.png"><h2 id="IP-Address"><a href="#IP-Address" class="headerlink" title="IP Address"></a>IP Address</h2>在VPC中启动的实例可以分配私有，公共和弹性IP地址，并且是ENI（弹性网络接口）的属性<h3 id="Private-IP-Addresses"><a href="#Private-IP-Addresses" class="headerlink" title="Private IP Addresses"></a>Private IP Addresses</h3></li>
<li>私有IP地址无法通过Internet访问，并且只能用于VPC内的实例之间的通信</li>
<li>所有实例都在子网的IP地址范围内分配给默认网络接口(eth0)的私有IP地址</li>
<li>主（Primary）IP地址在其生命周期内与网络接口相关联，即使实例已停止并重新启动，仅在实例终止时才释放</li>
<li>可以将其他私有IP地址（称为辅助专用IP地址）分配给实例，这些可以从一个网络接口重新分配给另一个网络接口（非eth0）<h3 id="Public-IP-Addresses"><a href="#Public-IP-Addresses" class="headerlink" title="Public IP Addresses"></a>Public IP Addresses</h3></li>
<li>公共IP地址可通过Internet访问，可用于实例与Internet之间的通信，也可用于具有公共端点的其他AWS服务</li>
<li>为实例分配公共IP地址取决于是否为子网启用了公共IP属性</li>
<li>通过在创建实例期间启用公共IP，也可以将公共IP分配给实例，从而覆盖子网的公共IP寻址属性</li>
<li>公共IP地址是从AWS IP地址池分配的，它与AWS账户无关，因此在实例停止并重新启动或终止时会释放<h3 id="Elastic-IP-Address"><a href="#Elastic-IP-Address" class="headerlink" title="Elastic IP Address"></a>Elastic IP Address</h3></li>
<li>弹性IP地址是静态的，持久的公共IP地址，可以根据需要与实例关联或解除关联</li>
<li>弹性IP地址在VPC上分配，并由帐户拥有，除非被释放</li>
<li>可以为网络接口分配公共IP或弹性IP。 如果分配已具有公共IP，分配弹性IP的实例时，则会释放公共IP</li>
<li>弹性IP地址可以从一个实例移动到另一个实例，该实例可以位于同一帐户中的相同或不同VPC内</li>
<li>弹性IP收取费用，即使其并未与相关实例绑定或ENI相关联<h2 id="Elastic-NetworkInterface（ENI）"><a href="#Elastic-NetworkInterface（ENI）" class="headerlink" title="Elastic NetworkInterface（ENI）"></a>Elastic NetworkInterface（ENI）</h2></li>
<li>每个实例都附带默认的弹性网络接口（主网络接口eth0），不能与实例分离</li>
<li>ENI包含以下属性<ul>
<li>Primary 私有IP</li>
<li>一个或多个辅助私有IP地址</li>
<li>每个privateIP地址一个弹性IP地址</li>
<li>一个公共IP地址，可以在启动实例时自动分配给eth0的网络接口，仅在为eth0创建网络接口而不是使用现有ENI</li>
<li>一个或多个安全组</li>
<li>一个MAC地址</li>
<li>源/目标检查标志</li>
<li>其他说明</li>
</ul>
</li>
<li>ENI的属性归属ENI，不属于实例，可以与实力连接或分离，并重新连接到另一个实例，属性仍然有效。 当ENI从一个实例移动到另一个实例时，网络流量将重定向到新实例。</li>
<li>实例可以附加多个ENI，有着一下好处<ul>
<li>独立管理网络</li>
<li>在VPC中使用网络和安全设施</li>
<li>使实例在不同子网上具有不同工作负载/角色</li>
<li>性价比、高可用的解决方案<h2 id="Route-Tables"><a href="#Route-Tables" class="headerlink" title="Route Tables"></a>Route Tables</h2></li>
</ul>
</li>
<li>路由表定义的规则，称为路由，确定在何处从子网的网络流量将被路由</li>
<li>每个VPC都有一个隐式路由器来路由网络流量</li>
<li>每个VPC都有一个主路由表，可以创建多个自定义路由表</li>
<li>VPC中的每个子网必须一次与一个路由表相关联，而路由表可以有多个与之关联的子网</li>
<li>如果未明确地与路由表关联，则子网与主路由表隐式关联</li>
<li>每个路由表都包含一个本地路由，该路由允许在VPC内进行无法修改或删除的通信</li>
<li>路由优先级通过匹配路由表中与流量匹配的最具体路由来确定</li>
<li>路由表需要更新，已经定义的好的网关设备如IGW，虚拟专用网关，VPC对等，VPC端点，NAT设备等<h2 id="Internet-Gateways-IGW"><a href="#Internet-Gateways-IGW" class="headerlink" title="Internet Gateways-IGW"></a>Internet Gateways-IGW</h2></li>
<li>IGW是一种水平扩展，冗余且高度可用的VPC组件，允许在VPC和Internet中的实例之间进行通信</li>
<li>IGW不对网络流量设置带宽限制或成为故障风险点（国内公有云有网络出口限制，同时这里的带宽不限制，是在不超过AWS自身网络限制）</li>
<li>IGW有两个用途：在VPC路由表中为可路由Internet的流量提供网关目标，以及实例（未分配公有IP地址）执行网络地址转换（NAT）</li>
<li>实例访问Internet需要满足一下要求<ul>
<li>将IGW关联到VPC</li>
<li>子网具有指向IGW网关的路由表</li>
<li>实例应分配公共IP或弹性IP</li>
<li>与实例关联的安全组和AACL应该允许其流量通过<h2 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h2></li>
</ul>
</li>
<li>NAT设备允许私有子网中的实例连接到Internet或其他AWS服务，但阻止Internet与实例的连接</li>
<li>NAT设备不支持IPv6流量，而应使用egress-only Internet gateway网关<h2 id="Egress-only-Internet-gateway"><a href="#Egress-only-Internet-gateway" class="headerlink" title="Egress-only Internet gateway"></a>Egress-only Internet gateway</h2></li>
<li>像NAT网关使用，并且仅支持IPv6</li>
<li>Egress Only Internet Gateway网关是一种水平扩展，冗余且高度可用的VPC组件，允许通过IPv6从VPC中的实例向Internet进行出站通信，并防止Internet实例的IPv6连接</li>
<li>仅用于IPv6,如果要启用IPv4请使用NGW<h2 id="VPC-amp-Subnet-Sizing"><a href="#VPC-amp-Subnet-Sizing" class="headerlink" title="VPC &amp; Subnet Sizing"></a>VPC &amp; Subnet Sizing</h2></li>
<li>VPC支持IPv4和IPv6寻址，并且每个都具有不同的CIDR块大小限制</li>
<li>VPC 可以同时有 IPv4 和 IPv6 CIDR 块与其关联</li>
<li>对于 IPv6，VPC 使用 /56 的固定大小</li>
<li>通过向现有 VPC 添加四 (4) 个辅助 IPv4 IP 范围 (CIDR) 来扩展 VPC。可通过删除已添加到 VPC 的辅助 CIDR 块来缩小 VPC；不能更改 VPC 的 IPv6 地址范围的大小</li>
<li>限制<ul>
<li>允许的块大小介于/ 28~ / 16网络掩码之间</li>
<li>CIDR块不得与任何与VPC关联的现有CIDR块重叠</li>
<li>CIDR块不能与任何VPC路由表中的路由的CIDR范围相同或更大，例如， 对于CIDR块10.0.0.0/24，只能关联较小的CIDR块，如10.0.0.0/25<h2 id="VPC-Security"><a href="#VPC-Security" class="headerlink" title="VPC Security"></a>VPC Security</h2>提供三种功能，以用来提高和监控 Virtual Private Cloud (VPC) 的安全性</li>
</ul>
</li>
<li>安全组 充当实例的虚拟防火墙以控制入站和出站流量</li>
<li>网络访问控制列表 (ACL) 是 VPC 的一个可选安全层，可用作防火墙来控制进出一个或多个子网的流量</li>
<li>VPC 流日志这项功能，捕获有关传入和传出VPC 中网络接口的 IP 流量的信息<br><img src="https://i.loli.net/2019/08/10/fQTSim9weNAOWLR.png" alt="security-diagram.png"><h3 id="Security-Groups"><a href="#Security-Groups" class="headerlink" title="Security Groups"></a>Security Groups</h3></li>
<li>在实例级别而不是在子网级别执行</li>
<li>可以为子网中的每个实例分配不同的安全组</li>
<li>可以为实例分配5个安全组，每个安全组具有50个规则</li>
<li>安全组允许为实例的入站（入口）和出站（出口）流量添加或删除规则（授权或撤消访问）<ul>
<li>默认安全组不允许外部入站流量，但允许来自具有相同安全组的实例的入站流量</li>
<li>默认安全组允许所有出站流量</li>
<li>新安全组仅以允许所有流量离开实例的出站规则开始</li>
</ul>
</li>
<li>安全组只能指定允许规则，但不包含拒绝规则</li>
<li>安全组可以授予对特定CIDR范围或VPC中的另一个安全组或对等VPC的访问权限（需要VPC对等连接）</li>
<li>安全组被评估为完整或累积规则，其中最宽松的规则优先。对于例如 如果有一个允许从IP地址203.0.113.1访问TCP端口22（SSH）的规则和允许从每个人访问TCP端口22的另一个规则，则每个人都可以访问TCP端口22</li>
<li>安全组是<font color="red">有状态的</font> - 无论出站规则如何，都允许对允许的入站流量的响应流出，反之亦然。 因此，不需要响应的出站规则</li>
<li>安全组与ENI（网络接口）相关联</li>
<li>可以更改与实例关联的安全组，这会更改与主网络接口（eth0）关联的安全组，并且更改将立即应用于与安全组关联的所有实例<h4 id="Connection-Tracking"><a href="#Connection-Tracking" class="headerlink" title="Connection Tracking"></a>Connection Tracking</h4></li>
<li>由于安全组是有状态的，基于流量的连接状态应用规则以确定允许还是拒绝流量</li>
<li>无论出站安全组规则如何，都允许对入站流量的响应流出实例，反之亦然。</li>
<li>仅当入站请求没有明确的出站规则时才会维护连接跟踪（响应流量基于允许响应流量的入站或出站规则流动，而不是基于跟踪信息流动）</li>
<li>但是，如果是入站请求一个明确的出站规则，响应流量被允许出站规则的基础上，而不是跟踪连接</li>
<li>跟踪流量</li>
</ul>
<ol>
<li>对于除 TCP、UDP 或 ICMP 以外的协议，仅跟踪 IP 地址和协议编号</li>
<li>实例将流量发送到另一个主机 (主机 B)，并且在原始请求或响应的 600 秒内，主机 B 在单独的请求中发起到实例的同一类型的流量，则无论入站安全组规则如何，实例都将接受该请求，因为该流量被视为响应流量</li>
</ol>
<ul>
<li>确保所有入站流量均遵循防火墙规则，可以使用网络 ACL — <font color="red">网络 ACL 是无状态的</font>，因此不会自动允许响应流量<h3 id="NACLs"><a href="#NACLs" class="headerlink" title="NACLs"></a>NACLs</h3></li>
<li>网络访问控制列表 (ACL) 是 VPC 的一个可选安全层，可用作防火墙来控制进出一个或多个子网的流量</li>
<li>NACL作用于子网级别控制，并且适用于该子网中的所有实例</li>
<li>网络ACL具有单独的入站和出站规则，每个规则可以允许或拒绝流量默认ACL允许所有入站和出站流量</li>
<li>新创建的ACL拒绝所有入站和出站流量</li>
<li>子网只能分配1个ACL，如果没有明确关联，则会与默认NACL隐式关联</li>
<li>网络ACL是按顺序评估的编号规则列表</li>
<li>从编号最小的规则开始，确定是否允许流量进出与网络ACL关联的任何子网</li>
<li>例如如果你有一个允许全部的规则100和一个全部拒绝的规则110，则允许全部优先，所有流量都将被允许</li>
<li>网络ACL是无状态的;对允许的入站流量的响应受出站流量规则的约束（反之亦然），例如：如果从特定IP地址启用端口22上的入站SSH，则还需要为响应添加出站规则<h4 id="安全组-vs-ACL"><a href="#安全组-vs-ACL" class="headerlink" title="安全组 vs ACL"></a>安全组 vs ACL</h4><table>
<thead>
<tr>
<th>安全组</th>
<th>ACL</th>
</tr>
</thead>
<tbody><tr>
<td>在实例级别运行</td>
<td>在子网级别运行</td>
</tr>
<tr>
<td>仅支持允许规则</td>
<td>支持允许规则和拒绝规则</td>
</tr>
<tr>
<td>有状态：返回数据流会被自动允许，不受任何规则的影响</td>
<td>无状态：返回数据流必须被规则明确允许</td>
</tr>
<tr>
<td>决定是否允许数据流前评估所有规则</td>
<td>决定是否允许数据流时按照数字顺序处理所有规则</td>
</tr>
<tr>
<td>只有在启动实例的同时指定安全组、或稍后将安全组与实例关联的情况下，操作才会被应用到实例</td>
<td>自动应用到关联子网内的所有实例（因此不需要依靠用户指定安全组）</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="VPC-Flow-logs"><a href="#VPC-Flow-logs" class="headerlink" title="VPC Flow logs"></a>VPC Flow logs</h2><ul>
<li>VPC 流日志这项功能，捕获有关传入和传出 VPC 中网络接口的 IP 流量的信息。流日志数据可以发布到 Amazon CloudWatch Logs 和 Amazon S3。创建流日志后，可以在选定目标中检索和查看其数据</li>
<li>可以为 VPC、子网或网络接口创建流日志。如果为子网或 VPC 创建流日志，则会监视 VPC 或子网中的每个网络接口</li>
<li>流日志不会为网络接口捕获实时日志流(通常需要几分钟开始收集数据并发布到指定目标)</li>
<li>可以为其他 AWS 创建的网络接口创建流日志，例如 Elastic Load Balancing、Amazon RDS、Amazon ElastiCache、Amazon Redshift 和 Amazon WorkSpaces<h2 id="Subnets"><a href="#Subnets" class="headerlink" title="Subnets"></a>Subnets</h2></li>
<li>子网跨越单个可用区，不同的位置设计为AZ级别故障隔离</li>
<li>子网可以配置IGW以启用Internet通信，或虚拟专用网关（VPN）连接以启用与公司网络的连通</li>
<li>子网可以是公共的或私有的，它取决于它是否具有因特网连接，即能够通过IGW将流量路由到因特网</li>
<li>应为公共子网中的实例分配公共IP或弹性IP地址，以便能够与Internet通信</li>
<li>对于未连接到Internet的子网，但通过虚拟专用网关路由的流量称为VPN专属子网</li>
<li>可以将子网配置为默认为子网内启动的所有实例分配公共IP地址</li>
<li>Subnet sizing <ul>
<li>分配给子网的CIDR块可以与VPC CIDR相同，在这种情况下，只能在VPC中启动一个子网</li>
<li>分配给子网的CIDR块可以是VPC CIDR的子集，允许在VPC中启动多个子网</li>
<li>分配给子网的CIDR块不应重叠</li>
<li>允许的CIDR块大小介于两者之间<ul>
<li>/28网络掩码（最小值为2 ^ 4  -  16个可用IP地址）</li>
<li>16网络掩码（最大为2 ^ 16  -  65536 IP地址）</li>
</ul>
</li>
<li>AWS在每个子网中保留5个IP地址（前4个和后1个IP地址），这些地址不可用，无法分配给实例。 例如 对于具有CIDR块10.0.0.0/24的子网，保留以下五个IP<ul>
<li>10.0.0.0：网络地址</li>
<li>10.0.0.1：AWS为VPC路由器保留</li>
<li>10.0.0.2：由AWS保留，用于映射到亚马逊提供的DNS</li>
<li>10.0.0.3：AWS保留供将来使用</li>
<li>10.0.0.255：网络广播地址。 AWS不支持VPC中的广播，因此保留地址</li>
</ul>
</li>
</ul>
</li>
<li>子网路由<ul>
<li>每个子网都与一个路由表相关联</li>
</ul>
</li>
<li>子网安全<ul>
<li>可以使用安全组和NACL配置子网安全性</li>
<li>安全组在实例级别工作，NACL在子网级别工作<h2 id="Shared-VPCs"><a href="#Shared-VPCs" class="headerlink" title="Shared VPCs"></a>Shared VPCs</h2></li>
</ul>
</li>
<li>VPC 共享允许多个 AWS 账户在共享的集中式管理的 Amazon Virtual Private Cloud (VPC) 中创建自己的应用程序资源，如 Amazon EC2、Amazon Relational Database Service (RDS)、Amazon Redshift 群集和 AWS Lambda</li>
<li>在此模型中，拥有 VPC 的账户（所有者）与属于 AWS Organizations 中同一组织的其他账户（参与者）共享一个或多个子网</li>
<li>共享子网之后，参与者可以查看、创建、修改和删除与他们共享的子网中的应用程序资源。参与者无法查看、修改或删除属于其他参与者或 VPC 拥有者的资源<h2 id="VPC-Endpoints"><a href="#VPC-Endpoints" class="headerlink" title="VPC Endpoints"></a>VPC Endpoints</h2></li>
<li>VPC 终端节点能够将 VPC 创建专有链接连接到支持的 AWS 服务和 VPC 终端节点服务（由 PrivateLink 提供支持），而无需 Internet 网关、NAT 设备、VPN 连接或 AWS Direct Connect 连接。VPC 中的实例无需公有 IP 地址便可与服务中的资源通信。VPC 和其他服务之间的通信不会离开 Amazon 网络。</li>
<li>终端节点是虚拟设备。这些是水平扩展、冗余且具备高可用性的 VPC 组件，通过使用这些组件，可以在 VPC 中的实例与服务之间进行通信，而不会对网络通信带来可用性风险或带宽限制</li>
<li>端点当前不支持跨区域请求，请确保在与服务相同相同的区域中创建端点</li>
<li>VPC 终端节点有两种类型：接口终端节点 和 网关终端节点</li>
<li>VPC 终端节点策略是一种 IAM 资源策略，在创建或修改终端节点时可将它附加到终端节点</li>
<li>默认策略来允许对服务进行完全访问。终端节点策略不会覆盖或取代 IAM 用户策略或服务特定策略 (如 S3 存储桶策略)<h3 id="网关-VPC-终端节点"><a href="#网关-VPC-终端节点" class="headerlink" title="网关 VPC 终端节点"></a>网关 VPC 终端节点</h3></li>
<li>网关终端节点是一个网关，作为路由表中的指定路由的目标（用于发送到受支持的 AWS 服务的流量）</li>
<li>支持Amazon S3&amp;DynamoDB<br><img src="https://i.loli.net/2019/08/10/oQuy8DG35PSltWF.png" alt="AWS-VPC-Endpoints.png"></li>
<li>Configuration<ol>
<li>指定要在其中创建终端节点的 VPC 以及要连接到的服务</li>
<li>端点需要与Route表关联，并且不能修改路由表以删除路由条目。只能通过删除与Route表的Endpoint关联来删除它</li>
<li>路由会自动添加到每个路由表中，同时会添加一个指定服务的前缀列表 ID 的目的地 (pl-xxxxxxxx) 以及一个具有终端节点 ID 的目标 (vpce-xxxxxxxx)；例如：</li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th>目的地</th>
<th>目标</th>
</tr>
</thead>
<tbody><tr>
<td>10.0.0.0/16</td>
<td>本地</td>
</tr>
<tr>
<td>pl-1a2b3c4d</td>
<td>vpce-11bb22cc</td>
</tr>
</tbody></table>
<ol start="4">
<li>端点策略可以控制对其他服务中的资源的访问</li>
<li>需要修改安全组以允许从VPC到端点中指定的服务的出站流量。 使用服务前缀列表ID，例如 com.amazonaws.us-east-1.s3作为出站规则中的目的地<ol start="6">
<li>在一个路由表中拥有针对不同服务的多个终端节点路由，并且可以在不同的路由表中拥有针对同一服务的多个终端节点路由，但不能在一个路由表中拥有针对同一服务的多个终端节点路由。例如，如果 VPC 中有两个针对 Amazon S3 的终端节点，则不能同时对这两个终端节点使用相同的路由表</li>
</ol>
</li>
</ol>
<ul>
<li>限制<ul>
<li>仅在同一地区内支持终端节点。无法在 VPC 和其他区域内的服务之间创建终端节点</li>
<li>无法将终端节点从一个 VPC 转移到另一个 VPC，也无法将终端节点从一项服务转移到另一项服务</li>
<li>无法将终端节点连接扩展到 VPC 之外。VPC 中的 VPN 连接、VPC 对等连接、AWS Direct Connect 连接或 ClassicLink 连接的另一端的资源不能使用终端节点与终端节点服务中的资源进行通信<h3 id="接口终端节点"><a href="#接口终端节点" class="headerlink" title="接口终端节点"></a>接口终端节点</h3></li>
</ul>
</li>
<li>接口 VPC 终端节，可连接到由 AWS PrivateLink 提供支持的服务</li>
<li>服务包括一些AWS服务，例如： CloudTrail，CloudWatch等，由其他AWS客户和合作伙伴在其自己的VPC（称为端点服务）中托管的服务，以及受支持的AWS Marketplace合作伙伴服务<br><img src="https://i.loli.net/2019/08/10/7VcIfgrZWMyF4GT.png" alt="VPC-Interface-Endpoints.png"></li>
<li>配置</li>
</ul>
<ol>
<li>选择要在其中创建接口终端节点的 VPC，然后提供要连接到的 AWS 服务、终端节点服务或 AWS Marketplace 服务的名称。</li>
<li>在 VPC 中选择使用接口终端节点的子网。将在该子网中创建一个终端节点网络接口</li>
<li>指定要与终端节点网络接口关联的安全组</li>
<li>为终端节点启用私有 DNS 以使能够使用服务的默认 DNS 主机名对服务发出请求（可选）</li>
</ol>
<ul>
<li>限制</li>
</ul>
<ol>
<li>对于每个接口终端节点，每个可用区只能选择一个子网</li>
<li>默认情况下，每个可用区的每个接口终端节点可支持高达 10 Gbps 的带宽</li>
<li>子网的网络ACL可以限制流量，需要正确配置</li>
<li><font color="red">接口终端节点仅支持 TCP 流量</font></li>
<li>仅在同一地区内支持终端节点</li>
<li>仅支持 IPv4 流量</li>
<li>无法将终端节点从一个 VPC 转移到另一个 VPC，也无法将终端节点从一项服务转移到另一项服务<h2 id="VPC-Peering"><a href="#VPC-Peering" class="headerlink" title="VPC Peering"></a>VPC Peering</h2></li>
</ol>
<ul>
<li>VPC 对等连接是两个 VPC 之间的网络连接，可通过此连接不公开地在这两个 VPC 之间路由流量(Private IP address)</li>
<li>两个 VPC 中的实例可以彼此通信，就像它们在同一网络中一样</li>
<li>可以在自己的 VPC 之间、自己的 VPC 与另一个 AWS 账户中的 VPC 或与其他 AWS 区域中的 VPC 之间创建 VPC 对等连接</li>
<li>AWS 使用 VPC 的现有基础设施来创建 VPC 对等连接；该连接既非网关也非 AWS Site-to-Site VPN 连接，且不依赖某个单独的物理硬件。</li>
<li>没有单点通信故障也没有带宽瓶颈</li>
</ul>
<h3 id="VPC-对等连接规则和限制"><a href="#VPC-对等连接规则和限制" class="headerlink" title="VPC 对等连接规则和限制"></a>VPC 对等连接规则和限制</h3><ol>
<li><p>不能在重叠CIDR块的VPC之间创建VPC对等连接<br><img src="https://i.loli.net/2019/08/10/xHQWgin81pjRylO.png" alt="Screen-Shot-2016-06-15-at-12.33.07-PM.png"></p>
</li>
<li><p>每个 VPC 创建数量有限的活动和待定 VPC 对等连接</p>
<table>
<thead>
<tr>
<th>源</th>
<th>默认限制</th>
<th>注释</th>
</tr>
</thead>
<tbody><tr>
<td>每个 VPC 的活动 VPC 对等连接</td>
<td>50</td>
<td>每个 VPC 的最大限制为 125 个对等连接。应相应地增加每个路由表的条目数；但是，网络性能可能会受到影响</td>
</tr>
<tr>
<td>未完成的 VPC 对等连接请求</td>
<td>25</td>
<td>这是账户请求的未完成 VPC 对等连接请求数的限制</td>
</tr>
<tr>
<td>未接受的 VPC 对等连接请求的过期时间</td>
<td>1</td>
<td>周 (168 小时)</td>
</tr>
</tbody></table>
</li>
<li><p>VPC 对等不支持传递的对等关系。在 VPC 对等连接中， VPC 无权访问对等 VPC 可能与之对等的任何其他 VPC<br><img src="https://i.loli.net/2019/08/10/Z7nO2QzRpqYMtld.png" alt="Screen-Shot-2016-06-15-at-12.33.00-PM.png"></p>
</li>
<li><p>VPC对等不支持通过网关或专用连接进行边缘到边缘路由</p>
</li>
<li><p>在VPC对等连接中，VPC无法访问对等VPC可能具有的任何其他连接， </p>
<ul>
<li>与企业网络之间的 VPN 连接或 AWS Direct Connect 连接</li>
<li>通过 Internet 网关建立的 Internet 连接</li>
<li>在私有子网中通过 NAT 设备建立的 Internet 连接</li>
<li>AWS 服务的 VPC 终端节点；例如，Amazon S3 的终端节点<br><img src="https://i.loli.net/2019/08/10/kciwbLvoAMqHDYj.png" alt="Screen-Shot-2016-06-15-at-12.35.38-PM.png"></li>
</ul>
</li>
<li><p>在同一个两个VPC之间只能建立一个VPC对等连接</p>
</li>
<li><p>VPC对等连接上的最大传输单元（MTU）为1500字节</p>
</li>
<li><p>置放组（place group）可在同一区域内建立对等连接，但在对等连接中无法获得全部带宽</p>
</li>
<li><p>VPC 对等连接创建的任何标签仅在创建它们的账户或区域中应用</p>
</li>
<li><p>不支持在 VPC 对等连接中进行单一地址反向传输路径转发</p>
</li>
<li><p><font color="red">必须为 VPC 对等连接启用 DNS 解析支持才能将对等 VPC 的私有 DNS 主机名解析为私有IP地址</font></p>
<h3 id="VPC-Peering-Architecture"><a href="#VPC-Peering-Architecture" class="headerlink" title="VPC Peering Architecture"></a>VPC Peering Architecture</h3><p><img src="https://i.loli.net/2019/08/10/H19WGByM87D6log.png" alt="Screen-Shot-2016-11-12-at-3.20.55-PM.png"></p>
</li>
</ol>
<ul>
<li>可以应用VPC对等来创建共享服务或使用本地实例执行身份验证</li>
<li>这将有助于创建单个联系，并将VPN连接限制到单个帐户或VPC<h2 id="VPC-VPN-Connections-amp-CloudHub"><a href="#VPC-VPN-Connections-amp-CloudHub" class="headerlink" title="VPC VPN Connections &amp; CloudHub"></a>VPC VPN Connections &amp; CloudHub</h2></li>
<li>VPC  VPN 连接用于将本地数据中心扩展到AWS云中</li>
<li>通过IPSec技术实现安全连接</li>
<li>硬件VPN<ul>
<li>VPC和远程网络之间创建IPSec隧道，硬件VPN连接来建立连接</li>
<li>在VPN连接的AWS端，虚拟专用网关（VGW）提供两个VPN端点以进行自动故障转移</li>
<li>在客户端需要配置客户网关（CGW），这是VPN连接远程端的物理设备或软件应用程序</li>
</ul>
</li>
<li>AWS Direct Connect<ul>
<li>AWS Direct Connect提供从远程网络到VPC的专用专用连接</li>
<li>Direct Connect可与AWS硬件VPN连接结合使用，以创建IPsec加密连接</li>
</ul>
</li>
<li>AWS VPN CloudHub<ul>
<li>有多个远程网络（例如，多个分公司），则可以通过虚拟专用网关创建多个 AWS Site-to-Site VPN 连接，来启用这些网络之间的通信</li>
</ul>
</li>
<li>Software VPN<ul>
<li>通过在 VPC 中使用正在运行软件 VPN 设备的 Amazon EC2 实例来创建与远程网络的 VPN 连接</li>
<li>AWS 不提供或维护第三方软件 VPN 设备；但是，可以选择合作伙伴和开源社区提供的一系列产品<h3 id="Hardware-VPN"><a href="#Hardware-VPN" class="headerlink" title="Hardware VPN"></a>Hardware VPN</h3><img src="https://i.loli.net/2019/08/10/1DJ9cQhp4KTznjE.png" alt="VPN-Connection.png"><h4 id="VPN-组件"><a href="#VPN-组件" class="headerlink" title="VPN 组件"></a>VPN 组件</h4></li>
</ul>
</li>
<li>虚拟专有网关—VGW<ul>
<li>VPN连接的AWS侧的VPN集中器</li>
</ul>
</li>
<li>客户网关——CGW<ul>
<li>客户网关是连接中用户侧使用的定位标记。它可以是物理或软件设备</li>
<li>VPN连接时，会在创建两端创建VPN隧道</li>
<li>VGW不是发起者，需要CGW启动VPN连接</li>
<li>若VPN链接建立之后超过10秒的空闲连接（具体看配置属性），隧道会自动关闭。为了避免这种情况，建议用网络监控工具保持周期性的ping，例如使用IP SLA。<h4 id="VPN-配置"><a href="#VPN-配置" class="headerlink" title="VPN 配置"></a>VPN 配置</h4></li>
</ul>
</li>
</ul>
<ul>
<li>VPC包含的VGW，并且远程网络部署客户网关，必须将两者配置为启用</li>
<li>配置路由，以便来自绑定到远程网络的VPC的任何流量都路由到虚拟专用网关（VGW）</li>
<li>每个VPN都有两个与之关联的隧道，可以在客户路由器上配置，因为不是单点故障</li>
<li>可以创建到单个VPC的多个VPN连接，并且可以配置第二个CGW以创建到同一外部位置的冗余连接或创建到多个地理位置的VPN连接<h4 id="VPN-路由"><a href="#VPN-路由" class="headerlink" title="VPN 路由"></a>VPN 路由</h4></li>
</ul>
<ul>
<li>对于VPN连接，应使用的路由类型（动态或静态）更新子网的路由表</li>
<li>路由表确定网络流量的定向。 发往VPN连接的流量必须经过到虚拟专用网关</li>
<li>路由的类型（VPN的设备和型号）<ul>
<li>静态路由<ul>
<li>如果设备不支持BGP，请指定静态路由。</li>
<li>使用静态路由，可以指定应传送到虚拟专用网关的路由（IP前缀）。</li>
<li>不支持BGP的设备也可以执行运行状况检查，以帮助在需要时故障转移到第二个隧道。</li>
</ul>
</li>
</ul>
</li>
<li>BGP动态路由<ul>
<li>如果VPN设备支持边界网关协议（BGP），请使用VPN连接指定动态路由。</li>
<li>使用BGP设备时，不需要为VPN连接指定静态路由，因为设备使用BGP进行自动发现并将其路由通告给虚拟专用网关。</li>
<li>建议使用支持BGP的设备，因为BGP协议提供强大的活动检测检查，如果第一个隧道出现故障，可以帮助故障转移到第二个VPN隧道</li>
<li>只有通过BGP通告或静态路由条目的虚拟专用网关已知的IP前缀才能从VPC接收流量。</li>
<li>VGW不会路由发布在BGP之外的任何其他流量，静态路由条目或其附加的VPC CIDR<h4 id="VPN-Connection-Redundancy"><a href="#VPN-Connection-Redundancy" class="headerlink" title="VPN Connection Redundancy"></a>VPN Connection Redundancy</h4><img src="https://i.loli.net/2019/08/10/yAYVsuhQUOBLtNv.png" alt="VPN-Connection-Redundancy.png"></li>
</ul>
</li>
<li>每个VPN连接都有两个隧道，以防止VGW成为单点故障，每个隧道使用唯一的虚拟专用网关公共IP地址</li>
<li>当一个隧道变得不可用时，例如为了维护，网络流量会自动路由到特定VPN连接的可用隧道</li>
<li>为防止在客户网关不可用时丢失连接，建议使用第二个客户网关为VPC和虚拟专用网关建立第二个VPN连接</li>
<li>建议使用边界网关协议（BGP）动态路由VPN连接，以在客户网关和虚拟专用网关之间交换路由信息</li>
<li>静态路由的VPN连接需要在客户网关侧输入网络的静态路由。</li>
<li>BGP通告和静态输入的路由信息允许双方的网关确定哪些隧道可用，并在发生故障时重新路由流量<h3 id="VPN-CloudHub"><a href="#VPN-CloudHub" class="headerlink" title="VPN CloudHub"></a>VPN CloudHub</h3></li>
<li>VPN CloudHub可用于在多站点之间提供多个VPN通道安全通信</li>
<li>远程站点彼此进行通信，而不只是与 VPC 进行通信</li>
<li>VPN CloudHub 在简单的星型拓扑连接模型上操作，可以在使用或不使用 VPC 的情况下操作 VPN CloudHub。</li>
<li>这种设计适合有多间分公司和现有 Internet 连接的客户，帮助他们实施方便、潜在低成本的星型拓扑连接模型，以便在这些远程办公室之间建立主要或备用连接<br><img src="https://i.loli.net/2019/08/10/4rd7DFAWt1SvoZe.png" alt="AWS_VPN_CloudHub-diagram.png"></li>
<li>蓝色虚线表示网络远程站点之间的流量通过其VPN连接进行路由的流量通道</li>
<li>使用 AWS VPN CloudHub，必须创建具有多个客户网关的虚拟专用网关</li>
<li>每个CGW使用唯一的边界网关协议 (BGP) 自治系统编号 (ASN)</li>
<li>客户网关可通过它们的 Site-to-Site VPN 连接传播适当的路由（BGP 前缀）</li>
<li>路由通告会被每个 BGP 对等体接收并重新通告，使每个站点都可以向其他站点发送或接受数据</li>
<li>每个辐射路由必须具有唯一的ASN，并且站点不得具有重叠的IP范围</li>
<li>每个站点还可以发送和从 VPC 接收数据（与使用标准 Site-to-Site VPN 连接的方式相同）</li>
<li>使用AWS Direct Connect连接到虚拟专用网关的站点也可以是AWS VPN CloudHub的一部分</li>
<li>配置VPN CloudHub VPN<ol>
<li>创建多个客户网关，每个网关都具有网关和ASN的唯一公共IP地址</li>
<li>每个CGW到公共虚拟专用网（VGW）关创建VPN连接</li>
<li>每个VPN连接必须通告其特定的BGP路由。 VPN连接的VPN配置文件来完成<h2 id="NAT网关-vs-NAT实例"><a href="#NAT网关-vs-NAT实例" class="headerlink" title="NAT网关 vs NAT实例"></a>NAT网关 vs NAT实例</h2></li>
</ol>
</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>NAT网关</th>
<th>NAT实例</th>
</tr>
</thead>
<tbody><tr>
<td>可用性</td>
<td>高度可用。每个可用区中的 NAT 网关都采用冗余实施。在每个可用区中创建一个 NAT 网关可确保架构不依赖于可用区</td>
<td>使用脚本管理实例之间的故障转移</td>
</tr>
<tr>
<td></td>
<td>NAT网关</td>
<td>NAT实例</td>
</tr>
<tr>
<td>————-</td>
<td>————-</td>
<td>——</td>
</tr>
<tr>
<td>可用性</td>
<td>高度可用。每个可用区中的 NAT 网关都采用冗余实施。在每个可用区中创建一个 NAT 网关可确保架构不依赖于可用区</td>
<td>使用脚本管理实例之间的故障转移</td>
</tr>
<tr>
<td>带宽</td>
<td>可以扩展到 45 Gbps</td>
<td>取决于实例类型的带宽</td>
</tr>
<tr>
<td>维护</td>
<td>由 AWS 管理。不需要进行任何维护</td>
<td>自行管理，例如需要对实例安装软件更新或操作系统补丁</td>
</tr>
<tr>
<td>性能</td>
<td>软件经过优化以便处理 NAT 流量</td>
<td>由配置来执行 NAT 的通用 AMI</td>
</tr>
<tr>
<td>费用</td>
<td>费用取决于使用的 NAT 网关的数量、使用时长以及通过 NAT 网关发送的数据量</td>
<td>费用取决于使用的 NAT 实例的数量、使用时长以及实例类型和大小</td>
</tr>
<tr>
<td>类型和大小</td>
<td>整合提供；不需要选择类型或范围</td>
<td>根据的预测工作负载选择适当的实例类型和大小</td>
</tr>
<tr>
<td>公有IP地址</td>
<td>在创建时选择弹性 IP 地址以与 NAT 网关关联</td>
<td>为 NAT 实例使用弹性 IP 地址或公有 IP 地址。随时可以通过将新的弹性 IP 地址与实例关联来更改公有 IP 地址</td>
</tr>
<tr>
<td>私有IP地址</td>
<td>在创建网关时自动从子网的 IP 地址范围中选择。</td>
<td>在启动实例时，从子网的 IP 地址范围内分配特定的私有 IP 地址</td>
</tr>
<tr>
<td>安全组</td>
<td>无法与 NAT 网关关联。可以将安全组与 NAT 网关之后的资源关联，以控制入站和出站流量。</td>
<td>与NAT 实例和 NAT 实例之后的资源关联，以控制入站和出站流量。</td>
</tr>
<tr>
<td>网络 ACL</td>
<td>使用网络 ACL 控制进出NAT 网关所在子网的流量</td>
<td>使用网络 ACL 控制进出 NAT 实例所在子网的流量</td>
</tr>
<tr>
<td>流日志</td>
<td>使用流日志捕获流量。</td>
<td>使用流日志捕获流量</td>
</tr>
<tr>
<td>端口转发</td>
<td>不支持</td>
<td>手动自定义配置以支持端口转发</td>
</tr>
<tr>
<td>堡垒服务器</td>
<td>不支持</td>
<td>用作堡垒服务器</td>
</tr>
<tr>
<td>流量指标</td>
<td><a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-nat-gateway-cloudwatch.html" target="_blank" rel="noopener">查看 NAT 网关的 CloudWatch 指标</a></td>
<td>查看实例的 CloudWatch 指标</td>
</tr>
<tr>
<td>超时行为</td>
<td>如果连接超时，NAT 网关向 NAT 网关后方的任何资源返回 RST 数据包，尝试继续进行连接 (它不发送 FIN 数据包)。</td>
<td>如果连接超时，NAT 实例向 NAT 实例后方的资源发送 FIN 数据包，以关闭连接</td>
</tr>
<tr>
<td>IP 分段</td>
<td>支持转发 UDP 协议的 IP 分段数据包。不支持 TCP 和 ICMP 协议的分段。将删除这些协议的分段数据包。</td>
<td>支持重组 UDP、TCP 和 ICMP 协议的 IP 分段数据包。</td>
</tr>
</tbody></table>
<h3 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h3><ol>
<li><a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/amazon-vpc-limits.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/amazon-vpc-limits.html</a></li>
<li>Amazon Virtual Private Cloud user guide </li>
<li>AWS Site-to-Site VPN user guide</li>
</ol>



</font>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2018/10/26/图计算/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/26/图计算/" itemprop="url">图计算</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-26T19:13:10+08:00">
                2018-10-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">Bigdata</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="图计算简介"><a href="#图计算简介" class="headerlink" title="图计算简介"></a>图计算简介</h1><h2 id="图结构数据"><a href="#图结构数据" class="headerlink" title="图结构数据"></a>图结构数据</h2><ul>
<li>许多大数据都是以大规模图或网络的形式呈现，如社交网络、传染病传播途径、交通事故对路网的影响</li>
<li>许多非图结构的大数据，也常常会被转换为图模型后进行分析</li>
<li>图数据结构很好地表达了数据之间的关联性</li>
<li>关联性计算是大数据计算的核心——通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用的信息<ul>
<li>比如，通过为购物者之间的关系建模，就能很快找到口味相似的用户，并为之推荐商品</li>
<li>或者在社交网络中，通过传播关系发现意见领袖<h2 id="传统图计算解决方案的不足之处"><a href="#传统图计算解决方案的不足之处" class="headerlink" title="传统图计算解决方案的不足之处"></a>传统图计算解决方案的不足之处</h2></li>
</ul>
</li>
</ul>
<ol>
<li>常常表现出比较差的内存访问局部性</li>
<li>针对单个顶点的处理工作过少</li>
<li>计算过程中伴随着并行度的改变<h2 id="图计算通用软件"><a href="#图计算通用软件" class="headerlink" title="图计算通用软件"></a>图计算通用软件</h2></li>
</ol>
<ul>
<li>第一种主要是基于遍历算法的、实时的图数据库，如Neo4j、OrientDB、DEX和 Infinite Graph</li>
<li>第二种则是以图顶点为中心的、基于消息传递批处理的并行引擎，如GoldenOrb、Giraph、Pregel和Hama，这些图处理软件主要是基于BSP模型实现的并行图处理系统<blockquote>
<ul>
<li>一次BSP(Bulk Synchronous Parallel Computing Model，又称“大同步”模型)计算过程包括一系列全局超步（所谓的超步就是计算中的一次迭代），每个超步主要包括三个组件：</li>
</ul>
</blockquote>
</li>
<li><strong>局部计算</strong>：每个参与的处理器都有自身的计算任务，它们只读取存储在本地内存中的值，不同处理器的计算任务都是异步并且独立的</li>
<li><strong>通讯</strong>：处理器群相互交换数据，交换的形式是，由一方发起推送(put)和获取(get)操作</li>
<li><strong>栅栏同步(Barrier Synchronization)</strong>：当一个处理器遇到“路障”（或栅栏），会等到其他所有处理器完成它们的计算步骤；每一次同步也是一个超步的完成和下一个超步的开始<br><img src="https://i.loli.net/2019/08/16/CbFHOziDv5lRt3f.png" alt="一个超步的垂直结构图 .png"><br><img src="https://i.loli.net/2019/08/16/xJ7QVRDH8P1dMCY.png" alt="超同步.png"><h1 id="Pregel简介"><a href="#Pregel简介" class="headerlink" title="Pregel简介"></a>Pregel简介</h1></li>
<li>Pregel是一种基于BSP模型实现的并行图处理系统</li>
<li>为了解决大型图的分布式计算问题，Pregel搭建了一套可扩展的、有容错机制的平台，该平台提供了一套非常灵活的API，可以描述各种各样的图计算</li>
<li>Pregel作为分布式图计算的计算框架，主要用于图遍历、最短路径、PageRank计算等等<h1 id="Pregel图计算模型"><a href="#Pregel图计算模型" class="headerlink" title="Pregel图计算模型"></a>Pregel图计算模型</h1><h2 id="有向图和顶点"><a href="#有向图和顶点" class="headerlink" title="有向图和顶点"></a>有向图和顶点</h2></li>
<li>Pregel计算模型以有向图作为输入</li>
<li>有向图的每个顶点都有一个String类型的顶点ID</li>
<li>每个顶点都有一个可修改的用户自定义值与之关联</li>
<li>每条有向边都和其源顶点关联，并记录了其目标顶点ID<br><img src="https://i.loli.net/2019/08/16/gLxEuf8voYjP6Qs.png" alt="有向图和顶点.png"></li>
<li>边上有一个可修改的用户自定义值与之关联</li>
<li>在每个超步S中，图中的所有顶点都会并行执行相同的用户自定义函数</li>
<li>每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构</li>
<li>在这种计算模式中，“边”并不是核心对象，在边上面不会运行相应的计算，只有顶点才会执行用户自定义函数进行相应计算<br><img src="https://i.loli.net/2019/08/16/xJ7QVRDH8P1dMCY.png" alt="超同步.png"><h2 id="顶点之间的消息传递"><a href="#顶点之间的消息传递" class="headerlink" title="顶点之间的消息传递"></a>顶点之间的消息传递</h2></li>
</ul>
<blockquote>
<ul>
<li>采用消息传递模型主要基于以下两个原因：</li>
</ul>
<ol>
<li>消息传递具有足够的表达能力，没有必要使用远程读取或共享内存的方式</li>
<li>有助于提升系统整体性能。大型图计算通常是由一个集群完成的，集群环境中执行远程数据读取会有较高的延迟；Pregel的消息模式采用异步和批量的方式传递消息，因此可以缓解远程读取的延迟<br><img src="https://i.loli.net/2019/08/16/bmZ8JU2CrKexPVa.png" alt="纯消息传递模型图 .png"></li>
</ol>
</blockquote>
<h2 id="Pregel的计算过程"><a href="#Pregel的计算过程" class="headerlink" title="Pregel的计算过程"></a>Pregel的计算过程</h2><ul>
<li>Pregel的计算过程是由一系列被称为“超步”的迭代组成的</li>
<li>在每个超步中，每个顶点上面都会并行执行用户自定义的函数，该函数描述了一个顶点V在一个超步S中需要执行的操作</li>
<li>该函数可以读取前一个超步(S-1)中其他顶点发送给顶点V的消息，执行相应计算后，修改顶点V及其出射边的状态，然后沿着顶点V的出射边发送消息给其他顶点，而且，一个消息可能经过多条边的传递后被发送到任意已知ID的目标顶点上去</li>
<li>这些消息将会在下一个超步(S+1)中被目标顶点接收，然后象上述过程一样开始下一个超步(S+1)的迭代过程<br><img src="https://i.loli.net/2019/08/16/eUrodhXamDiVS1Y.png" alt="Ptrgel计算过程.png"></li>
<li>在Pregel计算过程中，一个算法什么时候可以结束，是由所有顶点的状态决定的</li>
<li>在第0个超步，所有顶点处于活跃状态，都会参与该超步的计算过程</li>
<li>当一个顶点不需要继续执行进一步的计算时，就会把自己的状态设置为“停机”，进入非活跃状态</li>
<li>一旦一个顶点进入非活跃状态，后续超步中就不会再在该顶点上执行计算，除非其他顶点给该顶点发送消息把它再次激活</li>
<li>当一个处于非活跃状态的顶点收到来自其他顶点的消息时，Pregel计算框架必须根据条件判断来决定是否将其显式唤醒进入活跃状态</li>
<li>当图中所有的顶点都已经标识其自身达到“非活跃（inactive）”状态，并且没有消息在传送的时候，算法就可以停止运行<br><img src="https://i.loli.net/2019/08/16/Bfl9CYyFdSunm7i.png" alt="一个简单的状态机图.png"><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><img src="https://i.loli.net/2019/08/16/YzldRLfNjiSUwM3.png" alt="一个求最大值的Pregel计算过程图 .png"><h1 id="Pregel的C-API"><a href="#Pregel的C-API" class="headerlink" title="Pregel的C++ API"></a>Pregel的C++ API</h1></li>
<li>Pregel已经预先定义好一个基类——Vertex类：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">template &lt;typename VertexValue, typename EdgeValue, typename MessageValue&gt;</span><br><span class="line">class Vertex &#123;</span><br><span class="line">  public:</span><br><span class="line">	virtual void Compute(MessageIterator* msgs) = 0;</span><br><span class="line">	const string&amp; vertex_id() const;</span><br><span class="line">	int64 superstep() const;</span><br><span class="line">	const VertexValue&amp; GetValue();</span><br><span class="line">	VertexValue* MutableValue();</span><br><span class="line">	OutEdgeIterator GetOutEdgeIterator();	</span><br><span class="line">	void SendMessageTo(const string&amp; dest_vertex,	const MessageValue&amp; message);</span><br><span class="line">	void VoteToHalt();</span><br><span class="line">    &#125;;</span><br></pre></td></tr></table></figure>

<ul>
<li>在Vetex类中，定义了三个值类型参数，分别表示顶点、边和消息。每一个顶点都有一个给定类型的值与之对应</li>
<li>编写Pregel程序时，需要继承Vertex类，并且覆写Vertex类的虚函数Compute() </li>
<li>在Pregel执行计算过程时，在每个超步中都会并行调用每个顶点上定义的Compute()函数</li>
<li>允许Compute()方法查询当前顶点及其边的信息，以及发送消息到其他的顶点<ul>
<li>Compute()方法可以调用GetValue()方法来获取当前顶点的值</li>
<li>调用MutableValue()方法来修改当前顶点的值</li>
<li>通过由出射边的迭代器提供的方法来查看、修改出射边对应的值</li>
</ul>
</li>
<li>对状态的修改，对于被修改的顶点而言是可以立即被看见的，但是，对于其他顶点而言是不可见的，因此，不同顶点并发进行的数据访问是不存在竞争关系的<blockquote>
<ul>
<li>整个过程中，唯一需要在超步之间持久化的顶点级状态，是顶点和其对应的边所关联的值，因而，Pregel计算框架所需要管理的图状态就只包括顶点和边所关联的值，这种做法大大简化了计算流程，同时，也有利于图的分布和故障恢复</li>
</ul>
</blockquote>
<h2 id="消息传递机制"><a href="#消息传递机制" class="headerlink" title="消息传递机制"></a>消息传递机制</h2></li>
<li>顶点之间的通讯是借助于消息传递机制来实现的，每条消息都包含了消息值和需要到达的目标顶点ID。用户可以通过Vertex类的模板参数来设定消息值的数据类型</li>
<li>在一个超步S中，一个顶点可以发送任意数量的消息，这些消息将在下一个超步（S+1）中被其他顶点接收</li>
<li>也就是说，在超步（S+1）中，当Pregel计算框架在顶点V上执行用户自定义的Compute()方法时，所有在前一个超步S中发送给顶点V的消息，都可以通过一个迭代器来访问到。迭代器不能保证消息的顺序，不过可以保证消息一定会被传送并且不会被重复传送</li>
<li>一个顶点V通过与之关联的出射边向外发送消息，并且，消息要到达的目标顶点并不一定是与顶点V相邻的顶点，一个消息可以连续经过多条连通的边到达某个与顶点V不相邻的顶点U，U可以从接收的消息中获取到与其不相邻的顶点V的ID<br><img src="https://i.loli.net/2019/08/16/xJ7QVRDH8P1dMCY.png" alt="超同步.png"><h2 id="Combiner"><a href="#Combiner" class="headerlink" title="Combiner"></a>Combiner</h2></li>
<li>Pregel计算框架在消息发出去之前，Combiner可以将发往同一个顶点的多个整型值进行求和得到一个值，只需向外发送这个“求和结果”，从而实现了由多个消息合并成一个消息，大大减少了传输和缓存的开销</li>
<li>在默认情况下，Pregel计算框架并不会开启Combiner功能，因为，通常很难找到一种对所有顶点的Compute()函数都合适的Combiner</li>
<li>当用户打算开启Combiner功能时，可以继承Combiner类并覆写虚函数Combine()<br>此外，通常只对那些满足交换律和结合律的操作才可以去开启Combiner功能，因为，Pregel计算框架无法保证哪些消息会被合并，也无法保证消息传递给 Combine()的顺序和合并操作执行的顺序<br><img src="https://i.loli.net/2019/08/16/5LgTwuXAxR4lHJS.png" alt="Combiner应用的例子.png"><h2 id="Aggregator"><a href="#Aggregator" class="headerlink" title="Aggregator"></a>Aggregator</h2></li>
<li>Aggregator提供了一种全局通信、监控和数据查看的机制</li>
<li>在一个超步S中，每一个顶点都可以向一个Aggregator提供一个数据，Pregel计算框架会对这些值进行聚合操作产生一个值，在下一个超步（S+1）中，图中的所有顶点都可以看见这个值</li>
<li>Aggregator的聚合功能，允许在整型和字符串类型上执行最大值、最小值、求和操作，比如，可以定义一个“Sum”Aggregator来统计每个顶点的出射边数量，最后相加可以得到整个图的边的数量</li>
<li>Aggregator还可以实现全局协同的功能，比如，可以设计“and” Aggregator来决定在某个超步中Compute()函数是否执行某些逻辑分支，只有当“and” Aggregator显示所有顶点都满足了某条件时，才去执行这些逻辑分支<h2 id="拓扑改变"><a href="#拓扑改变" class="headerlink" title="拓扑改变"></a>拓扑改变</h2></li>
<li>Pregel计算框架允许用户在自定义函数Compute()中定义操作，修改图的拓扑结构，比如在图中增加（或删除）边或顶点</li>
<li>对于全局拓扑改变，Pregel采用了惰性协调机制，在改变请求发出时，Pregel不会对这些操作进行协调，只有当这些改变请求的消息到达目标顶点并被执行时，Pregel才会对这些操作进行协调，这样，所有针对某个顶点V的拓扑修改操作所引发的冲突，都会由V自己来处理</li>
<li>对于本地的局部拓扑改变，是不会引发冲突的，顶点或边的本地增减能够立即生效，很大程度上简化了分布式编程<h2 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h2></li>
<li>在Pregel计算框架中，图的保存格式多种多样，包括文本文件、关系数据库或键值数据库等</li>
<li>在Pregel中，“从输入文件生成得到图结构”和“执行图计算”这两个过程是分离的，从而不会限制输入文件的格式</li>
<li>对于输出，Pregel也采用了灵活的方式，可以以多种方式进行输出<h1 id="Pregel的体系结构"><a href="#Pregel的体系结构" class="headerlink" title="Pregel的体系结构"></a>Pregel的体系结构</h1><h2 id="Pregel的执行过程"><a href="#Pregel的执行过程" class="headerlink" title="Pregel的执行过程"></a>Pregel的执行过程</h2></li>
<li>在Pregel计算框架中，一个大型图会被划分成许多个分区，每个分区都包含了一部分顶点以及以其为起点的边</li>
<li>一个顶点应该被分配到哪个分区上，是由一个函数决定的，系统默认函数为hash(ID) mod N，其中，N为所有分区总数，ID是这个顶点的标识符；当然，用户也可以自己定义这个函数</li>
<li>这样，无论在哪台机器上，都可以简单根据顶点ID判断出该顶点属于哪个分区，即使该顶点可能已经不存在了</li>
</ul>
<p><img src="https://i.loli.net/2019/08/16/tl1LIS5QxwoK3vf.png" alt="图的划分图.png"></p>
<blockquote>
<ul>
<li>在理想的情况下（不发生任何错误），一个Pregel用户程序的执行过程如下：</li>
</ul>
<ol>
<li>选择集群中的多台机器执行图计算任务，每台机器上运行用户程序的一个副本，其中，有一台机器会被选为Master，其他机器作为Worker。Master只负责协调多个Worker执行任务，系统不会把图的任何分区分配给它。Worker借助于名称服务系统可以定位到Master的位置，并向Master发送自己的注册信息。</li>
<li>Master把一个图分成多个分区，并把分区分配到多个Worker。一个Worker会领到一个或多个分区，每个Worker知道所有其他Worker所分配到的分区情况。每个Worker负责维护分配给自己的那些分区的状态(顶点及边的增删)，对分配给自己的分区中的顶点执行Compute()函数，向外发送消息，并管理接收到的消息。<br><img src="https://i.loli.net/2019/08/16/8vGSQVM9zCxF5Ab.png" alt="Pregel的执行过程图 .png"></li>
<li>Master会把用户输入划分成多个部分，通常是基于文件边界进行划分。划分后，每个部分都是一系列记录的集合，每条记录都包含一定数量的顶点和边。然后，Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点，刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构。否则，该Worker会根据加载到的顶点的ID，把它发送到其所属的分区所在的Worker上。当所有的输入都被加载后，图中的所有顶点都会被标记为“活跃”状态。</li>
<li>Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。Worker会为自己管辖的每个分区分配一个线程，对于分区中的每个顶点，Worker会把来自上一个超步的、发给该顶点的消息传递给它，并调用处于“活跃”状态的顶点上的Compute()函数，在执行计算过程中，顶点可以对外发送消息，但是，所有消息的发送工作必须在本超步结束之前完成。当所有这些工作都完成以后，Worker会通知Master，并把自己在下一个超步还处于“活跃”状态的顶点的数量报告给Master。上述步骤会被不断重复，直到所有顶点都不再活跃并且系统中不会有任何消息在传输，这时，执行过程才会结束。</li>
<li>计算过程结束后，Master会给所有的Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储。</li>
</ol>
</blockquote>
<h2 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h2><ul>
<li>Pregel采用检查点机制来实现容错。在每个超步的开始，Master会通知所有的Worker把自己管辖的分区的状态（包括顶点值、边值以及接收到的消息），写入到持久化存储设备</li>
<li>Master会周期性地向每个Worker发送ping消息，Worker收到ping消息后会给Master发送反馈消息。如果Master在指定时间间隔内没有收到某个Worker的反馈消息，就会把该Worker标记为“失效”。同样地，如果一个Worker在指定的时间间隔内没有收到来自Master的ping消息，该Worker也会停止工作</li>
<li>每个Worker上都保存了一个或多个分区的状态信息，当一个Worker发生故障时，它所负责维护的分区的当前状态信息就会丢失。Master监测到一个Worker发生故障“失效”后，会把失效Worker所分配到的分区，重新分配到其他处于正常工作状态的Worker集合上，然后，所有这些分区会从最近的某超步S开始时写出的检查点中，重新加载状态信息<h2 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h2></li>
<li>在一个Worker中，它所管辖的分区的状态信息是保存在内存中的。分区中的顶点的状态信息包括：<ul>
<li>顶点的当前值</li>
<li>以该顶点为起点的出射边列表，每条出射边包含了目标顶点ID和边的值</li>
<li>消息队列，包含了所有接收到的、发送给该顶点的消息</li>
<li>标志位，用来标记顶点是否处于活跃状态</li>
</ul>
</li>
<li>在每个超步中，Worker会对自己所管辖的分区中的每个顶点进行遍历，并调用顶点上的Compute()函数，在调用时，会把以下三个参数传递进去：<ul>
<li>该顶点的当前值</li>
<li>一个接收到的消息的迭代器 </li>
<li>一个出射边的迭代器 </li>
</ul>
</li>
<li>在Pregel中，为了获得更好的性能，“标志位”和输入消息队列是分开保存的</li>
<li>对于每个顶点而言，Pregel只保存一份顶点值和边值，但是，会保存两份“标志位”和输入消息队列，分别用于当前超步和下一个超步</li>
<li>在超步S中，当一个Worker在进行顶点处理时，用于当前超步的消息会被处理，同时，它在处理过程中还会接收到来自其他Worker的消息，这些消息会在下一个超步S+1中被处理，因此，需要两个消息队列用于存放作用于当前超步S的消息和作用于下一个超步S+1的消息</li>
<li>如果一个顶点V在超步S接收到消息，那么，它表示V将会在下一个超步S+1中（而不是当前超步S中）处于“活跃”状态</li>
<li>当一个Worker上的一个顶点V需要发送消息到其他顶点U时，该Worker会首先判断目标顶点U是否位于自己机器上</li>
<li>如果目标顶点U在自己的机器上，就直接把消息放入到与目标顶点U对应的输入消息队列中</li>
<li>如果发现目标顶点U在远程机器上，这个消息就会被暂时缓存到本地，当缓存中的消息数目达到一个事先设定的阈值时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Worker上</li>
<li>如果存在用户自定义的Combiner操作，那么，当消息被加入到输出队列或者到达输入队列时，就可以对消息执行合并操作，这样可以节省存储空间和网络传输开销<h2 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h2></li>
<li>Master主要负责协调各个Worker执行任务，每个Worker会借助于名称服务系统定位到Master的位置，并向Master发送自己的注册信息，Master会为每个Worker分配一个唯一的ID</li>
<li>Master维护着关于当前处于“有效”状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息</li>
<li>虽然在集群中只有一个Master，但是，它仍然能够承担起一个大规模图计算的协调任务，这是因为Master中保存这些信息的数据结构的大小，只与分区的数量有关，而与顶点和边的数量无关</li>
<li>一个大规模图计算任务会被Master分解到多个Worker去执行，在每个超步开始时，Master都会向所有处于“有效”状态的Worker发送相同的指令，然后等待这些Worker的回应</li>
<li>如果在指定时间内收不到某个Worker的反馈，Master就认为这个Worker失效</li>
<li>如果参与任务执行的多个Worker中的任意一个发生了故障失效，Master就会进入恢复模式</li>
<li>在每个超步中，图计算的各种工作，比如输入、输出、计算、保存和从检查点中恢复，都会在“路障（barrier）”之前结束</li>
<li>如果路障同步成功，说明一个超步顺利结束，Master就会进入下一个处理阶段，图计算进入下一个超步的执行</li>
<li>Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息</li>
<li>用户可以通过网页随时监控图计算执行过程各个细节<ul>
<li>图的大小</li>
<li>关于出度分布的柱状图</li>
<li>处于活跃状态的顶点数量</li>
<li>在当前超步的时间信息和消息流量</li>
<li>所有用户自定义Aggregator的值</li>
</ul>
</li>
</ul>
<h2 id="Aggregator-1"><a href="#Aggregator-1" class="headerlink" title="Aggregator"></a>Aggregator</h2><ul>
<li>每个用户自定义的Aggregator都会采用聚合函数对一个值集合进行聚合计算得到一个全局值</li>
<li>每个Worker都保存了一个Aggregator的实例集，其中的每个实例都是由类型名称和实例名称来标识的</li>
<li>在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部归约值</li>
<li>在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后的汇总，得到全局值，然后提交给Master</li>
<li>在下一个超步S+1开始时，Master就会将Aggregator的全局值发送给每个Worker</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2018/10/16/流计算/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/16/流计算/" itemprop="url">流计算</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-16T19:02:37+08:00">
                2018-10-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">Bigdata</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="流计算概述"><a href="#流计算概述" class="headerlink" title="流计算概述"></a>流计算概述</h1><h2 id="静态数据和流数据"><a href="#静态数据和流数据" class="headerlink" title="静态数据和流数据"></a>静态数据和流数据</h2><ul>
<li>很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。技术人员可以利用数据挖掘和OLAP（On-Line Analytical Processing）分析工具从静态数据中找到对企业有价值的信息<br><img src="https://i.loli.net/2019/08/15/CMX6Q7n4UceDJi8.png" alt="静态数据.png"></li>
</ul>
<ul>
<li>近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达</li>
<li>实例：PM2.5检测、电子商务网站用户点击流</li>
<li>流数据具有如下特征：<ul>
<li>数据快速持续到达，潜在大小也许是无穷无尽的</li>
<li>数据来源众多，格式复杂</li>
<li>数据量大，但是不十分关注存储，一旦经过处理，要么被丢弃，要么被归档存储</li>
<li>注重数据的整体价值，不过分关注个别数据</li>
<li>数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序</li>
</ul>
</li>
</ul>
<h2 id="批量计算和实时计算"><a href="#批量计算和实时计算" class="headerlink" title="批量计算和实时计算"></a>批量计算和实时计算</h2><ul>
<li>对静态数据和流数据的处理，对应着两种截然不同的计算模式：批量计算和实时计算</li>
<li>批量计算：充裕时间处理静态数据，如Hadoop</li>
<li>流数据不适合采用批量计算，因为流数据不适合用传统的关系模型建模</li>
<li>流数据必须采用实时计算，响应时间为秒级</li>
<li>数据量少时，不是问题，但是，在大数据时代，数据格式复杂、来源众多、数据量巨大，对实时计算提出了很大的挑战。因此，针对流数据的实时计算——流计算，应运而生<br><img src="https://i.loli.net/2019/08/15/nC72xfOSIHaXW6J.png" alt="数据的两种处理模型.png"></li>
</ul>
<h2 id="流计算概念"><a href="#流计算概念" class="headerlink" title="流计算概念"></a>流计算概念</h2><ul>
<li>流计算：实时获取来自不同数据源的海量数据，经过实时分析处理，获得有价值的信息<br><img src="https://i.loli.net/2019/08/15/ryRDeMPvW2kVFZ8.png" alt="流计算示意图.png"></li>
<li>流计算秉承一个基本理念，即<strong>数据的价值随着时间的流逝而降低</strong>，如用户点击流。因此，当事件出现时就应该立即进行处理，而不是缓存起来进行批量处理。为了及时处理流数据，就需要一个低延迟、可扩展、高可靠的处理引擎</li>
<li>对于一个流计算系统来说，它应达到如下需求：<ul>
<li>高性能：处理大数据的基本要求，如每秒处理几十万条数据</li>
<li>海量式：支持TB级甚至是PB级的数据规模</li>
<li>实时性：保证较低的延迟时间，达到秒级别，甚至是毫秒级别</li>
<li>分布式：支持大数据的基本架构，必须能够平滑扩展</li>
<li>易用性：能够快速进行开发和部署</li>
<li>可靠性：能可靠地处理流数据</li>
</ul>
</li>
</ul>
<h2 id="流计算与Hadoop"><a href="#流计算与Hadoop" class="headerlink" title="流计算与Hadoop"></a>流计算与Hadoop</h2><ul>
<li>Hadoop设计的初衷是面向大规模数据的批量处理，每台机器并行运行MapReduce任务，最后对结果进行汇总输出</li>
<li>MapReduce是专门面向静态数据的批量处理的，内部各种实现机制都为批处理做了高度优化，不适合用于处理持续到达的动态数据</li>
<li>可能会想到一种“变通”的方案来降低批处理的时间延迟——将基于MapReduce的批量处理转为小批量处理，将输入数据切成小的片段，每隔一个周期就启动一次MapReduce作业。但这种方式也无法有效处理流数据<ul>
<li>切分成小片段，可以降低延迟，但是也增加了附加开销，还要处理片段之间依赖关系</li>
<li>需要改造MapReduce以支持流式处理<br><font color="red"><strong>结论：鱼和熊掌不可兼得，Hadoop擅长批处理，不适合流计算</strong></font></li>
</ul>
</li>
</ul>
<h2 id="流计算框架"><a href="#流计算框架" class="headerlink" title="流计算框架"></a>流计算框架</h2><ul>
<li>目前有三类常见的流计算框架和平台：商业级的流计算平台、开源流计算框架、公司为支持自身业务开发的流计算框架</li>
<li>商业级：IBM InfoSphere Streams和IBM StreamBase</li>
<li>较为常见的是开源流计算框架，代表如下：<ul>
<li>Twitter Storm：免费、开源的分布式实时计算系统，可简单、高效、可靠地处理大量的流数据</li>
<li>Yahoo! S4（Simple Scalable Streaming System）：开源流计算平台，是通用的、分布式的、可扩展的、分区容错的、可插拔的流式系统</li>
</ul>
</li>
<li>公司为支持自身业务开发的流计算框架：<ul>
<li>Facebook Puma</li>
<li>Dstream（百度）</li>
<li>银河流数据处理平台（淘宝）</li>
</ul>
</li>
</ul>
<h1 id="流计算处理流程"><a href="#流计算处理流程" class="headerlink" title="流计算处理流程"></a>流计算处理流程</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>传统的数据处理流程，需要先采集数据并存储在关系数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互</li>
<li>传统的数据处理流程隐含了两个前提：<ul>
<li><strong>存储的数据是旧的</strong>。存储的静态数据是过去某一时刻的快照，这些数据在查询时可能已不具备时效性了</li>
<li><strong>需要用户主动发出查询来获取结果</strong></li>
</ul>
</li>
</ul>
<h2 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h2><ul>
<li>数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠</li>
<li>以日志数据为例，由于分布式集群的广泛应用，数据分散存储在不同的机器上，因此需要实时汇总来自不同机器上的日志数据</li>
<li>目前有许多互联网公司发布的开源分布式日志采集系统均可满足每秒数百MB的数据采集和传输需求，如：<ul>
<li>Facebook的Scribe</li>
<li>LinkedIn的Kafka</li>
<li>淘宝的Time Tunnel</li>
<li>基于Hadoop的Chukwa和Flume<br><img src="https://i.loli.net/2019/08/16/uPW8HjeiowtgRVF.png" alt="数据实时采集.png"></li>
</ul>
</li>
</ul>
<h2 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h2><ul>
<li>数据采集系统的基本架构一般有以下三个部分：<ul>
<li>Agent：主动采集数据，并把数据推送到Collector部分</li>
<li>Collector：接收多个Agent的数据，并实现有序、可靠、高性能的转发</li>
<li>Store：存储Collector转发过来的数据（对于流计算不存储数据</li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/16/U3KCNEJhtjHvkPZ.png" alt="数据采集系统基本架构.png"></p>
<ul>
<li>数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果</li>
<li>经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃</li>
</ul>
<h2 id="实时查询服务"><a href="#实时查询服务" class="headerlink" title="实时查询服务"></a>实时查询服务</h2><ul>
<li>实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展示或储存</li>
<li>传统的数据处理流程，用户需要主动发出查询才能获得想要的结果。而在流处理流程中，实时查询服务可以不断更新结果，并将用户所需的结果实时推送给用户</li>
<li>虽然通过对传统的数据处理系统进行定时查询，也可以实现不断地更新结果和结果推送，但通过这样的方式获取的结果，仍然是根据过去某一时刻的数据得到的结果，与实时结果有着本质的区别<blockquote>
<ul>
<li>可见，流处理系统与传统的数据处理系统有如下不同：</li>
</ul>
</blockquote>
</li>
<li>流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据</li>
<li>用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统，获取的是过去某一时刻的结果</li>
<li>流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户</li>
</ul>
<h1 id="流计算应用"><a href="#流计算应用" class="headerlink" title="流计算应用"></a>流计算应用</h1><ul>
<li>流计算是针对流数据的实时计算，可以应用在多种场景中，如Web服务、机器翻译、广告投放、自然语言处理、气候模拟预测等</li>
<li>如百度、淘宝等大型网站中，每天都会产生大量流数据，包括用户的搜索内容、用户的浏览记录等数据。采用流计算进行实时数据分析，可以了解每个时刻的流量变化情况，甚至可以分析用户的实时浏览轨迹，从而进行实时个性化内容推荐</li>
<li>但是，并不是每个应用场景都需要用到流计算的。流计算适合于需要处理持续到达的流数据、对数据处理有较高实时性要求的场景</li>
</ul>
<h1 id="流计算开源框架-–-Storm"><a href="#流计算开源框架-–-Storm" class="headerlink" title="流计算开源框架 – Storm"></a>流计算开源框架 – Storm</h1><h2 id="Storm简介"><a href="#Storm简介" class="headerlink" title="Storm简介"></a>Storm简介</h2><ul>
<li>Twitter Storm是一个免费、开源的分布式实时计算系统，Storm对于实时计算的意义类似于Hadoop对于批处理的意义，Storm可以简单、高效、可靠地处理流数据，并支持多种编程语言</li>
<li>Storm框架可以方便地与数据库系统进行整合，从而开发出强大的实时计算系统</li>
</ul>
<p><img src="https://i.loli.net/2019/08/16/2OAfhoyKXdsYmjV.png" alt="Twitter的分层数据处理架构.png"></p>
<h2 id="Storm的特点"><a href="#Storm的特点" class="headerlink" title="Storm的特点"></a>Storm的特点</h2><ul>
<li>Storm可用于许多领域中，如实时分析、在线机器学习、持续计算、远程RPC、数据提取加载转换等</li>
<li>Storm具有以下主要特点：<ul>
<li>整合性：Storm可方便地与队列系统和数据库系统进行整合</li>
<li>简易的API：Storm的API在使用上即简单又方便</li>
<li>可扩展性：Storm的并行特性使其可以运行在分布式集群中</li>
<li>容错性：Storm可自动进行故障节点的重启、任务的重新分配</li>
<li>可靠的消息处理：Storm保证每个消息都能完整处理 </li>
<li>支持各种编程语言：Storm支持使用各种编程语言来定义任务</li>
<li>快速部署：Storm可以快速进行部署和使用</li>
<li>免费、开源：Storm是一款开源框架，可以免费使用</li>
</ul>
</li>
</ul>
<h2 id="Storm设计思想"><a href="#Storm设计思想" class="headerlink" title="Storm设计思想"></a>Storm设计思想</h2><ul>
<li>Storm主要术语包括Streams、Spouts、Bolts、Topology和Stream Groupings</li>
<li>Streams：Storm将流数据Stream描述成一个无限的Tuple序列，这些Tuple序列会以分布式的方式并行地创建和处理<br><img src="https://i.loli.net/2019/08/16/pyhWGTaef9SscdM.png" alt="Stream.png"></li>
<li>每个tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型</li>
<li>Tuple本来应该是一个Key-Value的Map，由于各个组件间传递的tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List（值列表）</li>
<li>Spout：Storm认为每个Stream都有一个源头，并把这个源头抽象为Spout</li>
<li>通常Spout会从外部数据源（队列、数据库等）读取数据，然后封装成Tuple形式，发送到Stream中。Spout是一个主动的角色，在接口内部有个nextTuple函数，Storm框架会不停的调用该函数<br><img src="https://i.loli.net/2019/08/16/uO6giCcEQ81NrBX.png" alt="Spout.png"></li>
<li>Bolt：Storm将Streams的状态转换过程抽象为Bolt。Bolt即可以处理Tuple，也可以将处理后的Tuple作为新的Streams发送给其他Bolt</li>
<li>Bolt可以执行过滤、函数操作、Join、操作数据库等任何操作</li>
<li>Bolt是一个被动的角色，其接口中有一个execute(Tuple input)方法，在接收到消息之后会调用此函数，用户可以在此方法中执行自己的处理逻辑<br><img src="https://i.loli.net/2019/08/16/TVfov9rABmMWx8D.png" alt="bboult.png"></li>
<li>Topology：Storm将Spouts和Bolts组成的网络抽象成Topology，它可以被提交到Storm集群执行。Topology可视为流转换图，图中节点是一个Spout或Bolt，边则表示Bolt订阅了哪个Stream。当Spout或者Bolt发送元组时，它会把元组发送到每个订阅了该Stream的Bolt上进行处理</li>
<li>Topology里面的每个处理组件（Spout或Bolt）都包含处理逻辑， 而组件之间的连接则表示数据流动的方向</li>
<li>Topology里面的每一个组件都是并行运行的</li>
<li>在Topology里面可以指定每个组件的并行度， Storm会在集群里面分配那么多的线程来同时计算</li>
<li>在Topology的具体实现上，Storm中的Topology定义仅仅是一些Thrift结构体（二进制高性能的通信中间件），支持各种编程语言进行定义<br><img src="https://i.loli.net/2019/08/16/sWUh84QkrPGbjdL.png" alt="Toploly.png"></li>
<li>Stream Groupings：Storm中的Stream Groupings用于告知Topology如何在两个组件间（如Spout和Bolt之间，或者不同的Bolt之间）进行Tuple的传送。每一个Spout和Bolt都可以有多个分布式任务，一个任务在什么时候、以什么方式发送Tuple就是由Stream Groupings来决定的<br><img src="https://i.loli.net/2019/08/16/IHnpzWscD8PEBmJ.png" alt="StramGrouping .png"><blockquote>
<ul>
<li>目前，Storm中的Stream Groupings有如下几种方式：</li>
</ul>
</blockquote>
</li>
</ul>
<ol>
<li>ShuffleGrouping：随机分组，随机分发Stream中的Tuple，保证每个Bolt的Task接收Tuple数量大致一致</li>
<li>FieldsGrouping：按照字段分组，保证相同字段的Tuple分配到同一个Task中</li>
<li>AllGrouping：广播发送，每一个Task都会收到所有的Tuple</li>
<li>GlobalGrouping：全局分组，所有的Tuple都发送到同一个Task中</li>
<li>NonGrouping：不分组，和ShuffleGrouping类似，当前Task的执行会和它的被订阅者在同一个线程中执行</li>
<li>DirectGrouping：直接分组，直接指定由某个Task来执行Tuple的处理</li>
</ol>
<h2 id="Storm框架设计"><a href="#Storm框架设计" class="headerlink" title="Storm框架设计"></a>Storm框架设计</h2><ul>
<li>Storm运行任务的方式与Hadoop类似：Hadoop运行的是MapReduce作业，而Storm运行的是“Topology”</li>
<li>但两者的任务大不相同，主要的不同是：MapReduce作业最终会完成计算并结束运行，而Topology将持续处理消息（直到人为终止）<table>
<thead>
<tr>
<th>Hadoop</th>
<th>Storm</th>
</tr>
</thead>
<tbody><tr>
<td>应用名称</td>
<td>Job</td>
</tr>
<tr>
<td>系统角色</td>
<td>JobTracker</td>
</tr>
<tr>
<td>系统角色</td>
<td>TaskTracker</td>
</tr>
<tr>
<td>组件接口</td>
<td>Map/Reduce</td>
</tr>
</tbody></table>
</li>
<li>Storm集群采用“Master—Worker”的节点方式：<ul>
<li>Master节点运行名为“Nimbus”的后台程序（类似Hadoop中的“JobTracker”），负责在集群范围内分发代码、为Worker分配任务和监测故障</li>
<li>Worker节点运行名为“Supervisor”的后台程序，负责监听分配给它所在机器的工作，即根据Nimbus分配的任务来决定启动或停止Worker进程，一个Worker节点上同时运行若干个Worker进程</li>
</ul>
</li>
<li>Storm使用Zookeeper来作为分布式协调组件，负责Nimbus和多个Supervisor之间的所有协调工作。借助于Zookeeper，若Nimbus进程或Supervisor进程意外终止，重启时也能读取、恢复之前的状态并继续工作，使得Storm极其稳定<br><img src="https://i.loli.net/2019/08/16/OxlvwgL9Juq8Zam.png" alt="Storm集群架构示意图.png"></li>
</ul>
<ol>
<li>worker:每个worker进程都属于一个特定的Topology，每个Supervisor节点的worker可以有多个，每个worker对Topology中的每个组件（Spout或 Bolt）运行一个或者多个executor线程来提供task的运行服务</li>
<li>executor：executor是产生于worker进程内部的线程，会执行同一个组件的一个或者多个task。</li>
<li>task:实际的数据处理由task完成，在Topology的生命周期中，每个组件的task数目是不会发生变化的，而executor的数目却不一定。executor数目小于等于task的数目，默认情况下，二者是相等的<br><img src="https://i.loli.net/2019/08/16/ivGbezZRMup6jDh.png" alt="worker excuter和task关系.png"><blockquote>
<ul>
<li>基于这样的架构设计，Storm的工作流程如下图所示：</li>
</ul>
</blockquote>
</li>
</ol>
<ul>
<li>所有Topology任务的提交必须在Storm客户端节点上进行，提交后，由Nimbus节点分配给其他Supervisor节点进行处理</li>
<li>Nimbus节点首先将提交的Topology进行分片，分成一个个Task，分配给相应的Supervisor，并将Task和Supervisor相关的信息提交到Zookeeper集群上</li>
<li>Supervisor会去Zookeeper集群上认领自己的Task，通知自己的Worker进程进行Task的处理</li>
<li>说明：在提交了一个Topology之后，Storm就会创建Spout/Bolt实例并进行序列化。之后，将序列化的组件发送给所有的任务所在的机器(即Supervisor节点)，在每一个任务上反序列化组件<br><img src="https://i.loli.net/2019/08/16/GHqlOyPUjadZnzF.png" alt="Storm工作流程示意图.png"><h1 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h1><h2 id="Spark-Streaming设计"><a href="#Spark-Streaming设计" class="headerlink" title="Spark Streaming设计"></a>Spark Streaming设计</h2></li>
<li>Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里<br><img src="https://i.loli.net/2019/08/16/KvQyUpZ9Rs1c8uH.png" alt="Spark Streaming支持的输入、输出数据源.png"></li>
<li>Spark Streaming的基本原理是将实时输入数据流以时间片（秒级）为单位进行拆分，然后经Spark引擎以类似批处理的方式处理每个时间片数据<br><img src="https://i.loli.net/2019/08/16/wYuM6RTUAn9B5vi.png" alt="Spark Streaming执行流程.png"></li>
<li>Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段的DStream，每一段数据转换为Spark中的RDD，并且对DStream的操作都最终转变为对相应的RDD的操作<br><img src="https://i.loli.net/2019/08/16/7uiej3OvUBofWPp.png" alt="DStream操作示意图.png"></li>
</ul>
<h2 id="Spark-Streaming与Storm的对比"><a href="#Spark-Streaming与Storm的对比" class="headerlink" title="Spark Streaming与Storm的对比"></a>Spark Streaming与Storm的对比</h2><ul>
<li>Spark Streaming和Storm最大的区别在于，Spark Streaming无法实现毫秒级的流计算，而Storm可以实现毫秒级响应</li>
<li>Spark Streaming构建在Spark上，一方面是因为Spark的低延迟执行引擎（100ms+）可以用于实时计算，另一方面，相比于Storm，RDD数据集更容易做高效的容错处理</li>
<li>Spark Streaming采用的小批量处理的方式使得它可以同时兼容批量和实时数据处理的逻辑和算法，因此，方便了一些需要历史数据和实时数据联合分析的特定应用场合</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2018/10/06/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/06/Spark/" itemprop="url">Spark</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-10-06T18:53:15+08:00">
                2018-10-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">Bigdata</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Spark概述"><a href="#Spark概述" class="headerlink" title="Spark概述"></a>Spark概述</h1><h2 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h2><ul>
<li>Spark最初由美国加州伯克利大学（UCBerkeley）的AMP实验室于2009年开发，是基于内存计算的大数据并行计算框架，可用于构建大型的、低延迟的数据分析应用程序</li>
<li>2013年Spark加入Apache孵化器项目后发展迅猛，如今已成为Apache软件基金会最重要的三大分布式计算系统开源项目之一（Hadoop、Spark、Storm）</li>
<li>Spark具有如下几个主要特点：<ul>
<li>运行速度快：使用DAG执行引擎以支持循环数据流与内存计算</li>
<li>容易使用：支持使用Scala、Java、Python和R语言进行编程，可以通过Spark Shell进行交互式编程   </li>
<li>通用性：Spark提供了完整而强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件</li>
<li>运行模式多样：可运行于独立的集群模式中，可运行于Hadoop中，也可运行于Amazon EC2等云环境中，并且可以访问HDFS、Cassandra、HBase、Hive等多种数据源</li>
</ul>
</li>
</ul>
<h2 id="Scala简介"><a href="#Scala简介" class="headerlink" title="Scala简介"></a>Scala简介</h2><ul>
<li>Scala是一门现代的多范式编程语言，运行于Java平台（JVM，Java 虚拟机），并兼容现有的Java程序</li>
<li>Scala的特性：<ul>
<li>Scala具备强大的并发性，支持函数式编程，可以更好地支持分布式系统</li>
<li>Scala语法简洁，能提供优雅的API</li>
<li>Scala兼容Java，运行速度快，且能融合到Hadoop生态圈中 </li>
</ul>
</li>
<li>Scala是Spark的主要编程语言，但Spark还支持Java、Python、R作为编程语言</li>
<li>Scala的优势是提供了REPL（Read-Eval-Print Loop，交互式解释器），提高程序开发效率</li>
</ul>
<h2 id="Spark与Hadoop的比较"><a href="#Spark与Hadoop的比较" class="headerlink" title="Spark与Hadoop的比较"></a>Spark与Hadoop的比较</h2><ul>
<li>Hadoop存在如下一些缺点：<ul>
<li>表达能力有限</li>
<li>磁盘IO开销大</li>
<li>延迟高</li>
<li>任务之间的衔接涉及IO开销，在前一个任务执行完成之前，其他任务就无法开始，难以胜任复杂、多阶段的计算任务 </li>
</ul>
</li>
<li>相比于Hadoop MapReduce，Spark主要具有如下优点：<ul>
<li>Spark的计算模式也属于MapReduce，但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活</li>
<li>Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高</li>
<li>Spark基于<strong>DAG</strong>的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制 </li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/15/vr9S5IBqmax1MFf.png" alt="Spark执行流程.png"></p>
<ul>
<li>使用Hadoop进行迭代计算非常耗资源</li>
<li>Spark将数据载入内存后，之后的迭代计算都可以直接使用内存中的中间结果作运算，避免了从磁盘中频繁读取数据</li>
</ul>
<p><img src="https://i.loli.net/2019/08/15/TVUy9Nz7DWkqpZ5.png" alt="Hadoop与Spark执行逻辑回归时间对比.png"></p>
<h1 id="Spark生态系统"><a href="#Spark生态系统" class="headerlink" title="Spark生态系统"></a>Spark生态系统</h1><blockquote>
<p>在实际应用中，大数据处理主要包括以下三个类型：</p>
<ul>
<li>复杂的批量数据处理：通常时间跨度在数十分钟到数小时之间</li>
<li>基于历史数据的交互式查询：通常时间跨度在数十秒到数分钟之间</li>
<li>基于实时数据流的数据处理：通常时间跨度在数百毫秒到数秒之间<br>当同时存在以上三种场景时，就需要同时部署三种不同的软件<br>比如: MapReduce  /  Impala  /  Storm</li>
</ul>
</blockquote>
<blockquote>
<p>这样做难免会带来一些问题： </p>
<ul>
<li>不同场景之间输入输出数据无法做到无缝共享，通常需要进行数据格式的转换</li>
<li>不同的软件需要不同的开发和维护团队，带来了较高的使用成本</li>
<li>比较难以对同一个集群中的各个系统进行统一的资源协调和分配</li>
</ul>
</blockquote>
<ul>
<li><p>Spark的设计遵循<strong>“一个软件栈满足不同应用场景”</strong>的理念，逐渐形成了一套完整的生态系统</p>
</li>
<li><p>既能够提供内存计算框架，也可以支持SQL即席查询、实时流式计算、机器学习和图计算等</p>
</li>
<li><p>Spark可以部署在资源管理器YARN之上，提供一站式的大数据解决方案</p>
</li>
<li><p>因此，Spark所提供的生态系统足以应对上述三种场景，即同时支持批处理、交互式查询和流数据处理</p>
</li>
<li><p>Spark的生态系统主要包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX 等组件</p>
</li>
</ul>
<p><img src="https://i.loli.net/2019/08/15/eGqLAQjrxYDIiB4.png" alt="BDAS架构.png"></p>
<table>
<thead>
<tr>
<th>应用场景</th>
<th>时间跨度</th>
<th>其他框架</th>
<th>Spark生态系统中的组件</th>
</tr>
</thead>
<tbody><tr>
<td>复杂的批量数据处理</td>
<td>小时级</td>
<td>MapReduce、Hive</td>
<td>Spark</td>
</tr>
<tr>
<td>基于历史数据的交互式查询</td>
<td>分钟级、秒级</td>
<td>Impala、Dremel、Drill</td>
<td>Spark SQL</td>
</tr>
<tr>
<td>基于实时数据流的数据处理</td>
<td>毫秒、秒级</td>
<td>Storm、S4</td>
<td>Spark Streaming</td>
</tr>
<tr>
<td>基于历史数据的数据挖掘</td>
<td>-</td>
<td>Mahout</td>
<td>MLlib</td>
</tr>
<tr>
<td>图结构数据的处理</td>
<td>-</td>
<td>Pregel、Hama</td>
<td>GraphX</td>
</tr>
</tbody></table>
<h1 id="Spark运行架构"><a href="#Spark运行架构" class="headerlink" title="Spark运行架构"></a>Spark运行架构</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ul>
<li>RDD：是Resillient Distributed Dataset（弹性分布式数据集）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型</li>
<li>DAG：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系</li>
<li>Executor：是运行在工作节点（WorkerNode）的一个进程，负责运行Task</li>
<li>Application：用户编写的Spark应用程序</li>
<li>Task：运行在Executor上的工作单元 </li>
<li>Job：一个Job包含多个RDD及作用于相应RDD上的各种操作</li>
<li>Stage：是Job的基本调度单位，一个Job会分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集</li>
</ul>
<h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><ul>
<li>Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor）</li>
<li>资源管理器可以自带或Mesos或YARN<blockquote>
<p>与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点：</p>
</blockquote>
</li>
<li>一是利用多线程来执行具体的任务，减少任务的启动开销</li>
<li>二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，有效减少IO开销<br><img src="https://i.loli.net/2019/08/15/ayNnsW1oELcmMrV.png" alt="Spark运行架构.png"></li>
<li>一个Application由一个Driver和若干个Job构成，一个Job由多个Stage构成，一个Stage由多个没有Shuffle关系的Task组成</li>
<li>当执行一个Application时，Driver会向集群管理器申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果会返回给Driver，或者写到HDFS或者其他数据库中<br><img src="https://i.loli.net/2019/08/15/TcBFl2LCj9PdIGM.png" alt="Spark中各种概念之间的相互关系.png"></li>
</ul>
<h2 id="Spark运行基本流程"><a href="#Spark运行基本流程" class="headerlink" title="Spark运行基本流程"></a>Spark运行基本流程</h2><p><img src="https://i.loli.net/2019/08/15/gHdGTBRl4L8Ekhy.png" alt="Spark运行基本流程图.png"></p>
<ol>
<li>运行环境，即由Driver创建一个SparkContext，进行资源的申请、任务的分配和监控</li>
<li>资源管理器为Executor分配资源，并启动Executor进程</li>
<li>SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，并提供应用程序代码</li>
<li>Task在Executor上运行，把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源 <blockquote>
<p><em>总体而言，Spark运行架构具有以下特点：</em></p>
</blockquote>
</li>
<li>每个Application都有自己专属的Executor进程，并且该进程在Application运行期间一直驻留。Executor进程以多线程的方式运行Task</li>
<li>Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可</li>
<li>Task采用了数据本地性和推测执行等优化机制</li>
</ol>
<h2 id="RDD-运行原理"><a href="#RDD-运行原理" class="headerlink" title="RDD 运行原理"></a>RDD 运行原理</h2><ol>
<li>设计背景</li>
</ol>
<ul>
<li>许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，共同之处是，不同计算阶段之间会重用中间结果</li>
<li>目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销</li>
<li>RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，避免中间数据存储</li>
</ul>
<ol start="2">
<li>RDD概念</li>
</ol>
<ul>
<li>一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算</li>
<li>RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和group by）而创建得到新的RDD</li>
<li>RDD提供了一组丰富的操作以支持常见的数据运算，分为“动作”（Action）和“转换”（Transformation）两种类型</li>
<li>RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改（不适合网页爬虫）</li>
<li>表面上RDD的功能很受限、不够强大，实际上RDD已经被实践证明可以高效地表达许多框架的编程模型（比如MapReduce、SQL、Pregel）</li>
<li>Spark用Scala语言实现了RDD的API，程序员可以通过调用API实现对RDD的各种操作</li>
<li>RDD典型的执行过程如下：<ul>
<li>RDD读入外部数据源进行创建</li>
<li>RDD经过一系列的转换（Transformation）操作，每一次都会产生不同的RDD，供给下一个转换操作使用</li>
<li>最后一个RDD经过“动作”操作进行转换，并输出到外部数据源 <blockquote>
<ul>
<li>这一系列处理称为一个Lineage（血缘关系），即DAG拓扑排序的结果<br>优点：惰性调用、管道化、避免同步等待、不需要保存中间结果、每次操作变得简单<br><img src="https://i.loli.net/2019/08/15/a1Y9sXAxECQTD6I.png" alt="RDD执行过程的一个实例.png"></li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<ol start="3">
<li>RDD特性<br>Spark采用RDD以后能够实现高效计算的原因主要在于：</li>
<li>高效的容错性<ul>
<li>现有容错机制：数据复制或者记录日志</li>
<li>RDD：血缘关系、重新计算丢失分区、无需回滚系统、重算过程在不同节点之间并行、只记录粗粒度的操作</li>
</ul>
</li>
<li>中间结果持久化到内存，数据在内存中的多个RDD操作之间进行传递，避免了不必要的读写磁盘开销</li>
<li>存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化</li>
<li>RDD之间的依赖关系<br><img src="https://i.loli.net/2019/08/15/v71RxrEqTLsDjNI.png" alt="窄依赖与宽依赖的区别.png"></li>
</ol>
<ul>
<li>窄依赖表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区</li>
<li>宽依赖则表现为存在一个父RDD的一个分区对应一个子RDD的多个分区</li>
</ul>
<ol start="5">
<li>Stage的划分<blockquote>
<ul>
<li>Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分Stage，具体划分方法是：</li>
</ul>
</blockquote>
</li>
</ol>
<ul>
<li>在DAG中进行反向解析，遇到宽依赖就断开</li>
<li>遇到窄依赖就把当前的RDD加入到Stage中</li>
<li>　将窄依赖尽量划分在同一个Stage中，可以实现流水线计算<blockquote>
<ul>
<li>被分成三个Stage，在Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作<br><img src="https://i.loli.net/2019/08/15/iEwLC2qAhSlpY6M.png" alt="根据RDD分区的依赖关系划分Stage.png"></li>
</ul>
</blockquote>
</li>
<li>流水线操作实例<ul>
<li>分区7通过map操作生成的分区9，可以不用等待分区8到分区10这个map操作的计算结束，而是继续进行union操作，得到分区13，这样流水线执行大大提高了计算的效率</li>
</ul>
</li>
<li>Stage的类型包括两种：ShuffleMapStage和ResultStage，具体如下：<ul>
<li>ShuffleMapStage：不是最终的Stage，在它之后还有其他Stage，所以，它的输出一定需要经过Shuffle过程，并作为后续Stage的输入；这种Stage是以Shuffle为输出边界，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出，其输出可以是另一个Stage的开始；在一个Job里可能有该类型的Stage，也可能没有该类型Stage；</li>
<li>ResultStage：最终的Stage，没有输出，而是直接产生结果或存储。这种Stage是直接输出结果，其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出。在一个Job里必定有该类型Stage。<br>因此，一个Job含有一个或多个Stage，其中至少含有一个ResultStage</li>
</ul>
</li>
</ul>
<ol start="6">
<li>RDD运行过程<blockquote>
<ul>
<li>通过上述对RDD概念、依赖关系和Stage划分的介绍，结合之前介绍的Spark运行基本流程，再总结一下RDD在Spark架构中的运行过程：</li>
</ul>
</blockquote>
</li>
<li>创建RDD对象；</li>
<li>SparkContext负责计算RDD之间的依赖关系，构建DAG；</li>
<li>DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task，每个Task会被TaskScheduler分发给各个WorkerNode上的Executor去执行。<br><img src="https://i.loli.net/2019/08/15/N6DdUuQT8E5MXRn.png" alt="RDD在Spark中的运行过程.png"></li>
</ol>
<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><ul>
<li>Spark SQL在Hive兼容层面仅依赖HiveQL解析、Hive元数据，也就是说，从HQL被解析成抽象语法树（AST）起，就全部由Spark SQL接管了。Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责<br><img src="https://i.loli.net/2019/08/15/B4gFD6Nnx75dblL.png" alt="Spark SQL架构.png"><ul>
<li>Spark SQL增加了SchemaRDD（即带有Schema信息的RDD），使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据</li>
<li>Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范<br><img src="https://i.loli.net/2019/08/15/irYBgfjqWhAZCpx.png" alt="Spark SQL支持的数据格式和编程语言.png"></li>
</ul>
</li>
</ul>
<h1 id="Spark的部署和应用方式"><a href="#Spark的部署和应用方式" class="headerlink" title="Spark的部署和应用方式"></a>Spark的部署和应用方式</h1><h2 id="Spark三种部署方式"><a href="#Spark三种部署方式" class="headerlink" title="Spark三种部署方式"></a>Spark三种部署方式</h2><ul>
<li>Standalone（类似于MapReduce1.0，slot为资源分配单位）</li>
<li>Spark on Mesos（和Spark有血缘关系，更好支持Mesos）</li>
<li>Spark on YARN<br><img src="https://i.loli.net/2019/08/15/IXHGPSlbvZxKoBa.png" alt="Spark on Yarn.png"></li>
</ul>
<h2 id="从Hadoop-Storm架构转向Spark架构"><a href="#从Hadoop-Storm架构转向Spark架构" class="headerlink" title="从Hadoop+Storm架构转向Spark架构"></a>从Hadoop+Storm架构转向Spark架构</h2><p><img src="https://i.loli.net/2019/08/15/Fnlm51KXpfL6NjB.png" alt="采用Hadoop+Storm部署方式的一个案例.png"></p>
<blockquote>
<ul>
<li>用Spark架构具有如下优点：</li>
</ul>
<ul>
<li>实现一键式安装和配置、线程级别的任务监控和告警</li>
<li>降低硬件集群、软件维护、任务监控和应用开发的难度</li>
<li>便于做成统一的硬件、计算平台资源池</li>
</ul>
<ul>
<li>需要说明的是，Spark Streaming无法实现毫秒级的流计算，因此，对于需要毫秒级实时响应的企业应用而言，仍然需要采用流计算框架（如Storm）<br><img src="https://i.loli.net/2019/08/15/vfjaYQR8shSqDJt.png" alt="用Spark架构满足批处理和流处理需求.png"></li>
</ul>
</blockquote>
<h2 id="Hadoop和Spark的统一部署"><a href="#Hadoop和Spark的统一部署" class="headerlink" title="Hadoop和Spark的统一部署"></a>Hadoop和Spark的统一部署</h2><ul>
<li>由于Hadoop生态系统中的一些组件所实现的功能，目前还是无法由Spark取代的，比如，Storm</li>
<li>现有的Hadoop组件开发的应用，完全转移到Spark上需要一定的成本<blockquote>
<ul>
<li>不同的计算框架统一运行在YARN中，可以带来如下好处：</li>
</ul>
</blockquote>
</li>
<li>计算资源按需伸缩</li>
<li>不用负载应用混搭，集群利用率高</li>
<li>共享底层存储，避免数据跨集群迁移<br><img src="https://i.loli.net/2019/08/15/RPfTV8uejXhgvlw.png" alt="Hadoop和Spark的统一部署.png"></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2018/09/27/NoSQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/27/NoSQL/" itemprop="url">NoSQL</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-27T19:28:50+08:00">
                2018-09-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">Bigdata</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="NoSQL兴起原因"><a href="#NoSQL兴起原因" class="headerlink" title="NoSQL兴起原因"></a>NoSQL兴起原因</h1><ol>
<li>关系数据库已经无法满足Web2.0的需求。主要表现在以下几个方面：<ul>
<li>无法满足海量数据的管理需求</li>
<li>无法满足数据高并发的需求</li>
<li>无法满足高可扩展性和高可用性的需求</li>
</ul>
</li>
<li>One size fits all”模式很难适用于截然不同的业务场景<ul>
<li>关系模型作为统一的数据模型既被用于数据分析，也被用于在线业务。但这两者一个强调高吞吐，一个强调低延时，已经演化出完全不同的架构。用同一套模型来抽象显然是不合适的</li>
<li>Hadoop就是针对数据分析</li>
<li>MongoDB、Redis等是针对在线业务，两者都抛弃了关系模型</li>
</ul>
</li>
<li>关系数据库的关键特性包括完善的事务机制和高效的查询机制。但是，关系数据库引以为傲的两个关键特性，到了Web2.0时代却成了鸡肋，主要表现在以下几个方面：<ul>
<li>Web2.0网站系统通常不要求严格的数据库事务</li>
<li>Web2.0并不要求严格的读写实时性</li>
<li>Web2.0通常不包含大量复杂的SQL查询（去结构化，存储空间换取更好的查询性能）</li>
</ul>
</li>
</ol>
<h1 id="NoSQL与关系型数据库的比较"><a href="#NoSQL与关系型数据库的比较" class="headerlink" title="NoSQL与关系型数据库的比较"></a>NoSQL与关系型数据库的比较</h1><table>
<thead>
<tr>
<th align="left">比较标准</th>
<th align="left">RDBMS</th>
<th align="left">NoSQL</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="left">数据库原理</td>
<td align="left">完全支持</td>
<td align="left">部分支持</td>
<td align="left">RDBMS有关系代数理论作为基础,NoSQL没有统一的理论基础</td>
</tr>
<tr>
<td align="left">数据规模</td>
<td align="left">大</td>
<td align="left">超大</td>
<td align="left">RDBMS很难实现横向扩展，纵向扩展的空间也比较有限，性能会随着数据规模的增大而降低,NoSQL可以很容易通过添加更多设备来支持更大规模的数据</td>
</tr>
<tr>
<td align="left">数据库模式</td>
<td align="left">固定</td>
<td align="left">灵活</td>
<td align="left">RDBMS需要定义数据库模式，严格遵守数据定义和相关约束条件,NoSQL不存在数据库模式，可以自由灵活定义并存储各种不同类型的数据</td>
</tr>
<tr>
<td align="left">查询效率</td>
<td align="left">快</td>
<td align="left">可以实现高效的简单查询，但是不具备高度结构化查询等特性，复杂查询的性能不尽人意</td>
<td align="left">RDBMS借助于索引机制可以实现快速查询（包括记录查询和范围查询）NoSQL数据库没有面向复杂查询的索引，虽然NoSQL可以使用MapReduce来加速查询，但是，在复杂查询方面的性能仍然不如RDBMS</td>
</tr>
<tr>
<td align="left">一致性</td>
<td align="left">强一致性</td>
<td align="left">弱一致性</td>
<td align="left">RDBMS严格遵守事务ACID模型，可以保证事务强一致性,很多NoSQL数据库放松了对事务ACID四性的要求，而是遵守BASE模型，只能保证最终一致性</td>
</tr>
<tr>
<td align="left">数据完整性</td>
<td align="left">容易实现</td>
<td align="left">很难实现</td>
<td align="left">任何一个RDBMS都可以很容易实现数据完整性，比如通过主键或者非空约束来实现实体完整性，通过主键、外键来实现参照完整性，通过约束或者触发器来实现用户自定义完整性,但是，在NoSQL数据库却无法实现</td>
</tr>
<tr>
<td align="left">扩展性</td>
<td align="left">一般</td>
<td align="left">好</td>
<td align="left">RDBMS很难实现横向扩展，纵向扩展的空间也比较有限,NoSQL在设计之初就充分考虑了横向扩展的需求，可以很容易通过添加廉价设备实现扩展</td>
</tr>
<tr>
<td align="left">可用性</td>
<td align="left">好</td>
<td align="left">很好</td>
<td align="left">RDBMS在任何时候都以保证数据一致性为优先目标，其次才是优化系统性能，随着数据规模的增大，RDBMS为了保证严格的一致性，只能提供相对较弱的可用性,大多数NoSQL都能提供较高的可用性</td>
</tr>
<tr>
<td align="left">标准化</td>
<td align="left">是</td>
<td align="left">否</td>
<td align="left">RDBMS已经标准化（SQL）,NoSQL还没有行业标准，不同的NoSQL数据库都有自己的查询语言，很难规范应用程序接口,StoneBraker认为：NoSQL缺乏统一查询语言，将会拖慢NoSQL发展</td>
</tr>
<tr>
<td align="left">技术支持</td>
<td align="left">高</td>
<td align="left">低</td>
<td align="left">RDBMS经过几十年的发展，已经非常成熟，Oracle等大型厂商都可以提供很好的技术支持,NoSQL在技术支持方面仍然处于起步阶段，还不成熟，缺乏有力的技术支持</td>
</tr>
<tr>
<td align="left">可维护性</td>
<td align="left">复杂</td>
<td align="left">复杂</td>
<td align="left">RDBMS需要专门的数据库管理员(DBA)维护 ,NoSQL数据库虽然没有DBMS复杂，也难以维护</td>
</tr>
</tbody></table>
<p><strong>总结</strong></p>
<ol>
<li>关系数据库<br>优势：以完善的关系代数理论作为基础，有严格的标准，支持事务ACID四性，借助索引机制可以实现高效的查询，技术成熟，有专业公司的技术支持<br>劣势：可扩展性较差，无法较好支持海量数据存储，数据模型过于死板、无法较好支持Web2.0应用，事务机制影响了系统的整体性能等</li>
<li>NoSQL数据库<br>优势：可以支持超大规模数据存储，灵活的数据模型可以很好地支持Web2.0应用，具有强大的横向扩展能力等<br>劣势：缺乏数学理论基础，复杂查询性能不高，大都不能实现事务强一致性，很难实现数据完整性，技术尚不成熟，缺乏专业团队的技术支持，维护较困难等</li>
</ol>
<ul>
<li>关系数据库应用场景：电信、银行等领域的关键业务系统，需要保证强事务一致性</li>
<li>NoSQL数据库应用场景：互联网企业、传统企业的非关键业务（比如数据分析）</li>
<li>采用混合架构</li>
<li>案例：亚马逊公司就使用不同类型的数据库来支撑它的电子商务应用<ul>
<li>对于“购物篮”这种临时性数据，采用键值存储会更加高效</li>
<li>当前的产品和订单信息则适合存放在关系数据库中</li>
<li>大量的历史订单信息则适合保存在类似MongoDB的文档数据库中</li>
</ul>
</li>
</ul>
<h1 id="NoSQL的四大类型"><a href="#NoSQL的四大类型" class="headerlink" title="NoSQL的四大类型"></a>NoSQL的四大类型</h1><blockquote>
<p>NoSQL数据库虽然数量众多，但是，归结起来，典型的NoSQL数据库通常包括键值数据库、列族数据库、文档数据库和图形数据库.</p>
</blockquote>
<p><img src="https://i.loli.net/2019/08/14/kMvYIWtdT8xOosR.png" alt="NoSQL-1.png"><br><img src="https://i.loli.net/2019/08/14/p5rvDxPIwzKhVAa.png" alt="NoSQL-2.png"><br><img src="https://i.loli.net/2019/08/14/OseyBMhXY2QZjl9.jpg" alt="四类数据库.jpg"></p>
<h2 id="键值数据库"><a href="#键值数据库" class="headerlink" title="键值数据库"></a>键值数据库</h2><table>
<thead>
<tr>
<th>相关产品</th>
<th>Redis、Riak、SimpleDB、Chordless、Scalaris、Memcached</th>
</tr>
</thead>
<tbody><tr>
<td>数据模型</td>
<td>键/值对,键是一个字符串对象,值可以是任意类型的数据，比如整型、字符型、数组、列表、集合等</td>
</tr>
<tr>
<td>典型应用</td>
<td>涉及频繁读写、拥有简单数据模型的应用 ,内容缓存，比如会话、配置文件、参数、购物车等,存储配置和用户数据信息的移动应用</td>
</tr>
<tr>
<td>优点</td>
<td>扩展性好，灵活性好，大量写操作时性能高</td>
</tr>
<tr>
<td>缺点</td>
<td>无法存储结构化信息，条件查询效率较低</td>
</tr>
<tr>
<td>不适用情形</td>
<td>不是通过键而是通过值来查：键值数据库根本没有通过值查询的途径 ,要存储数据之间的关系：在键值数据库中，不能通过两个或两个以上的键来关联数据 需要事务的支持,在一些键值数据库中，产生故障时，不可以回滚</td>
</tr>
<tr>
<td>使用者</td>
<td>(Redis）、GitHub（Riak）、BestBuy（Riak）、Twitter（Redis和Memcached）、StackOverFlow（Redis）、Instagram （Redis）、Youtube（Memcached）、Wikipedia（Memcached）</td>
</tr>
</tbody></table>
<p><img src="https://i.loli.net/2019/08/14/MFu8kUZoDWVYthr.png" alt="键值数据库成为理想的缓冲层解决方案.png"><br>Redis有时候会被人们称为“强化版的Memcached” 支持持久化、数据恢复、更多数据类型</p>
<h2 id="列族数据库"><a href="#列族数据库" class="headerlink" title="列族数据库"></a>列族数据库</h2><table>
<thead>
<tr>
<th>相关产品</th>
<th>BigTable、HBase、Cassandra、HadoopDB、GreenPlum、PNUTS</th>
</tr>
</thead>
<tbody><tr>
<td>数据模型</td>
<td>列族</td>
</tr>
<tr>
<td>典型应用</td>
<td>“分布式数据存储与管理,数据在地理上分布于多个数据中心的应用程序,可以容忍副本中存在短期不一致情况的应用程序,拥有动态字段的应用程序 拥有潜在大量数据的应用程序，大到几百TB的数据</td>
</tr>
<tr>
<td>优点</td>
<td>查找速度快，可扩展性强，容易进行分布式扩展，复杂性低</td>
</tr>
<tr>
<td>缺点</td>
<td>功能较少，大都不支持强事务一致性</td>
</tr>
<tr>
<td>不适用情形</td>
<td>需要ACID事务支持的情形，Cassandra等产品就不适用</td>
</tr>
<tr>
<td>使用者</td>
<td>Ebay（Cassandra）、Instagram（Cassandra）、NASA（Cassandra）、Twitter（Cassandra and HBase）、Facebook（HBase）、Yahoo!（HBase）</td>
</tr>
</tbody></table>
<h2 id="文档数据库"><a href="#文档数据库" class="headerlink" title="文档数据库"></a>文档数据库</h2><blockquote>
<p>“文档”其实是一个数据记录，这个记录能够对包含的数据类型和内容进行“自我描述”。XML文档、HTML文档和JSON 文档就属于这一类。SequoiaDB就是使用JSON格式的文档数据库，它的存储的数据是这样的：</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> &quot;ID&quot;:1,</span><br><span class="line"> &quot;NAME&quot;:&quot;SequiaDB&quot;,</span><br><span class="line"> &quot;Tel&quot;: &#123;</span><br><span class="line">      &quot;Office&quot;:&quot;123456&quot;,&quot;Tel&quot;：“1273928”</span><br><span class="line">        &#125;</span><br><span class="line">   &quot;Addr&quot;: &quot;China,GZ&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>关系数据库：<br>必须有schema信息才能理解数据的含义，如学生（学号，姓名，性别，年龄，系，年级）<br>（1001，张三，男，20，计算机，2002）</p>
</li>
<li><p>一个XML文档：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>数据是不规则的，每一条记录包含了所有的有关“SequoiaDB”的信息而没有任何外部的引用，这条记录就是“自包含”的</p>
</li>
<li><p>这使得记录很容易完全移动到其他服务器，因为这条记录的所有信息都包含在里面了，不需要考虑还有信息在别的表没有一起迁移走</p>
</li>
<li><p>同时，因为在移动过程中，只有被移动的那一条记录（文档）需要操作，而不像关系型中每个有关联的表都需要锁住来保证一致性，这样一来ACID的保证就会变得更快速，读写的速度也会有很大的提升</p>
</li>
</ul>
<table>
<thead>
<tr>
<th>相关产品</th>
<th>MongoDB、CouchDB、Terrastore、ThruDB、RavenDB、SisoDB、RaptorDB、CloudKit、Perservere、Jackrabbit</th>
</tr>
</thead>
<tbody><tr>
<td>数据模型</td>
<td>键/值,值（value）是版本化的文档</td>
</tr>
<tr>
<td>典型应用</td>
<td>存储、索引并管理面向文档的数据或者类似的半结构化数据,比如，用于后台具有大量读写操作的网站、使用JSON数据结构的应用、使用嵌套结构等非规范化数据的应用程序</td>
</tr>
<tr>
<td>优点</td>
<td>性能好（高并发），灵活性高，复杂性低，数据结构灵活,提供嵌入式文档功能，将经常查询的数据存储在同一个文档中,既可以根据键来构建索引，也可以根据内容构建索引</td>
</tr>
<tr>
<td>缺点</td>
<td>缺乏统一的查询语法</td>
</tr>
<tr>
<td>不适用情形</td>
<td>在不同的文档上添加事务。文档数据库并不支持文档间的事务，如果对这方面有需求则不应该选用这个解决方案</td>
</tr>
<tr>
<td>使用者</td>
<td>（MongoDB）、SAP （MongoDB）、Codecademy （MongoDB）、Foursquare （MongoDB）、NBC News （RavenDB）</td>
</tr>
</tbody></table>
<h2 id="图数据库"><a href="#图数据库" class="headerlink" title="图数据库"></a>图数据库</h2><table>
<thead>
<tr>
<th>相关产品</th>
<th>Neo4J、OrientDB、InfoGrid、Infinite Graph、GraphDB</th>
</tr>
</thead>
<tbody><tr>
<td>数据模型</td>
<td>图结构</td>
</tr>
<tr>
<td>典型应用</td>
<td>专门用于处理具有高度相互关联关系的数据，比较适合于社交网络、模式识别、依赖分析、推荐系统以及路径寻找等问题</td>
</tr>
<tr>
<td>优点</td>
<td>灵活性高，支持复杂的图形算法，可用于构建复杂的关系图谱</td>
</tr>
<tr>
<td>缺点</td>
<td>复杂性高，只能支持一定的数据规模</td>
</tr>
<tr>
<td>使用者</td>
<td>Adobe（Neo4J）、Cisco（Neo4J）、T-Mobile（Neo4J）</td>
</tr>
</tbody></table>
<h1 id="NoSQL的三大基石"><a href="#NoSQL的三大基石" class="headerlink" title="NoSQL的三大基石"></a>NoSQL的三大基石</h1><p><img src="https://i.loli.net/2019/08/14/3PKEurnWJwMZlqf.png" alt="NoSQL三大基石.png"></p>
<h2 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h2><ul>
<li><strong>C（Consistency）</strong>：一致性，是指任何一个读操作总是能够读到之前完成的写操作的结果，也就是在分布式环境中，多点的数据是一致的，或者说，所有节点在同一时间具有相同的数据</li>
<li><strong>A:（Availability）</strong>：可用性，是指快速获取数据，可以在确定的时间内返回操作结果，保证每个请求不管成功或者失败都有响应；</li>
<li><strong>P（Tolerance of Network Partition）</strong>：分区容忍性，是指当出现网络分区的情况时（即系统中的一部分节点无法和其他节点进行通信），分离的系统也能够正常运行，也就是说，系统中任意信息的丢失或失败不会影响系统的继续运作。</li>
</ul>
<blockquote>
<p>CAP理论告诉我们，一个分布式系统不可能同时满足一致性、可用性和分区容忍性这三个需求，最多只能同时满足其中两个，正所谓“鱼和熊掌不可兼得”。</p>
</blockquote>
<p><img src="https://i.loli.net/2019/08/14/xeFK2I74shtjcyA.png" alt="CAP.png"></p>
<ol>
<li>CA：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。很显然，这种做法会严重影响系统的可扩展性。传统的关系数据库（MySQL、SQL Server和PostgreSQL），都采用了这种设计原则，因此，扩展性都比较差</li>
<li>CP：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务</li>
<li>AP：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据<br><img src="https://i.loli.net/2019/08/14/nJDRbsSMBaPUmhX.png" alt="不同产品在CAP理论下的不同设计原则 .png"></li>
</ol>
<h2 id="BASE"><a href="#BASE" class="headerlink" title="BASE"></a>BASE</h2><blockquote>
<p>说起BASE（Basically Availble, Soft-state, Eventual consistency），不得不谈到ACID。</p>
</blockquote>
<table>
<thead>
<tr>
<th>ACID</th>
<th>BASE</th>
</tr>
</thead>
<tbody><tr>
<td>原子性(Atomicity)</td>
<td>基本可用(Basically Available)</td>
</tr>
<tr>
<td>一致性(Consistency)</td>
<td>软状态/柔性事务(Soft state)</td>
</tr>
<tr>
<td>隔离性(Isolation)</td>
<td>最终一致性 (Eventual consistency)</td>
</tr>
<tr>
<td>持久性 (Durable)</td>
<td></td>
</tr>
</tbody></table>
<ul>
<li><p>一个数据库事务具有ACID四性：</p>
<ul>
<li>A（Atomicity）：原子性，是指事务必须是原子工作单元，对于其数据修改，要么全都执行，要么全都不执行</li>
<li>C（Consistency）：一致性，是指事务在完成时，必须使所有的数据都保持一致状态</li>
<li>I（Isolation）：隔离性，是指由并发事务所做的修改必须与任何其它并发事务所做的修改隔离</li>
<li>D（Durability）：持久性，是指事务完成之后，它对于系统的影响是永久性的，该修改即使出现致命的系统故障也将一直保持</li>
</ul>
</li>
<li><p>BASE的基本含义是基本可用（Basically Availble）、软状态（Soft-state）和最终一致性（Eventual consistency）：</p>
<ul>
<li>基本可用<ul>
<li>基本可用，是指一个分布式系统的一部分发生问题变得不可用时，其他部分仍然可以正常使用，也就是允许分区失败的情形出现</li>
</ul>
</li>
<li>软状态<ul>
<li>“软状态（soft-state）”是与“硬状态（hard-state）”相对应的一种提法。数据库保存的数据是“硬状态”时，可以保证数据一致性，即保证数据一直是正确的。“软状态”是指状态可以有一段时间不同步，具有一定的滞后性</li>
</ul>
</li>
<li>最终一致性<ul>
<li>一致性的类型包括强一致性和弱一致性，二者的主要区别在于高并发的数据访问操作下，后续操作是否能够获取最新的数据。对于强一致性而言，当执行完一次更新操作后，后续的其他读操作就可以保证读到更新后的最新数据；反之，如果不能保证后续访问读到的都是更新后的最新数据，那么就是弱一致性。而最终一致性只不过是弱一致性的一种特例，允许后续的访问操作可以暂时读不到更新后的数据，但是经过一段时间之后，必须最终读到更新后的数据。</li>
<li>最常见的实现最终一致性的系统是DNS（域名系统）。一个域名更新操作根据配置的形式被分发出去，并结合有过期机制的缓存；最终所有的客户端可以看到最新的值。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="最终一致性"><a href="#最终一致性" class="headerlink" title="最终一致性"></a>最终一致性</h2><blockquote>
<p>最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，又可以区分为：</p>
</blockquote>
<ul>
<li>因果一致性：如果进程A通知进程B它已更新了一个数据项，那么进程B的后续访问将获得A写入的最新值。而与进程A无因果关系的进程C的访问，仍然遵守一般的最终一致性规则</li>
<li>“读己之所写”一致性：可以视为因果一致性的一个特例。当进程A自己执行一个更新操作之后，它自己总是可以访问到更新过的值，绝不会看到旧值</li>
<li>单调读一致性：如果进程已经看到过数据对象的某个值，那么任何后续访问都不会返回在那个值之前的值</li>
<li>会话一致性：它把访问存储系统的进程放到会话（session）的上下文中，只要会话还存在，系统就保证“读己之所写”一致性。如果由于某些失败情形令会话终止，就要建立新的会话，而且系统保证不会延续到新的会话</li>
<li>单调写一致性：系统保证来自同一个进程的写操作顺序执行。系统必须保证这种程度的一致性，否则就非常难以编程了</li>
</ul>
<h3 id="对于分布式数据系统："><a href="#对于分布式数据系统：" class="headerlink" title="对于分布式数据系统："></a>对于分布式数据系统：</h3><ul>
<li>N — 数据复制的份数</li>
<li>W — 更新数据是需要保证写完成的节点数</li>
<li>R — 读取数据的时候需要读取的节点数<ul>
<li>如果W+R&gt;N，写的节点和读的节点重叠，则是强一致性。例如对于典型的一主一备同步复制的关系型数据库，N=2,W=2,R=1，则不管读的是主库还是备库的数据，都是一致的。一般设定是R＋W = N+1，这是保证强一致性的最小设定</li>
<li>如果W+R&lt;=N，则是弱一致性。例如对于一主一备异步复制的关系型数据库，N=2,W=1,R=1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱一致性。</li>
</ul>
</li>
<li>对于分布式系统，为了保证高可用性，一般设置N&gt;=3。不同的N,W,R组合，是在可用性和一致性之间取一个平衡，以适应不同的应用场景。</li>
<li>如果N=W,R=1，任何一个写节点失效，都会导致写失败，因此可用性会降低，但是由于数据分布的N个节点是同步写入的，因此可以保证强一致性。</li>
<li>实例：HBase是借助其底层的HDFS来实现其数据冗余备份的。HDFS采用的就是强一致性保证。在数据没有完全同步到N个节点前，写操作是不会返回成功的。也就是说它的W＝N，而读操作只需要读到一个值即可，也就是说它R＝1。</li>
<li>像Voldemort，Cassandra和Riak这些类Dynamo的系统，通常都允许用户按需要设置N，R，W三个值，即使是设置成W＋R&lt;= N也是可以的。也就是说他允许用户在强一致性和最终一致性之间自由选择。而在用户选择了最终一致性，或者是W&lt;N的强一致性时，则总会出现一段“各个节点数据不同步导致系统处理不一致的时间”。为了提供最终一致性的支持，这些系统会提供一些工具来使数据更新被最终同步到所有相关节点。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://sevengarden.club/2018/09/23/HBase/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="seven">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="岳阳北寒">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/09/23/HBase/" itemprop="url">HBase</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-23T19:19:04+08:00">
                2018-09-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Bigdata/" itemprop="url" rel="index">
                    <span itemprop="name">Bigdata</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><blockquote>
<p>HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式，利用廉价计算机集群处理由超过10亿行数据和数百万列元素组成的数据表 </p>
<ul>
<li>Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于Hadoop MapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求</li>
<li>HDFS面向批量访问模式，不是随机访问模式</li>
<li>传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决）</li>
<li>传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间</li>
<li>因此，业界出现了一类面向半结构化数据存储和处理的高可扩展、低写入/查询延迟的系统，例如，键值数据库、文档数据库和列族数据库（如BigTable和HBase等）</li>
<li>HBase已经成功应用于互联网服务领域和传统行业的众多在线式数据分析处理系统中</li>
</ul>
</blockquote>
<h2 id="HBase与传统关系数据库的对比分析"><a href="#HBase与传统关系数据库的对比分析" class="headerlink" title="HBase与传统关系数据库的对比分析"></a>HBase与传统关系数据库的对比分析</h2><ol>
<li>数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串</li>
<li>数据操作：关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系</li>
<li>存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的</li>
<li>数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来</li>
<li>数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留</li>
<li>可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩</li>
</ol>
<h1 id="HBase访问接口"><a href="#HBase访问接口" class="headerlink" title="HBase访问接口"></a>HBase访问接口</h1><table>
<thead>
<tr>
<th>类型</th>
<th>特点</th>
<th>场合</th>
</tr>
</thead>
<tbody><tr>
<td>Native Java API</td>
<td>最常规和高效的访问方式</td>
<td>适合Hadoop MapReduce作业并行批处理HBase表数据</td>
</tr>
<tr>
<td>HBase Shell</td>
<td>HBase的命令行工具，最简单的接口</td>
<td>适合HBase管理使用</td>
</tr>
<tr>
<td>Thrift Gateway</td>
<td>利用Thrift序列化技术，支持C++、PHP、Python等多种语言</td>
<td>适合其他异构系统在线访问HBase表数据</td>
</tr>
<tr>
<td>REST Gateway</td>
<td>解除了语言限制</td>
<td>支持REST风格的Http API访问HBase</td>
</tr>
<tr>
<td>Pig</td>
<td>使用Pig Latin流式编程语言来处理HBase中的数据</td>
<td>适合做数据统计</td>
</tr>
<tr>
<td>Hive</td>
<td>简单</td>
<td>当需要以类似SQL语言方式来访问HBase的时候</td>
</tr>
</tbody></table>
<h1 id="Hbase数据模型"><a href="#Hbase数据模型" class="headerlink" title="Hbase数据模型"></a>Hbase数据模型</h1><h2 id="数据模型概述"><a href="#数据模型概述" class="headerlink" title="数据模型概述"></a>数据模型概述</h2><ul>
<li>HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳</li>
<li>每个值是一个未经解释的字符串，没有数据类型</li>
<li>用户在表中存储数据，每一行都有一个可排序的行键和任意多的列</li>
<li>表在水平方向由一个或者多个列族组成，一个列族中可以包含任意多个列，同一个列族里面的数据存储在一起</li>
<li>列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行据类型转换</li>
<li>HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留（这是和HDFS只允许追加不允许修改的特性相关的）<h2 id="数据模型相关概念"><a href="#数据模型相关概念" class="headerlink" title="数据模型相关概念"></a>数据模型相关概念</h2><img src="https://i.loli.net/2019/08/14/N3DW2LA4kxIYuFa.png" alt="HBase数据模型.png"></li>
<li>表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族</li>
<li>行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。</li>
<li>列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元</li>
<li>列限定符：列族里的数据通过列限定符（或列）来定位</li>
<li>单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]</li>
<li>时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引</li>
</ul>
<h2 id="数据坐标"><a href="#数据坐标" class="headerlink" title="数据坐标"></a>数据坐标</h2><ul>
<li>HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行键, 列族, 列限定符, 时间戳]<table>
<thead>
<tr>
<th><strong>键</strong></th>
<th><strong>值</strong></th>
</tr>
</thead>
<tbody><tr>
<td>[“201505003”, “Info”, “email”, 1174184619081]</td>
<td>“xie@qq.com”</td>
</tr>
<tr>
<td>[“201505003”, “Info”, “email”, 1174184620720]</td>
<td>“you@163.com”</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="概念视图"><a href="#概念视图" class="headerlink" title="概念视图"></a>概念视图</h2><table>
<thead>
<tr>
<th>行键</th>
<th>时间戳</th>
<th>列族contents</th>
<th>列族anchor</th>
</tr>
</thead>
<tbody><tr>
<td>“com.cnn.www”</td>
<td>t5</td>
<td>anchor:cnnsi.com=”CNN”</td>
<td></td>
</tr>
<tr>
<td>t4</td>
<td>anchor:my.look.ca=”CNN.com”</td>
<td></td>
<td></td>
</tr>
<tr>
<td>t3</td>
<td>contents:html=”<html>...“</html></td>
<td></td>
<td></td>
</tr>
<tr>
<td>t2</td>
<td>contents:html=”<html>...“</html></td>
<td></td>
<td></td>
</tr>
<tr>
<td>t1</td>
<td>contents:html=”<html>...“</html></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h2><table>
<thead>
<tr>
<th>行键</th>
<th>时间戳</th>
<th>列族contents</th>
</tr>
</thead>
<tbody><tr>
<td>“com.cnn.www”</td>
<td>t3</td>
<td>contents:html=”<html>...“</html></td>
</tr>
<tr>
<td>t2</td>
<td>contents:html=”<html>...“</html></td>
<td></td>
</tr>
<tr>
<td>t1</td>
<td>contents:html=”<html>...“</html></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>行键</th>
<th>时间戳</th>
<th>列族anchor</th>
</tr>
</thead>
<tbody><tr>
<td>“com.cnn.www”</td>
<td>t5</td>
<td>anchor:cnnsi.com=”CNN”</td>
</tr>
<tr>
<td>t4</td>
<td>anchor:my.look.ca=”CNN.com”</td>
<td></td>
</tr>
</tbody></table>
<h2 id="面向列的存储"><a href="#面向列的存储" class="headerlink" title="面向列的存储"></a>面向列的存储</h2><p><img src="https://i.loli.net/2019/08/14/CrtHkdTRqnQeKZ3.png" alt="面向列的存储.png"><br><img src="https://i.loli.net/2019/08/14/296eZVrmyGhSgE8.png" alt="列式存储.png"><br><img src="https://i.loli.net/2019/08/14/Pun632tgqSbTJpM.png" alt="行式存储.png"></p>
<h1 id="HBase实现原理"><a href="#HBase实现原理" class="headerlink" title="HBase实现原理"></a>HBase实现原理</h1><h2 id="HBase-功能组件"><a href="#HBase-功能组件" class="headerlink" title="HBase 功能组件"></a>HBase 功能组件</h2><ul>
<li>HBase的实现包括三个主要的功能组件：<ol>
<li>库函数：链接到每个客户端</li>
<li>一个Master主服务器</li>
<li>许多个Region服务器</li>
</ol>
</li>
<li>主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡</li>
<li>Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求</li>
<li>客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据</li>
<li>客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小 </li>
</ul>
<h2 id="表和Region"><a href="#表和Region" class="headerlink" title="表和Region"></a>表和Region</h2><ul>
<li>开始只有一个Region，后来不断分裂</li>
<li>Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件<br><img src="https://i.loli.net/2019/08/14/67qygZGCpdfDHY4.jpg" alt="Region表.jpg"><br><img src="https://i.loli.net/2019/08/14/kKYREnvAGqcCP65.jpg" alt="Region表分裂.jpg"></li>
<li>每个Region默认大小是100MB到200MB（2006年以前的硬件配置）<ul>
<li>每个Region的最佳大小取决于单台服务器的有效处理能力</li>
<li>目前每个Region最佳大小建议1GB-2GB（2013年以后的硬件配置）</li>
</ul>
</li>
<li>同一个Region不会被分拆到多个Region服务器</li>
<li>每个Region服务器存储10-1000个Region<br><img src="https://i.loli.net/2019/08/14/kDWwbKAnXqZs1xI.jpg" alt="Region表分布.jpg"></li>
</ul>
<h2 id="Region-定位"><a href="#Region-定位" class="headerlink" title="Region 定位"></a>Region 定位</h2><ul>
<li>元数据表，又名.META.表，存储了Region和Region服务器的映射关系</li>
<li>当HBase表很大时， .META.表也会被分裂成多个Region</li>
<li>根数据表，又名-ROOT-表，记录所有元数据的具体位置</li>
<li>-ROOT-表只有唯一一个Region，名字是在程序中被写死的</li>
<li>Zookeeper文件记录了-ROOT-表的位置<br><img src="https://i.loli.net/2019/08/14/Z3FcJ1iXt7xrKTq.jpg" alt="HBase三层结构1.jpg"></li>
</ul>
<table>
<thead>
<tr>
<th>层次</th>
<th>名称</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>第一层</td>
<td>Zookeeper文件</td>
<td>记录了-ROOT-表的位置信息</td>
</tr>
<tr>
<td>第二层</td>
<td>-ROOT-表</td>
<td>记录了.META.表的Region位置信息</td>
</tr>
<tr>
<td>-ROOT-表只能有一个Region。通过-ROOT-表，就可以访问.META.表中的数据</td>
<td></td>
<td></td>
</tr>
<tr>
<td>第三层</td>
<td>.META.表</td>
<td>记录了用户数据表的Region位置信息，.META.表可以有多个Region，保存了HBase中所有用户数据表的Region位置信息</td>
</tr>
</tbody></table>
<ul>
<li>为了加快访问速度，.META.表的全部Region都会被保存在内存中</li>
<li>假设.META.表的每行（一个映射条目）在内存中大约占用1KB，并且每个Region限制为128MB，那么，上面的三层结构可以保存的用户数据表的Region数目的计算方法是：</li>
<li>（-ROOT-表能够寻址的.META.表的Region个数）×（每个.META.表的 Region可以寻址的用户数据表的Region个数）</li>
<li>一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行（一个映射条目）占用1KB内存计算，128MB空间可以容纳128MB/1KB=217行，也就是说，一个-ROOT-表可以寻址217个.META.表的Region。</li>
<li>同理，每个.META.表的 Region可以寻址的用户数据表的Region个数是128MB/1KB=217。</li>
<li>最终，三层结构可以保存的Region数目是(128MB/1KB) × (128MB/1KB) = 234个Region</li>
</ul>
<h3 id="客户端访问数据时的“三级寻址”"><a href="#客户端访问数据时的“三级寻址”" class="headerlink" title="客户端访问数据时的“三级寻址”"></a>客户端访问数据时的“三级寻址”</h3><ul>
<li>为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题</li>
<li>寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器</li>
</ul>
<h1 id="HBase运行机制"><a href="#HBase运行机制" class="headerlink" title="HBase运行机制"></a>HBase运行机制</h1><h2 id="HBase系统架构"><a href="#HBase系统架构" class="headerlink" title="HBase系统架构"></a>HBase系统架构</h2><p><img src="https://i.loli.net/2019/08/14/68D2kyjXOWYxwRQ.jpg" alt="HBase系统架构.jpg"></p>
<ol>
<li>客户端</li>
</ol>
<ul>
<li>客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程</li>
</ul>
<ol start="2">
<li>Zookeeper服务器</li>
</ol>
<ul>
<li>Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题</li>
<li>Zookeeper是一个很好的集群管理工具，被大量用于分布式计算，提供配置维护、域名服务、分布式同步、组服务等。</li>
</ul>
<p><img src="https://i.loli.net/2019/08/14/nd9wWM5jDcLrSGF.jpg" alt="Zookeeper .jpg"></p>
<ol start="3">
<li>Master</li>
</ol>
<ul>
<li>主服务器Master主要负责表和Region的管理工作：<ul>
<li>管理用户对表的增加、删除、修改、查询等操作</li>
<li>实现不同Region服务器之间的负载均衡</li>
<li>在Region分裂或合并后，负责重新调整Region的分布</li>
<li>对发生故障失效的Region服务器上的Region进行迁移</li>
</ul>
</li>
</ul>
<ol start="4">
<li>Region服务器</li>
</ol>
<ul>
<li>Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求</li>
</ul>
<h2 id="Region服务器工作原理"><a href="#Region服务器工作原理" class="headerlink" title="Region服务器工作原理"></a>Region服务器工作原理</h2><p><img src="https://i.loli.net/2019/08/14/zRVdymofpnXAkY6.jpg" alt="Region服务器向HDFS文件系统中读写数据 .jpg"></p>
<ol>
<li>用户读写数据过程<ul>
<li>用户写入数据时，被分配到相应Region服务器去执行</li>
<li>用户数据首先被写入到MemStore和Hlog中</li>
<li>只有当操作写入Hlog之后，commit()调用才会将其返回给客户端</li>
<li>当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找</li>
</ul>
</li>
<li>缓存的刷新<ul>
<li>系统会周期性地把MemStore缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记</li>
<li>每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件</li>
<li>每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务</li>
</ul>
</li>
<li>StoreFile的合并<ul>
<li>每次刷写都生成一个新的StoreFile，数量太多，影响查找速度</li>
<li>调用Store.compact()把多个合并成一个</li>
<li>合并操作比较耗费资源，只有数量达到一个阈值才启动合并</li>
</ul>
</li>
</ol>
<h2 id="Store工作原理"><a href="#Store工作原理" class="headerlink" title="Store工作原理"></a>Store工作原理</h2><ul>
<li>Store是Region服务器的核心</li>
<li>多个StoreFile合并成一个</li>
<li>单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region<br><img src="https://i.loli.net/2019/08/14/8O5SjMrPXwiKYtB.jpg" alt="Store是Region服务器的核心 .jpg"></li>
</ul>
<h2 id="HLog工作原理"><a href="#HLog工作原理" class="headerlink" title="HLog工作原理"></a>HLog工作原理</h2><ul>
<li>分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复</li>
<li>HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）</li>
<li>用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘</li>
<li>Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master</li>
<li>Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录</li>
<li>系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器</li>
<li>Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复</li>
<li>共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志</li>
</ul>
<h1 id="HBase应用方案"><a href="#HBase应用方案" class="headerlink" title="HBase应用方案"></a>HBase应用方案</h1><h2 id="HBase实际应用中的性能优化方法"><a href="#HBase实际应用中的性能优化方法" class="headerlink" title="HBase实际应用中的性能优化方法"></a>HBase实际应用中的性能优化方法</h2><h3 id="行健（Row-Key）"><a href="#行健（Row-Key）" class="headerlink" title="行健（Row Key）"></a>行健（Row Key）</h3><ul>
<li>行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。</li>
<li>举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用Long.MAX_VALUE - timestamp作为行键，这样能保证新写入的数据在读取时可以被快速命中。<h3 id="Inmemory"><a href="#Inmemory" class="headerlink" title="Inmemory"></a>Inmemory</h3></li>
<li>创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。<h3 id="Max-Version"><a href="#Max-Version" class="headerlink" title="Max Version"></a>Max Version</h3></li>
<li>创建表的时候，可以通过HColumnDescriptor.setMaxVersions(int maxVersions)设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置setMaxVersions(1)。<h3 id="Time-To-Live"><a href="#Time-To-Live" class="headerlink" title="Time To Live"></a>Time To Live</h3></li>
<li>创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 * 24 * 60 * 60)。<h2 id="HBase性能监视"><a href="#HBase性能监视" class="headerlink" title="HBase性能监视"></a>HBase性能监视</h2></li>
<li>Master-status(自带)</li>
<li>Ambari</li>
<li>OpenTSDB</li>
<li>Ganglia<h2 id="在HBase之上构建SQL引擎"><a href="#在HBase之上构建SQL引擎" class="headerlink" title="在HBase之上构建SQL引擎"></a>在HBase之上构建SQL引擎</h2></li>
<li>Hive整合HBase</li>
<li>Phoenix<h2 id="构建HBase二级索引"><a href="#构建HBase二级索引" class="headerlink" title="构建HBase二级索引"></a>构建HBase二级索引</h2></li>
<li>HBase只有一个针对行健的索引，访问HBase表中的行，只有三种方式：<ul>
<li>通过单个行健访问</li>
<li>通过一个行健的区间来访问</li>
<li>全表扫描</li>
</ul>
</li>
<li>二级索引延展<ul>
<li>Hindex二级索引</li>
<li>HBase+Redis</li>
<li>HBase+solr</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">seven</p>
              <p class="site-description motion-element" itemprop="description">seven 的精神家园，学习笔记</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="1988xuegang@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">seven</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
